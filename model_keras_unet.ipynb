{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_keras_unet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Hoiy/kaggle-tgs-salt-identification-challenge/blob/master/model_keras_unet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "r9DlwwbXq6o_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5bf4df53-65ea-4959-86af-0dea1aa7685b"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import dotenv\n",
        "import keras\n",
        "import os\n",
        "import gc\n",
        "dotenv.load_dotenv('.env')\n",
        "\n",
        "!mkdir model\n",
        "# !gsutil rsync gs://{os.environ['GCP_BUCKET']}/model model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WoL4Y1yXqtaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30e7584d-4aa8-4efb-ae44-4a0454da5547"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('raw/train.csv').set_index('id')\n",
        "test = pd.read_csv('raw/sample_submission.csv').set_index('id')\n",
        "depths = pd.read_csv('raw/depths.csv').set_index('id')\n",
        "\n",
        "train = pd.merge(train, depths, left_index=True, right_index=True, how='inner')\n",
        "train.shape, test.shape, depths.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 2), (18000, 1), (22000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "6TALjxylQlox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_size_target = 101"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6nC41yXq8j0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "03e1d1f5-067b-409f-e493-b5405b31bd1d"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "\n",
        "train[\"image\"] = [np.array(load_img(\"./raw/images/{}.png\".format(idx), grayscale=False)) / 255 for idx in tqdm(train.index)]\n",
        "train[\"mask\"] = [np.array(load_img(\"./raw/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train.index)]\n",
        "\n",
        "X_train = np.array(train['image'].tolist()).reshape(-1, img_size_target, img_size_target, 3)\n",
        "y_train = np.array(train['mask'].tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
        "\n",
        "X_test = np.array([np.array(load_img(\"./raw/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(test.index)]).reshape(-1, img_size_target, img_size_target, 3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4000/4000 [00:03<00:00, 1092.66it/s]\n",
            "100%|██████████| 4000/4000 [00:01<00:00, 2343.01it/s]\n",
            "100%|██████████| 18000/18000 [00:14<00:00, 1263.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "16XOGte5q_0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, SpatialDropout2D, UpSampling2D, Activation, Add\n",
        "\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return 1. - score\n",
        "  \n",
        "  \n",
        "\n",
        "# define iou or jaccard loss function\n",
        "def jaccard_loss(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    \n",
        "    score = (intersection + epsilon) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + epsilon)\n",
        "    return 1 - score\n",
        "\n",
        "\n",
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "  \n",
        "\n",
        "def iou_bce_loss(y_true, y_pred):\n",
        "    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.to_float(tf.greater(tf.round(y_true), y_pred)))\n",
        "    \n",
        "\n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     i = tf.round(y_pred) * tf.round(y_true)\n",
        "#     o = tf.round(y_pred) * tf.round(y_true)\n",
        "#     mean_iou = tf.metrics.mean_iou(y_true, tf.round(y_pred), 1)\n",
        "#     print(mean_iou)\n",
        "#     print(tf.tile(mean_iou, 10))\n",
        "#     print(mean_iou[0])\n",
        "#     return mean_iou[0]\n",
        "#     return tf.reduce_mean(tf.tile(mean_iou, 10))\n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     prec = []\n",
        "#     for t in np.arange(0.5, 1.0, 0.05):\n",
        "#         y_pred_ = tf.to_int32(y_pred > t)\n",
        "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         with tf.control_dependencies([up_opt]):\n",
        "#             score = tf.identity(score)\n",
        "#         prec.append(score)\n",
        "#     return K.mean(K.stack(prec), axis=0)\n",
        "\n",
        "\n",
        "# def convolution_block(x, filters, size):\n",
        "#     return x\n",
        "\n",
        "  \n",
        "def residual_block(blockInput, filters):\n",
        "    x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(blockInput)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(x)\n",
        "    x = Add()([x, blockInput])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "def res_block(i, filters=16):\n",
        "    n1 = Conv2D(filters, 1, activation='relu', padding='same')(i)\n",
        "    n3 = Conv2D(filters, 3, activation='relu', padding='same')(i)\n",
        "    n5 = Conv2D(filters, 5, activation='relu', padding='same')(i)\n",
        "    n = Concatenate()([n1, n3, n5])\n",
        "    n = BatchNormalization()(n)\n",
        "    \n",
        "    n1 = Conv2D(filters, 1, activation='relu', padding='same')(n)\n",
        "    n3 = Conv2D(filters, 3, activation='relu', padding='same')(n)\n",
        "    n5 = Conv2D(filters, 5, activation='relu', padding='same')(n)\n",
        "    n = Concatenate()([n1, n3, n5])\n",
        "    n = BatchNormalization()(n)\n",
        "    \n",
        "#     n = Conv2D(1, 1, activation='relu', padding='same')(n)\n",
        "#     n = Concatenate()([i, n])\n",
        "#     n = BatchNormalization()(n)\n",
        "    return n\n",
        "\n",
        "  \n",
        "# def level_block(inp, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "#     if depth > 0:\n",
        "#         n = conv_block(m, dim, acti, bn, res)\n",
        "#         m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "#         m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
        "#         if up:\n",
        "#             m = UpSampling2D()(m)\n",
        "#             m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "#         else:\n",
        "# #             padding = ['valid', 'same','same','valid','same','valid']\n",
        "#             padding = ['same','same','valid','same','valid']\n",
        "#             m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding=padding[depth-1])(m)\n",
        "#         n = Concatenate()([n, m])\n",
        "#         m = conv_block(n, dim, acti, bn, res)\n",
        "#     else:\n",
        "#         m = conv_block(m, dim, acti, bn, res, do=do)\n",
        "#     return m\n",
        "\n",
        "  \n",
        "def ResNet():\n",
        "    filter_size = 8\n",
        "    i = Input(shape=(img_size_target,img_size_target,1))\n",
        "    o = Conv2D(filter_size, 1, strides=(1,1), padding='same', activation='relu')(i)\n",
        "    for depth in tqdm(range(128)):\n",
        "      o = residual_block(o, filter_size)\n",
        "    o = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(o)\n",
        "    return Model(inputs=i, outputs=o)\n",
        "  \n",
        "  \n",
        "def ResNet_2():\n",
        "    resnet = keras.applications.resnet50.ResNet50(include_top=False, input_shape=(img_size_target, img_size_target, 3))\n",
        "    o = Conv2D(1, 1, strides=(1,1), activation='sigmoid')(resnet.output)\n",
        "    return Model(inputs=resnet.input, outputs=o)\n",
        "  \n",
        "\n",
        "# ResNet_2().summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NDV_LZ8Ernrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "15956319-08c6-415f-85cf-48624a1d6192"
      },
      "cell_type": "code",
      "source": [
        "def build_train_generator(X_train, y_train, batch_size=32):\n",
        "    # Ref: https://keras.io/preprocessing/image/\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(\n",
        "#       rotation_range=360.,\n",
        "#       width_shift_range=0.1,\n",
        "#       height_shift_range=0.1,\n",
        "      fill_mode='reflect',\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "    )\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
        "    seed = 1\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "    \n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "\n",
        "    # combine generators into one which yields image and masks\n",
        "    return zip(image_generator, mask_generator)\n",
        "\n",
        "\n",
        "def cv(X_train, y_train, X_test, fold, fast=False):\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    \n",
        "    loss = []\n",
        "    metric = []\n",
        "    pred_test = np.zeros(X_test.shape)\n",
        "    fold_index = 0\n",
        "    \n",
        "    start_ch = 32\n",
        "    depth = 5\n",
        "         \n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train):      \n",
        "#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\n",
        "      model = ResNet_2()\n",
        "      model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[mean_iou, \"binary_crossentropy\"])\n",
        "\n",
        "      early_stopping = EarlyStopping(patience=10, verbose=1)\n",
        "      model_checkpoint = ModelCheckpoint(\"./model/keras_%d.model\"%fold_index, save_best_only=True, verbose=1)\n",
        "      reduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "      epochs = 20000\n",
        "      batch_size = 2\n",
        "\n",
        "      history = model.fit_generator(\n",
        "          build_train_generator(X_train[train_index], y_train[train_index], batch_size),\n",
        "          validation_data=(X_train[test_index], y_train[test_index]),\n",
        "          steps_per_epoch=np.ceil(len(X_train[train_index]) / batch_size), \n",
        "          epochs=epochs,\n",
        "          callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "#           callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "      \n",
        "      best_idx = np.argmin(history.history['val_loss'])\n",
        "      val_loss = history.history['val_loss'][best_idx]\n",
        "      val_mean_iou = history.history['val_mean_iou'][best_idx]\n",
        "      \n",
        "      print(\"Best val_loss: %.4f, val_mean_iou: %.4f\"%(val_loss, val_mean_iou))\n",
        "      loss.append(val_loss)\n",
        "      metric.append(val_mean_iou)\n",
        "      \n",
        "      model = keras.models.load_model('model/keras_%d.model'%fold_index, custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss, 'dice_loss': dice_loss})\n",
        "      pred_test += model.predict(X_test) / fold\n",
        "      fold_index = fold_index + 1\n",
        "      \n",
        "      del model\n",
        "      gc.collect()\n",
        "      \n",
        "      if fast:\n",
        "        break\n",
        "      \n",
        "      \n",
        "    print('%d fold val_loss: %.4f, val_mean_iou: %.4f'%(fold, np.mean(loss), np.mean(metric)))\n",
        "    return pred_test, np.mean(loss), np.mean(metric)\n",
        "\n",
        "  \n",
        "pred_test, val_loss, val_mean_iou = cv(X_train, y_train, X_test, 5, fast=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8e318d9dd2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-8e318d9dd2b3>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(X_train, y_train, X_test, fold, fast)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmean_iou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-009f6546d696>\u001b[0m in \u001b[0;36mResNet_2\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mResNet_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/applications/resnet50.py\u001b[0m in \u001b[0;36mResNet50\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                                       \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                                       weights=weights)\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/applications/imagenet_utils.py\u001b[0m in \u001b[0;36m_obtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    300\u001b[0m                     raise ValueError('Input size must be at least ' +\n\u001b[1;32m    301\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'x'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'; got '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                                      '`input_shape=' + str(input_shape) + '`')\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input size must be at least 197x197; got `input_shape=(101, 101, 3)`"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HRemgbrVOpjK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # summarize history for loss\n",
        "# plt.plot(history.history['loss'][1:])\n",
        "# plt.plot(history.history['val_loss'][1:])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train','Validation'], loc='upper left')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNqZwFOebfzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# best_idx = np.argmin(history.history['val_loss'])\n",
        "# val_loss = history.history['val_loss'][best_idx]\n",
        "# val_mean_iou = history.history['val_mean_iou'][best_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgBEjwKHCn1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('model/keras.model', custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EqepTCQa1djX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RLenc(img, order='F', format=True):\n",
        "    \"\"\"\n",
        "    img is binary mask image, shape (r,c)\n",
        "    order is down-then-right, i.e. Fortran\n",
        "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
        "\n",
        "    returns run length as an array or string (if format is True)\n",
        "    \"\"\"\n",
        "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
        "    runs = []  ## list of run lengths\n",
        "    r = 0  ## the current run length\n",
        "    pos = 1  ## count starts from 1 per WK\n",
        "    for c in bytes:\n",
        "        if (c == 0):\n",
        "            if r != 0:\n",
        "                runs.append((pos, r))\n",
        "                pos += r\n",
        "                r = 0\n",
        "            pos += 1\n",
        "        else:\n",
        "            r += 1\n",
        "\n",
        "    # if last run is unsaved (i.e. data ends with 1)\n",
        "    if r != 0:\n",
        "        runs.append((pos, r))\n",
        "        pos += r\n",
        "        r = 0\n",
        "\n",
        "    if format:\n",
        "        z = ''\n",
        "\n",
        "        for rr in runs:\n",
        "            z += '{} {} '.format(rr[0], rr[1])    \n",
        "        return z[:-1]\n",
        "    else:\n",
        "        return runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5wnyMSkd50M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class FasterRle(object):\n",
        "#     \"\"\"Perform RLE in paralell.\"\"\"\n",
        "\n",
        "#     def __init__(self, num_consumers=2):\n",
        "#         \"\"\"Initialize class.\"\"\"\n",
        "#         self._tasks = multiprocessing.JoinableQueue()\n",
        "#         self._results = multiprocessing.Queue()\n",
        "#         self._n_consumers = num_consumers\n",
        "\n",
        "#         # Initialize consumers\n",
        "#         self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n",
        "#         for w in self._consumers:\n",
        "#             w.start()\n",
        "\n",
        "#     def add(self, array, startIndex):\n",
        "#         \"\"\"Add a task to perform.\"\"\"\n",
        "#         self._tasks.put(FasterTask(array, startIndex))\n",
        "\n",
        "#     def get_results(self):\n",
        "#         \"\"\"Close all tasks.\"\"\"\n",
        "#         # Provide poison pill\n",
        "#         [self._tasks.put(None) for _ in range(self._n_consumers)]\n",
        "#         # Wait for finish\n",
        "#         self._tasks.join()\n",
        "#         # Return results\n",
        "#         singles = []\n",
        "#         while not self._results.empty():\n",
        "#             singles.append(self._results.get())\n",
        "            \n",
        "#         resultDic = dict()\n",
        "#         for rles, start in singles:\n",
        "#             #print('start:', start)\n",
        "#             for i,rle in enumerate(rles):\n",
        "#                 #print('i:', i)\n",
        "#                 resultDic[str(start+i)] = rle\n",
        "#         return resultDic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7wRK-ZUEmzA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch(func):\n",
        "  def batch_func(b):\n",
        "    return np.array([func(e) for e in tqdm(b)])\n",
        "  return batch_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sy0eHi4-P5PP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preds_test = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hIcfQ4_pPlHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e52d1ce9-b9b2-4fdf-f039-e606626a790a"
      },
      "cell_type": "code",
      "source": [
        "test['rle_mask'] = batch(RLenc)(np.round(pred_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18000/18000 [00:58<00:00, 309.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ST2JPIMLP666",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7aaf49b7-1c59-4efc-cfb8-2bdd7e3a885f"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_FILE = 'keras_unet_sc32de5wbecjaccloss_%.4f_%.4f.csv'%(val_loss, val_mean_iou)\n",
        "test.to_csv(SUBMISSION_FILE)\n",
        "SUBMISSION_FILE"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'keras_unet_sc32de5wbecjaccloss_0.1320_0.7477.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "7iJCa8ZqRppy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cc737439-5883-4295-c670-658d69aaff48"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_MESSAGE='\"Keras U-net iou_bce_loss: val_loss: %.4f, val_mean_iou: %.4f\"'%(val_loss, val_mean_iou)\n",
        "!kaggle competitions submit -f '{SUBMISSION_FILE}' -m '{SUBMISSION_MESSAGE}'"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using competition: tgs-salt-identification-challenge\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.4.6)\n",
            "Successfully submitted to TGS Salt Identification Challenge"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ql1uneNwTuEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "c9be94ee-85ec-4429-d351-98436cfaf45f"
      },
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv('.env')\n",
        "!gsutil rsync model gs://{os.environ['GCP_BUCKET']}/model"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "Copying file://model/keras_0.model [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "Copying file://model/keras_1.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_2.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_3.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m -o ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying file://model/keras_4.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "Operation completed over 5 objects/751.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6UwBwalwuZpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "bad2519d-f469-4e15-ce5c-1a921ed92c84"
      },
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# def batch_iou(pred, truth):\n",
        "#     pred_ = tf.round(pred)\n",
        "#     truth_ = truth\n",
        "    \n",
        "#     i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "#     u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "#     epsilon = tf.keras.backend.epsilon()\n",
        "#     return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "  \n",
        "# def mean_iou(pred, truth):\n",
        "#     ious = batch_iou(pred, truth)\n",
        "#     ious = tf.reshape(ious, (-1,1))\n",
        "#     ious = tf.tile(ious, (1,10))\n",
        "#     thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "#     return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "# BATCH_SIZE = 100\n",
        "# ones = tf.constant(np.ones((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# zeros = tf.constant(np.zeros((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# half = tf.coonstant(np.zeros(BATCH_SIZE, ))\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "  \n",
        "#   print(mean_iou(ones, zeros).eval())\n",
        "#   print(mean_iou(ones, ones).eval())\n",
        "#   print(mean_iou(zeros, ones).eval())\n",
        "#   print(mean_iou(zeros, zeros).eval())\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1AayNFWou9ms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}