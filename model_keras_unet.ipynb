{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_keras_unet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Hoiy/kaggle-tgs-salt-identification-challenge/blob/master/model_keras_unet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "r9DlwwbXq6o_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "667249c2-faa3-4063-cbc7-9fd56242179f"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import dotenv\n",
        "import keras\n",
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "dotenv.load_dotenv('.env')\n",
        "\n",
        "!mkdir model\n",
        "# !gsutil rsync gs://{os.environ['GCP_BUCKET']}/model model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WoL4Y1yXqtaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8178cc85-a1bf-4eab-c75f-9a2f7ae2aafd"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('raw/train.csv').set_index('id')\n",
        "test = pd.read_csv('raw/sample_submission.csv').set_index('id')\n",
        "depths = pd.read_csv('raw/depths.csv').set_index('id')\n",
        "\n",
        "train = pd.merge(train, depths, left_index=True, right_index=True, how='inner')\n",
        "train.shape, test.shape, depths.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 2), (18000, 1), (22000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "6TALjxylQlox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6nC41yXq8j0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e0ef7f4e-c6b5-4e78-b72b-e2f7e4d1457a"
      },
      "cell_type": "code",
      "source": [
        "X_train = [cv2.resize(cv2.imread(\"./raw/images/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(train.index)]\n",
        "X_train = np.expand_dims(np.array(X_train), -1)\n",
        "X_train = X_train / 255\n",
        "\n",
        "y_train = [cv2.resize(cv2.imread(\"./raw/masks/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(train.index)]\n",
        "y_train = np.expand_dims(np.array(y_train), -1)\n",
        "y_train = y_train / 255\n",
        "\n",
        "\n",
        "X_test = [cv2.resize(cv2.imread(\"./raw/images/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(test.index)]\n",
        "X_test = np.expand_dims(np.array(X_test), -1)\n",
        "X_test = X_test / 255\n",
        "\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4000/4000 [00:04<00:00, 941.87it/s] \n",
            "100%|██████████| 4000/4000 [00:02<00:00, 1339.93it/s]\n",
            "100%|██████████| 18000/18000 [00:18<00:00, 969.59it/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 128, 128, 1), (4000, 128, 128, 1), (18000, 128, 128, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "nd29HRsWbwC2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/albu/albumentations/blob/master/notebooks/example_kaggle_salt.ipynb\n",
        "# from matplotlib import pyplot as plt\n",
        "\n",
        "# def visualize(image, mask, original_image=None, original_mask=None):\n",
        "#     fontsize = 18\n",
        "    \n",
        "#     if original_image is None and original_mask is None:\n",
        "#         f, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
        "\n",
        "#         ax[0].imshow(image)\n",
        "#         ax[1].imshow(mask)\n",
        "#     else:\n",
        "#         f, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
        "\n",
        "#         ax[0, 0].imshow(original_image)\n",
        "#         ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
        "        \n",
        "#         ax[1, 0].imshow(original_mask)\n",
        "#         ax[1, 0].set_title('Original mask', fontsize=fontsize)\n",
        "        \n",
        "#         ax[0, 1].imshow(image)\n",
        "#         ax[0, 1].set_title('Transformed image', fontsize=fontsize)\n",
        "        \n",
        "#         ax[1, 1].imshow(mask)\n",
        "#         ax[1, 1].set_title('Transformed mask', fontsize=fontsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DgfNeryExtjV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# visualize(X_train[2].reshape(IMG_SIZE, IMG_SIZE), y_train[2].reshape(IMG_SIZE, IMG_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16XOGte5q_0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3731
        },
        "outputId": "f620ab75-e7b4-4ed9-b632-8c6aade27167"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, SpatialDropout2D, UpSampling2D, Activation, Add\n",
        "\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return 1. - score\n",
        "  \n",
        "  \n",
        "\n",
        "# define iou or jaccard loss function\n",
        "def jaccard_loss(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    \n",
        "    score = (intersection + epsilon) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + epsilon)\n",
        "    return 1 - score\n",
        "\n",
        "\n",
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    gts = tf.reduce_sum(gt_sorted)\n",
        "    intersection = gts - tf.cumsum(gt_sorted)\n",
        "    union = gts + tf.cumsum(1. - gt_sorted)\n",
        "    jaccard = 1. - intersection / union\n",
        "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
        "    return jaccard\n",
        "  \n",
        "  \n",
        "  \n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        def treat_image(log_lab):\n",
        "            log, lab = log_lab\n",
        "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
        "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
        "            return lovasz_hinge_flat(log, lab)\n",
        "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
        "        loss = tf.reduce_mean(losses)\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_loss():\n",
        "        labelsf = tf.cast(labels, logits.dtype)\n",
        "        signs = 2. * labelsf - 1.\n",
        "        errors = 1. - logits * tf.stop_gradient(signs)\n",
        "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
        "        gt_sorted = tf.gather(labelsf, perm)\n",
        "        grad = lovasz_grad(gt_sorted)\n",
        "        #loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        loss = tf.tensordot(tf.nn.elu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        return loss\n",
        "\n",
        "    # deal with the void prediction case (only void pixels)\n",
        "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
        "                   lambda: tf.reduce_sum(logits) * 0.,\n",
        "                   compute_loss,\n",
        "                   strict=True,\n",
        "                   name=\"loss\"\n",
        "                   )\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = tf.reshape(scores, (-1,))\n",
        "    labels = tf.reshape(labels, (-1,))\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = tf.not_equal(labels, ignore)\n",
        "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
        "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
        "    return vscores, vlabels\n",
        "\n",
        "def lovasz_loss(y_true, y_pred):\n",
        "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
        "    #logits = K.log(y_pred / (1. - y_pred))\n",
        "    logits = y_pred #Jiaxin\n",
        "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
        "    return loss\n",
        "  \n",
        "\n",
        "# def iou_bce_loss(y_true, y_pred):\n",
        "#     return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.to_float(tf.greater(tf.round(y_true), y_pred)))\n",
        "    \n",
        "\n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     i = tf.round(y_pred) * tf.round(y_true)\n",
        "#     o = tf.round(y_pred) * tf.round(y_true)\n",
        "#     mean_iou = tf.metrics.mean_iou(y_true, tf.round(y_pred), 1)\n",
        "#     print(mean_iou)\n",
        "#     print(tf.tile(mean_iou, 10))\n",
        "#     print(mean_iou[0])\n",
        "#     return mean_iou[0]\n",
        "#     return tf.reduce_mean(tf.tile(mean_iou, 10))\n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     prec = []\n",
        "#     for t in np.arange(0.5, 1.0, 0.05):\n",
        "#         y_pred_ = tf.to_int32(y_pred > t)\n",
        "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         with tf.control_dependencies([up_opt]):\n",
        "#             score = tf.identity(score)\n",
        "#         prec.append(score)\n",
        "#     return K.mean(K.stack(prec), axis=0)\n",
        "\n",
        "\n",
        "# def convolution_block(x, filters, size):\n",
        "#     return x\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# def residual_block(blockInput, filters):\n",
        "#     x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(blockInput)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(x)\n",
        "#     x = Add()([x, blockInput])\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     return x\n",
        "  \n",
        "  \n",
        "# def res_block(i, filters=16):\n",
        "#     n1 = Conv2D(filters, 1, activation='relu', padding='same')(i)\n",
        "#     n3 = Conv2D(filters, 3, activation='relu', padding='same')(i)\n",
        "#     n5 = Conv2D(filters, 5, activation='relu', padding='same')(i)\n",
        "#     n = Concatenate()([n1, n3, n5])\n",
        "#     n = BatchNormalization()(n)\n",
        "    \n",
        "#     n1 = Conv2D(filters, 1, activation='relu', padding='same')(n)\n",
        "#     n3 = Conv2D(filters, 3, activation='relu', padding='same')(n)\n",
        "#     n5 = Conv2D(filters, 5, activation='relu', padding='same')(n)\n",
        "#     n = Concatenate()([n1, n3, n5])\n",
        "#     n = BatchNormalization()(n)\n",
        "    \n",
        "# #     n = Conv2D(1, 1, activation='relu', padding='same')(n)\n",
        "# #     n = Concatenate()([i, n])\n",
        "# #     n = BatchNormalization()(n)\n",
        "#     return n\n",
        "\n",
        "  \n",
        "# def level_block(inp, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "#     if depth > 0:\n",
        "#         n = conv_block(m, dim, acti, bn, res)\n",
        "#         m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "#         m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
        "#         if up:\n",
        "#             m = UpSampling2D()(m)\n",
        "#             m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "#         else:\n",
        "# #             padding = ['valid', 'same','same','valid','same','valid']\n",
        "#             padding = ['same','same','valid','same','valid']\n",
        "#             m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding=padding[depth-1])(m)\n",
        "#         n = Concatenate()([n, m])\n",
        "#         m = conv_block(n, dim, acti, bn, res)\n",
        "#     else:\n",
        "#         m = conv_block(m, dim, acti, bn, res, do=do)\n",
        "#     return m\n",
        "\n",
        "  \n",
        "# def ResNet():\n",
        "#     filter_size = 8\n",
        "#     i = Input(shape=(img_size_target,img_size_target,1))\n",
        "#     o = Conv2D(filter_size, 1, strides=(1,1), padding='same', activation='relu')(i)\n",
        "#     for depth in tqdm(range(128)):\n",
        "#       o = residual_block(o, filter_size)\n",
        "#     o = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(o)\n",
        "#     return Model(inputs=i, outputs=o)\n",
        "  \n",
        "  \n",
        "# def ResNet_2():\n",
        "#     resnet = keras.applications.resnet50.ResNet50(include_top=False, pooling=None, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "#     o = Conv2D(1, 1, strides=(1,1), activation='sigmoid')(resnet.output)\n",
        "#     return Model(inputs=resnet.input, outputs=o)\n",
        "  \n",
        "def res_block(inputs, filters):\n",
        "    inputs = BatchNormalization()(inputs)\n",
        "    conv = Conv2D(filters, 3, strides=1, padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = Conv2D(filters, 3, strides=1, padding='same', activation='relu')(conv)\n",
        "    \n",
        "    conv = Add()([conv, inputs])\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    return Conv2D(filters, 3, strides=2, padding='same', activation='relu')(conv)\n",
        "  \n",
        "\n",
        "def res_block_r(inputs, filters, res):\n",
        "    inputs = Add()([inputs, res])\n",
        "    inputs = BatchNormalization()(inputs)\n",
        "    conv = Conv2DTranspose(filters, 3, strides=1, padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = Conv2DTranspose(filters, 3, strides=1, padding='same', activation='relu')(conv)\n",
        "    \n",
        "    conv = Add()([conv, inputs])\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    return Conv2DTranspose(filters, 3, strides=2, padding='same', activation='relu')(conv)\n",
        "  \n",
        "  \n",
        "# def build_model(start_channels=8, depth=6):\n",
        "#     inp = Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "#     l = BatchNormalization()(inp)\n",
        "#     l = Conv2D(start_channels, 3, strides=1, padding='same', activation='relu')(l)\n",
        "    \n",
        "#     layers = [l]\n",
        "    \n",
        "#     for i in range(depth):\n",
        "#       layers.append(res_block(layers[-1], start_channels * (2 ** i)))\n",
        "      \n",
        "      \n",
        "#     for i in range(depth):\n",
        "#       layers.append(res_block_r(layers[-1], start_channels * (2 ** (depth - 1 - i)), layers[-2*i-1]))\n",
        "      \n",
        "#     l = BatchNormalization()(layers[-1])\n",
        "#     out = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(l)\n",
        "    \n",
        "#     return Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "def build_model(channels=64, depth=6):\n",
        "    inp = Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "    l = BatchNormalization()(inp)\n",
        "    l = Conv2D(channels, 3, strides=1, padding='same', activation='relu')(l)\n",
        "    \n",
        "    layers = [l]\n",
        "    \n",
        "    for i in range(depth):\n",
        "      layers.append(res_block(layers[-1], channels))\n",
        "      \n",
        "      \n",
        "    for i in range(depth):\n",
        "      layers.append(res_block_r(layers[-1], channels, layers[-2*i-1]))\n",
        "      \n",
        "    l = BatchNormalization()(layers[-1])\n",
        "    out = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(l)\n",
        "    \n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "  \n",
        "build_model().summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 128, 128, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128, 128, 1)  4           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 64) 640         batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 128, 128, 64) 0           conv2d_3[0][0]                   \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 128, 128, 64) 256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 64, 64, 64)   0           conv2d_6[0][0]                   \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 64, 64, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 64)   0           conv2d_9[0][0]                   \n",
            "                                                                 batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 64)   0           conv2d_12[0][0]                  \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 64)     0           conv2d_15[0][0]                  \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 4, 4, 64)     256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 4, 4, 64)     256         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 4, 4, 64)     0           conv2d_18[0][0]                  \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 4, 4, 64)     256         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 2, 2, 64)     36928       batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 2, 2, 64)     0           conv2d_19[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 2, 2, 64)     256         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 2, 2, 64)     36928       batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 2, 2, 64)     256         conv2d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 2, 2, 64)     36928       batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 2, 2, 64)     0           conv2d_transpose_2[0][0]         \n",
            "                                                                 batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 2, 2, 64)     256         add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 4, 4, 64)     0           conv2d_transpose_3[0][0]         \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 64)     256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 4, 4, 64)     256         conv2d_transpose_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 4, 4, 64)     0           conv2d_transpose_5[0][0]         \n",
            "                                                                 batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 4, 4, 64)     256         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 8, 8, 64)     0           conv2d_transpose_6[0][0]         \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 64)     256         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 64)     256         conv2d_transpose_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 8, 8, 64)     0           conv2d_transpose_8[0][0]         \n",
            "                                                                 batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 64)     256         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTrans (None, 16, 16, 64)   36928       batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_9[0][0]         \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 64)   256         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DTran (None, 16, 16, 64)   36928       batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 64)   256         conv2d_transpose_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_11 (Conv2DTran (None, 16, 16, 64)   36928       batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_11[0][0]        \n",
            "                                                                 batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 16, 16, 64)   256         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 32, 32, 64)   0           conv2d_transpose_12[0][0]        \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 64)   256         add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 64)   256         conv2d_transpose_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_14 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 32, 32, 64)   0           conv2d_transpose_14[0][0]        \n",
            "                                                                 batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 32, 32, 64)   256         add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_15 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 64, 64, 64)   0           conv2d_transpose_15[0][0]        \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 64, 64, 64)   256         add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_16 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 64, 64, 64)   256         conv2d_transpose_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_17 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 64, 64, 64)   0           conv2d_transpose_17[0][0]        \n",
            "                                                                 batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 64, 64, 64)   256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_18 (Conv2DTran (None, 128, 128, 64) 36928       batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 128, 128, 64) 256         conv2d_transpose_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 128, 128, 1)  65          batch_normalization_38[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 1,339,589\n",
            "Trainable params: 1,334,851\n",
            "Non-trainable params: 4,738\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NDV_LZ8Ernrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3657
        },
        "outputId": "a05643c3-94dc-41af-9902-4ad3e8ea1c21"
      },
      "cell_type": "code",
      "source": [
        "def build_train_generator(X_train, y_train, batch_size=32):\n",
        "    # Ref: https://keras.io/preprocessing/image/\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(\n",
        "#       rotation_range=360.,\n",
        "#       width_shift_range=0.1,\n",
        "#       height_shift_range=0.1,\n",
        "      fill_mode='reflect',\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "    )\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
        "    seed = 1\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "    \n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "\n",
        "    # combine generators into one which yields image and masks\n",
        "    return zip(image_generator, mask_generator)\n",
        "\n",
        "\n",
        "def cv(X_train, y_train, X_test, fold, fast=False):\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    \n",
        "    loss = []\n",
        "    metric = []\n",
        "    pred_test = np.zeros(X_test.shape)\n",
        "    fold_index = 0\n",
        "    \n",
        "    start_ch = 32\n",
        "    depth = 5\n",
        "         \n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train):      \n",
        "#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\n",
        "      model = build_model()\n",
        "      model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[mean_iou, \"binary_crossentropy\"])\n",
        "\n",
        "      early_stopping = EarlyStopping(patience=10, verbose=1)\n",
        "      model_checkpoint = ModelCheckpoint(\"./model/keras_%d.model\"%fold_index, save_best_only=True, verbose=1)\n",
        "      reduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "      epochs = 20000\n",
        "      batch_size = 32\n",
        "\n",
        "      history = model.fit_generator(\n",
        "          build_train_generator(X_train[train_index], y_train[train_index], batch_size),\n",
        "          validation_data=(X_train[test_index], y_train[test_index]),\n",
        "          steps_per_epoch=np.ceil(len(X_train[train_index]) / batch_size), \n",
        "          epochs=epochs,\n",
        "          callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "#           callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "      \n",
        "      best_idx = np.argmin(history.history['val_loss'])\n",
        "      val_loss = history.history['val_loss'][best_idx]\n",
        "      val_mean_iou = history.history['val_mean_iou'][best_idx]\n",
        "      \n",
        "      print(\"Best val_loss: %.4f, val_mean_iou: %.4f\"%(val_loss, val_mean_iou))\n",
        "      loss.append(val_loss)\n",
        "      metric.append(val_mean_iou)\n",
        "      \n",
        "      model = keras.models.load_model('model/keras_%d.model'%fold_index, custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss, 'dice_loss': dice_loss})\n",
        "      pred_test += model.predict(X_test) / fold\n",
        "      fold_index = fold_index + 1\n",
        "      \n",
        "      del model\n",
        "      gc.collect()\n",
        "      \n",
        "      if fast:\n",
        "        break\n",
        "      \n",
        "      \n",
        "    print('%d fold val_loss: %.4f, val_mean_iou: %.4f'%(fold, np.mean(loss), np.mean(metric)))\n",
        "    return pred_test, np.mean(loss), np.mean(metric)\n",
        "\n",
        "  \n",
        "pred_test, val_loss, val_mean_iou = cv(X_train, y_train, X_test, 5, fast=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20000\n",
            "100/100 [==============================] - 86s 859ms/step - loss: 0.5180 - mean_iou: 0.1783 - binary_crossentropy: 0.5180 - val_loss: 0.9916 - val_mean_iou: 0.0225 - val_binary_crossentropy: 0.9916\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.99164, saving model to ./model/keras_0.model\n",
            "Epoch 2/20000\n",
            "100/100 [==============================] - 68s 684ms/step - loss: 0.3656 - mean_iou: 0.4100 - binary_crossentropy: 0.3656 - val_loss: 0.8975 - val_mean_iou: 0.2701 - val_binary_crossentropy: 0.8975\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.99164 to 0.89751, saving model to ./model/keras_0.model\n",
            "Epoch 3/20000\n",
            "100/100 [==============================] - 68s 677ms/step - loss: 0.3169 - mean_iou: 0.4714 - binary_crossentropy: 0.3169 - val_loss: 2.3119 - val_mean_iou: 0.0882 - val_binary_crossentropy: 2.3119\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.89751\n",
            "Epoch 4/20000\n",
            "100/100 [==============================] - 68s 684ms/step - loss: 0.2889 - mean_iou: 0.5080 - binary_crossentropy: 0.2889 - val_loss: 0.4513 - val_mean_iou: 0.4614 - val_binary_crossentropy: 0.4513\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.89751 to 0.45128, saving model to ./model/keras_0.model\n",
            "Epoch 5/20000\n",
            "100/100 [==============================] - 68s 678ms/step - loss: 0.2704 - mean_iou: 0.5337 - binary_crossentropy: 0.2704 - val_loss: 0.4263 - val_mean_iou: 0.3379 - val_binary_crossentropy: 0.4263\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.45128 to 0.42625, saving model to ./model/keras_0.model\n",
            "Epoch 6/20000\n",
            "100/100 [==============================] - 67s 668ms/step - loss: 0.2684 - mean_iou: 0.5280 - binary_crossentropy: 0.2684 - val_loss: 0.2697 - val_mean_iou: 0.3144 - val_binary_crossentropy: 0.2697\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.42625 to 0.26974, saving model to ./model/keras_0.model\n",
            "Epoch 7/20000\n",
            "100/100 [==============================] - 67s 674ms/step - loss: 0.2595 - mean_iou: 0.5395 - binary_crossentropy: 0.2595 - val_loss: 0.2778 - val_mean_iou: 0.4550 - val_binary_crossentropy: 0.2778\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.26974\n",
            "Epoch 8/20000\n",
            "100/100 [==============================] - 67s 667ms/step - loss: 0.2610 - mean_iou: 0.5484 - binary_crossentropy: 0.2610 - val_loss: 0.2335 - val_mean_iou: 0.5744 - val_binary_crossentropy: 0.2335\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.26974 to 0.23352, saving model to ./model/keras_0.model\n",
            "Epoch 9/20000\n",
            "100/100 [==============================] - 67s 673ms/step - loss: 0.2352 - mean_iou: 0.5591 - binary_crossentropy: 0.2352 - val_loss: 0.2013 - val_mean_iou: 0.6370 - val_binary_crossentropy: 0.2013\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.23352 to 0.20130, saving model to ./model/keras_0.model\n",
            "Epoch 10/20000\n",
            "100/100 [==============================] - 66s 657ms/step - loss: 0.2342 - mean_iou: 0.5700 - binary_crossentropy: 0.2342 - val_loss: 0.2786 - val_mean_iou: 0.5359 - val_binary_crossentropy: 0.2786\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.20130\n",
            "Epoch 11/20000\n",
            "100/100 [==============================] - 66s 657ms/step - loss: 0.2332 - mean_iou: 0.5741 - binary_crossentropy: 0.2332 - val_loss: 0.2854 - val_mean_iou: 0.4415 - val_binary_crossentropy: 0.2854\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.20130\n",
            "Epoch 12/20000\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.2318 - mean_iou: 0.5688 - binary_crossentropy: 0.2318 - val_loss: 0.2764 - val_mean_iou: 0.4691 - val_binary_crossentropy: 0.2764\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.20130\n",
            "Epoch 13/20000\n",
            "100/100 [==============================] - 66s 658ms/step - loss: 0.2230 - mean_iou: 0.5834 - binary_crossentropy: 0.2230 - val_loss: 0.1877 - val_mean_iou: 0.6229 - val_binary_crossentropy: 0.1877\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.20130 to 0.18766, saving model to ./model/keras_0.model\n",
            "Epoch 14/20000\n",
            "100/100 [==============================] - 66s 658ms/step - loss: 0.2172 - mean_iou: 0.5956 - binary_crossentropy: 0.2172 - val_loss: 0.2197 - val_mean_iou: 0.6093 - val_binary_crossentropy: 0.2197\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.18766\n",
            "Epoch 15/20000\n",
            "100/100 [==============================] - 66s 658ms/step - loss: 0.2352 - mean_iou: 0.5845 - binary_crossentropy: 0.2352 - val_loss: 0.2161 - val_mean_iou: 0.6008 - val_binary_crossentropy: 0.2161\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.18766\n",
            "Epoch 16/20000\n",
            "100/100 [==============================] - 68s 677ms/step - loss: 0.2209 - mean_iou: 0.5808 - binary_crossentropy: 0.2209 - val_loss: 0.2115 - val_mean_iou: 0.6006 - val_binary_crossentropy: 0.2115\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.18766\n",
            "Epoch 17/20000\n",
            "100/100 [==============================] - 67s 675ms/step - loss: 0.2073 - mean_iou: 0.6036 - binary_crossentropy: 0.2073 - val_loss: 0.2059 - val_mean_iou: 0.5624 - val_binary_crossentropy: 0.2059\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.18766\n",
            "Epoch 18/20000\n",
            "100/100 [==============================] - 68s 677ms/step - loss: 0.2005 - mean_iou: 0.6049 - binary_crossentropy: 0.2005 - val_loss: 0.1813 - val_mean_iou: 0.5900 - val_binary_crossentropy: 0.1813\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.18766 to 0.18131, saving model to ./model/keras_0.model\n",
            "Epoch 19/20000\n",
            "100/100 [==============================] - 68s 684ms/step - loss: 0.1932 - mean_iou: 0.6250 - binary_crossentropy: 0.1932 - val_loss: 0.2186 - val_mean_iou: 0.6005 - val_binary_crossentropy: 0.2186\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.18131\n",
            "Epoch 20/20000\n",
            "100/100 [==============================] - 67s 667ms/step - loss: 0.1911 - mean_iou: 0.6209 - binary_crossentropy: 0.1911 - val_loss: 0.1802 - val_mean_iou: 0.6185 - val_binary_crossentropy: 0.1802\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.18131 to 0.18015, saving model to ./model/keras_0.model\n",
            "Epoch 21/20000\n",
            "100/100 [==============================] - 68s 684ms/step - loss: 0.1918 - mean_iou: 0.6201 - binary_crossentropy: 0.1918 - val_loss: 0.1638 - val_mean_iou: 0.6410 - val_binary_crossentropy: 0.1638\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.18015 to 0.16384, saving model to ./model/keras_0.model\n",
            "Epoch 22/20000\n",
            "100/100 [==============================] - 67s 673ms/step - loss: 0.1949 - mean_iou: 0.6217 - binary_crossentropy: 0.1949 - val_loss: 0.1839 - val_mean_iou: 0.5214 - val_binary_crossentropy: 0.1839\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.16384\n",
            "Epoch 23/20000\n",
            "100/100 [==============================] - 68s 678ms/step - loss: 0.1878 - mean_iou: 0.6201 - binary_crossentropy: 0.1878 - val_loss: 0.2130 - val_mean_iou: 0.6339 - val_binary_crossentropy: 0.2130\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.16384\n",
            "Epoch 24/20000\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.1771 - mean_iou: 0.6469 - binary_crossentropy: 0.1771 - val_loss: 0.2001 - val_mean_iou: 0.6345 - val_binary_crossentropy: 0.2001\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.16384\n",
            "Epoch 25/20000\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.1728 - mean_iou: 0.6416 - binary_crossentropy: 0.1728 - val_loss: 0.1569 - val_mean_iou: 0.6687 - val_binary_crossentropy: 0.1569\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.16384 to 0.15688, saving model to ./model/keras_0.model\n",
            "Epoch 26/20000\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.1663 - mean_iou: 0.6521 - binary_crossentropy: 0.1663 - val_loss: 0.1873 - val_mean_iou: 0.5354 - val_binary_crossentropy: 0.1873\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.15688\n",
            "Epoch 27/20000\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.1698 - mean_iou: 0.6504 - binary_crossentropy: 0.1698 - val_loss: 0.1706 - val_mean_iou: 0.5415 - val_binary_crossentropy: 0.1706\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.15688\n",
            "Epoch 28/20000\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.1663 - mean_iou: 0.6552 - binary_crossentropy: 0.1663 - val_loss: 0.1606 - val_mean_iou: 0.7028 - val_binary_crossentropy: 0.1606\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.15688\n",
            "Epoch 29/20000\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.1700 - mean_iou: 0.6577 - binary_crossentropy: 0.1700 - val_loss: 0.1617 - val_mean_iou: 0.6609 - val_binary_crossentropy: 0.1617\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.15688\n",
            "Epoch 30/20000\n",
            "100/100 [==============================] - 67s 674ms/step - loss: 0.1604 - mean_iou: 0.6628 - binary_crossentropy: 0.1604 - val_loss: 0.1932 - val_mean_iou: 0.5889 - val_binary_crossentropy: 0.1932\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.15688\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 31/20000\n",
            "100/100 [==============================] - 68s 677ms/step - loss: 0.1432 - mean_iou: 0.6891 - binary_crossentropy: 0.1432 - val_loss: 0.1497 - val_mean_iou: 0.7119 - val_binary_crossentropy: 0.1497\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.15688 to 0.14968, saving model to ./model/keras_0.model\n",
            "Epoch 32/20000\n",
            "100/100 [==============================] - 67s 674ms/step - loss: 0.1420 - mean_iou: 0.6990 - binary_crossentropy: 0.1420 - val_loss: 0.1448 - val_mean_iou: 0.7124 - val_binary_crossentropy: 0.1448\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.14968 to 0.14483, saving model to ./model/keras_0.model\n",
            "Epoch 33/20000\n",
            "100/100 [==============================] - 68s 679ms/step - loss: 0.1345 - mean_iou: 0.6968 - binary_crossentropy: 0.1345 - val_loss: 0.1534 - val_mean_iou: 0.7015 - val_binary_crossentropy: 0.1534\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.14483\n",
            "Epoch 34/20000\n",
            "100/100 [==============================] - 67s 671ms/step - loss: 0.1334 - mean_iou: 0.7021 - binary_crossentropy: 0.1334 - val_loss: 0.1383 - val_mean_iou: 0.7106 - val_binary_crossentropy: 0.1383\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.14483 to 0.13829, saving model to ./model/keras_0.model\n",
            "Epoch 35/20000\n",
            "100/100 [==============================] - 68s 678ms/step - loss: 0.1334 - mean_iou: 0.7028 - binary_crossentropy: 0.1334 - val_loss: 0.1358 - val_mean_iou: 0.7099 - val_binary_crossentropy: 0.1358\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.13829 to 0.13579, saving model to ./model/keras_0.model\n",
            "Epoch 36/20000\n",
            "100/100 [==============================] - 68s 677ms/step - loss: 0.1341 - mean_iou: 0.7071 - binary_crossentropy: 0.1341 - val_loss: 0.1350 - val_mean_iou: 0.7159 - val_binary_crossentropy: 0.1350\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.13579 to 0.13502, saving model to ./model/keras_0.model\n",
            "Epoch 37/20000\n",
            "100/100 [==============================] - 68s 677ms/step - loss: 0.1260 - mean_iou: 0.7095 - binary_crossentropy: 0.1260 - val_loss: 0.1461 - val_mean_iou: 0.7209 - val_binary_crossentropy: 0.1461\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.13502\n",
            "Epoch 38/20000\n",
            "100/100 [==============================] - 67s 671ms/step - loss: 0.1298 - mean_iou: 0.7093 - binary_crossentropy: 0.1298 - val_loss: 0.1360 - val_mean_iou: 0.7253 - val_binary_crossentropy: 0.1360\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.13502\n",
            "Epoch 39/20000\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.1254 - mean_iou: 0.7088 - binary_crossentropy: 0.1254 - val_loss: 0.1373 - val_mean_iou: 0.7110 - val_binary_crossentropy: 0.1373\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.13502\n",
            "Epoch 40/20000\n",
            "100/100 [==============================] - 66s 662ms/step - loss: 0.1210 - mean_iou: 0.7075 - binary_crossentropy: 0.1210 - val_loss: 0.1519 - val_mean_iou: 0.6915 - val_binary_crossentropy: 0.1519\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.13502\n",
            "Epoch 41/20000\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.1149 - mean_iou: 0.7104 - binary_crossentropy: 0.1149 - val_loss: 0.1482 - val_mean_iou: 0.6982 - val_binary_crossentropy: 0.1482\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.13502\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 42/20000\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.1145 - mean_iou: 0.7274 - binary_crossentropy: 0.1145 - val_loss: 0.1353 - val_mean_iou: 0.7306 - val_binary_crossentropy: 0.1353\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.13502\n",
            "Epoch 43/20000\n",
            "100/100 [==============================] - 66s 658ms/step - loss: 0.1066 - mean_iou: 0.7323 - binary_crossentropy: 0.1066 - val_loss: 0.1397 - val_mean_iou: 0.7242 - val_binary_crossentropy: 0.1397\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.13502\n",
            "Epoch 44/20000\n",
            "100/100 [==============================] - 66s 663ms/step - loss: 0.1021 - mean_iou: 0.7322 - binary_crossentropy: 0.1021 - val_loss: 0.1477 - val_mean_iou: 0.7314 - val_binary_crossentropy: 0.1477\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.13502\n",
            "Epoch 45/20000\n",
            "100/100 [==============================] - 67s 674ms/step - loss: 0.1034 - mean_iou: 0.7323 - binary_crossentropy: 0.1034 - val_loss: 0.1406 - val_mean_iou: 0.6637 - val_binary_crossentropy: 0.1406\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.13502\n",
            "Epoch 46/20000\n",
            "100/100 [==============================] - 67s 673ms/step - loss: 0.1075 - mean_iou: 0.7304 - binary_crossentropy: 0.1075 - val_loss: 0.1426 - val_mean_iou: 0.7050 - val_binary_crossentropy: 0.1426\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.13502\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 00046: early stopping\n",
            "Best val_loss: 0.1350, val_mean_iou: 0.7159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e4e8b77388ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-e4e8b77388ec>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(X_train, y_train, X_test, fold, fast)\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_mean_iou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/keras_%d.model'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'mean_iou'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmean_iou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iou_bce_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miou_bce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dice_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m       \u001b[0mpred_test\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0mfold_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'iou_bce_loss' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HRemgbrVOpjK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'][1:])\n",
        "plt.plot(history.history['val_loss'][1:])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNqZwFOebfzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# best_idx = np.argmin(history.history['val_loss'])\n",
        "# val_loss = history.history['val_loss'][best_idx]\n",
        "# val_mean_iou = history.history['val_mean_iou'][best_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgBEjwKHCn1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('model/keras.model', custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EqepTCQa1djX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RLenc(img, order='F', format=True):\n",
        "    \"\"\"\n",
        "    img is binary mask image, shape (r,c)\n",
        "    order is down-then-right, i.e. Fortran\n",
        "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
        "\n",
        "    returns run length as an array or string (if format is True)\n",
        "    \"\"\"\n",
        "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
        "    runs = []  ## list of run lengths\n",
        "    r = 0  ## the current run length\n",
        "    pos = 1  ## count starts from 1 per WK\n",
        "    for c in bytes:\n",
        "        if (c == 0):\n",
        "            if r != 0:\n",
        "                runs.append((pos, r))\n",
        "                pos += r\n",
        "                r = 0\n",
        "            pos += 1\n",
        "        else:\n",
        "            r += 1\n",
        "\n",
        "    # if last run is unsaved (i.e. data ends with 1)\n",
        "    if r != 0:\n",
        "        runs.append((pos, r))\n",
        "        pos += r\n",
        "        r = 0\n",
        "\n",
        "    if format:\n",
        "        z = ''\n",
        "\n",
        "        for rr in runs:\n",
        "            z += '{} {} '.format(rr[0], rr[1])    \n",
        "        return z[:-1]\n",
        "    else:\n",
        "        return runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5wnyMSkd50M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class FasterRle(object):\n",
        "#     \"\"\"Perform RLE in paralell.\"\"\"\n",
        "\n",
        "#     def __init__(self, num_consumers=2):\n",
        "#         \"\"\"Initialize class.\"\"\"\n",
        "#         self._tasks = multiprocessing.JoinableQueue()\n",
        "#         self._results = multiprocessing.Queue()\n",
        "#         self._n_consumers = num_consumers\n",
        "\n",
        "#         # Initialize consumers\n",
        "#         self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n",
        "#         for w in self._consumers:\n",
        "#             w.start()\n",
        "\n",
        "#     def add(self, array, startIndex):\n",
        "#         \"\"\"Add a task to perform.\"\"\"\n",
        "#         self._tasks.put(FasterTask(array, startIndex))\n",
        "\n",
        "#     def get_results(self):\n",
        "#         \"\"\"Close all tasks.\"\"\"\n",
        "#         # Provide poison pill\n",
        "#         [self._tasks.put(None) for _ in range(self._n_consumers)]\n",
        "#         # Wait for finish\n",
        "#         self._tasks.join()\n",
        "#         # Return results\n",
        "#         singles = []\n",
        "#         while not self._results.empty():\n",
        "#             singles.append(self._results.get())\n",
        "            \n",
        "#         resultDic = dict()\n",
        "#         for rles, start in singles:\n",
        "#             #print('start:', start)\n",
        "#             for i,rle in enumerate(rles):\n",
        "#                 #print('i:', i)\n",
        "#                 resultDic[str(start+i)] = rle\n",
        "#         return resultDic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7wRK-ZUEmzA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch(func):\n",
        "  def batch_func(b):\n",
        "    return np.array([func(e) for e in tqdm(b)])\n",
        "  return batch_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sy0eHi4-P5PP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preds_test = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hIcfQ4_pPlHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e52d1ce9-b9b2-4fdf-f039-e606626a790a"
      },
      "cell_type": "code",
      "source": [
        "test['rle_mask'] = batch(RLenc)(np.round(pred_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18000/18000 [00:58<00:00, 309.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ST2JPIMLP666",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7aaf49b7-1c59-4efc-cfb8-2bdd7e3a885f"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_FILE = 'keras_unet_sc32de5wbecjaccloss_%.4f_%.4f.csv'%(val_loss, val_mean_iou)\n",
        "test.to_csv(SUBMISSION_FILE)\n",
        "SUBMISSION_FILE"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'keras_unet_sc32de5wbecjaccloss_0.1320_0.7477.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "7iJCa8ZqRppy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cc737439-5883-4295-c670-658d69aaff48"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_MESSAGE='\"Keras U-net iou_bce_loss: val_loss: %.4f, val_mean_iou: %.4f\"'%(val_loss, val_mean_iou)\n",
        "!kaggle competitions submit -f '{SUBMISSION_FILE}' -m '{SUBMISSION_MESSAGE}'"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using competition: tgs-salt-identification-challenge\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.4.6)\n",
            "Successfully submitted to TGS Salt Identification Challenge"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ql1uneNwTuEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "c9be94ee-85ec-4429-d351-98436cfaf45f"
      },
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv('.env')\n",
        "!gsutil rsync model gs://{os.environ['GCP_BUCKET']}/model"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "Copying file://model/keras_0.model [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "Copying file://model/keras_1.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_2.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_3.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m -o ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying file://model/keras_4.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "Operation completed over 5 objects/751.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6UwBwalwuZpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "bad2519d-f469-4e15-ce5c-1a921ed92c84"
      },
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# def batch_iou(pred, truth):\n",
        "#     pred_ = tf.round(pred)\n",
        "#     truth_ = truth\n",
        "    \n",
        "#     i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "#     u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "#     epsilon = tf.keras.backend.epsilon()\n",
        "#     return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "  \n",
        "# def mean_iou(pred, truth):\n",
        "#     ious = batch_iou(pred, truth)\n",
        "#     ious = tf.reshape(ious, (-1,1))\n",
        "#     ious = tf.tile(ious, (1,10))\n",
        "#     thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "#     return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "# BATCH_SIZE = 100\n",
        "# ones = tf.constant(np.ones((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# zeros = tf.constant(np.zeros((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# half = tf.coonstant(np.zeros(BATCH_SIZE, ))\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "  \n",
        "#   print(mean_iou(ones, zeros).eval())\n",
        "#   print(mean_iou(ones, ones).eval())\n",
        "#   print(mean_iou(zeros, ones).eval())\n",
        "#   print(mean_iou(zeros, zeros).eval())\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1AayNFWou9ms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}