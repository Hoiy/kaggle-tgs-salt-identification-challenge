{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_tpu_unet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Hoiy/kaggle-tgs-salt-identification-challenge/blob/master/model_tpu_unet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "RgW68azy-I2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "df5cfde0-7f65-4cc7-c14b-e47526a562e0"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import dotenv\n",
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "\n",
        "dotenv.load_dotenv('.env')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "tQMYWUFxAqcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8522aba-7e04-4cf5-d602-37efb4cf3c0f"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('raw/train.csv').set_index('id')\n",
        "test = pd.read_csv('raw/sample_submission.csv').set_index('id')\n",
        "depths = pd.read_csv('raw/depths.csv').set_index('id')\n",
        "\n",
        "train = pd.merge(train, depths, left_index=True, right_index=True, how='inner')\n",
        "train.shape, test.shape, depths.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 2), (18000, 1), (22000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "RWGJQRBXAqxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Whe2VDUGAquf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "f85310c9-4144-47b5-a828-aad8e112f407"
      },
      "cell_type": "code",
      "source": [
        "X_train = [cv2.resize(cv2.imread(\"./raw/images/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(train.index)]\n",
        "X_train = np.expand_dims(np.array(X_train), -1)\n",
        "X_train = X_train / 255\n",
        "X_train = X_train.astype(np.float32)\n",
        "\n",
        "y_train = [cv2.resize(cv2.imread(\"./raw/masks/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(train.index)]\n",
        "y_train = np.expand_dims(np.array(y_train), -1)\n",
        "y_train = y_train / 255\n",
        "y_train = y_train.astype(np.float32)\n",
        "\n",
        "\n",
        "X_test = [cv2.resize(cv2.imread(\"./raw/images/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(test.index)]\n",
        "X_test = np.expand_dims(np.array(X_test), -1)\n",
        "X_test = X_test / 255\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4000/4000 [00:01<00:00, 2519.68it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 8365.24it/s]\n",
            "100%|██████████| 18000/18000 [00:06<00:00, 2669.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 128, 128, 1), (4000, 128, 128, 1), (18000, 128, 128, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "9D2pdhBiAqrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7151
        },
        "outputId": "2af45ca6-8a4a-4bfe-edf0-7124786f2d3f"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "# from keras import Model\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "# from keras.models import load_model\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, SpatialDropout2D, UpSampling2D, Activation, Add\n",
        "\n",
        "# from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return 1. - score\n",
        "  \n",
        "  \n",
        "\n",
        "# define iou or jaccard loss function\n",
        "def jaccard_loss(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    \n",
        "    score = (intersection + epsilon) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + epsilon)\n",
        "    return 1 - score\n",
        "\n",
        "\n",
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    gts = tf.reduce_sum(gt_sorted)\n",
        "    intersection = gts - tf.cumsum(gt_sorted)\n",
        "    union = gts + tf.cumsum(1. - gt_sorted)\n",
        "    jaccard = 1. - intersection / union\n",
        "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
        "    return jaccard\n",
        "  \n",
        "  \n",
        "  \n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        def treat_image(log_lab):\n",
        "            log, lab = log_lab\n",
        "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
        "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
        "            return lovasz_hinge_flat(log, lab)\n",
        "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
        "        loss = tf.reduce_mean(losses)\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_loss():\n",
        "        labelsf = tf.cast(labels, logits.dtype)\n",
        "        signs = 2. * labelsf - 1.\n",
        "        errors = 1. - logits * tf.stop_gradient(signs)\n",
        "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
        "        gt_sorted = tf.gather(labelsf, perm)\n",
        "        grad = lovasz_grad(gt_sorted)\n",
        "        #loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        loss = tf.tensordot(tf.nn.elu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        return loss\n",
        "\n",
        "    # deal with the void prediction case (only void pixels)\n",
        "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
        "                   lambda: tf.reduce_sum(logits) * 0.,\n",
        "                   compute_loss,\n",
        "                   strict=True,\n",
        "                   name=\"loss\"\n",
        "                   )\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = tf.reshape(scores, (-1,))\n",
        "    labels = tf.reshape(labels, (-1,))\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = tf.not_equal(labels, ignore)\n",
        "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
        "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
        "    return vscores, vlabels\n",
        "\n",
        "def lovasz_loss(y_true, y_pred):\n",
        "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
        "    #logits = K.log(y_pred / (1. - y_pred))\n",
        "    logits = y_pred #Jiaxin\n",
        "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
        "    return loss\n",
        "  \n",
        "\n",
        "# def iou_bce_loss(y_true, y_pred):\n",
        "#     return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.to_float(tf.greater(tf.round(y_true), y_pred)))\n",
        "    \n",
        "\n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     i = tf.round(y_pred) * tf.round(y_true)\n",
        "#     o = tf.round(y_pred) * tf.round(y_true)\n",
        "#     mean_iou = tf.metrics.mean_iou(y_true, tf.round(y_pred), 1)\n",
        "#     print(mean_iou)\n",
        "#     print(tf.tile(mean_iou, 10))\n",
        "#     print(mean_iou[0])\n",
        "#     return mean_iou[0]\n",
        "#     return tf.reduce_mean(tf.tile(mean_iou, 10))\n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     prec = []\n",
        "#     for t in np.arange(0.5, 1.0, 0.05):\n",
        "#         y_pred_ = tf.to_int32(y_pred > t)\n",
        "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         with tf.control_dependencies([up_opt]):\n",
        "#             score = tf.identity(score)\n",
        "#         prec.append(score)\n",
        "#     return K.mean(K.stack(prec), axis=0)\n",
        "\n",
        "\n",
        "# def convolution_block(x, filters, size):\n",
        "#     return x\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# def residual_block(blockInput, filters):\n",
        "#     x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(blockInput)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(x)\n",
        "#     x = Add()([x, blockInput])\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     return x\n",
        "  \n",
        "  \n",
        "# def res_block(i, filters=16):\n",
        "#     n1 = Conv2D(filters, 1, activation='relu', padding='same')(i)\n",
        "#     n3 = Conv2D(filters, 3, activation='relu', padding='same')(i)\n",
        "#     n5 = Conv2D(filters, 5, activation='relu', padding='same')(i)\n",
        "#     n = Concatenate()([n1, n3, n5])\n",
        "#     n = BatchNormalization()(n)\n",
        "    \n",
        "#     n1 = Conv2D(filters, 1, activation='relu', padding='same')(n)\n",
        "#     n3 = Conv2D(filters, 3, activation='relu', padding='same')(n)\n",
        "#     n5 = Conv2D(filters, 5, activation='relu', padding='same')(n)\n",
        "#     n = Concatenate()([n1, n3, n5])\n",
        "#     n = BatchNormalization()(n)\n",
        "    \n",
        "# #     n = Conv2D(1, 1, activation='relu', padding='same')(n)\n",
        "# #     n = Concatenate()([i, n])\n",
        "# #     n = BatchNormalization()(n)\n",
        "#     return n\n",
        "\n",
        "  \n",
        "# def level_block(inp, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "#     if depth > 0:\n",
        "#         n = conv_block(m, dim, acti, bn, res)\n",
        "#         m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "#         m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
        "#         if up:\n",
        "#             m = UpSampling2D()(m)\n",
        "#             m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "#         else:\n",
        "# #             padding = ['valid', 'same','same','valid','same','valid']\n",
        "#             padding = ['same','same','valid','same','valid']\n",
        "#             m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding=padding[depth-1])(m)\n",
        "#         n = Concatenate()([n, m])\n",
        "#         m = conv_block(n, dim, acti, bn, res)\n",
        "#     else:\n",
        "#         m = conv_block(m, dim, acti, bn, res, do=do)\n",
        "#     return m\n",
        "\n",
        "  \n",
        "# def ResNet():\n",
        "#     filter_size = 8\n",
        "#     i = Input(shape=(img_size_target,img_size_target,1))\n",
        "#     o = Conv2D(filter_size, 1, strides=(1,1), padding='same', activation='relu')(i)\n",
        "#     for depth in tqdm(range(128)):\n",
        "#       o = residual_block(o, filter_size)\n",
        "#     o = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(o)\n",
        "#     return Model(inputs=i, outputs=o)\n",
        "  \n",
        "  \n",
        "# def ResNet_2():\n",
        "#     resnet = keras.applications.resnet50.ResNet50(include_top=False, pooling=None, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "#     o = Conv2D(1, 1, strides=(1,1), activation='sigmoid')(resnet.output)\n",
        "#     return Model(inputs=resnet.input, outputs=o)\n",
        "  \n",
        "def res_block(inputs, filters):\n",
        "    inputs = BatchNormalization()(inputs)\n",
        "    conv = Conv2D(filters, 3, strides=1, padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = Conv2D(filters, 3, strides=1, padding='same', activation='relu')(conv)\n",
        "    \n",
        "    conv = Add()([conv, inputs])\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    return Conv2D(filters, 3, strides=2, padding='same', activation='relu')(conv)\n",
        "  \n",
        "\n",
        "def res_block_r(inputs, filters, res):\n",
        "    inputs = Add()([inputs, res])\n",
        "    inputs = BatchNormalization()(inputs)\n",
        "    conv = Conv2DTranspose(filters, 3, strides=1, padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = Conv2DTranspose(filters, 3, strides=1, padding='same', activation='relu')(conv)\n",
        "    \n",
        "    conv = Add()([conv, inputs])\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    return Conv2DTranspose(filters, 3, strides=2, padding='same', activation='relu')(conv)\n",
        "  \n",
        "  \n",
        "# def build_model(start_channels=8, depth=6):\n",
        "#     inp = Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "#     l = BatchNormalization()(inp)\n",
        "#     l = Conv2D(start_channels, 3, strides=1, padding='same', activation='relu')(l)\n",
        "    \n",
        "#     layers = [l]\n",
        "    \n",
        "#     for i in range(depth):\n",
        "#       layers.append(res_block(layers[-1], start_channels * (2 ** i)))\n",
        "      \n",
        "      \n",
        "#     for i in range(depth):\n",
        "#       layers.append(res_block_r(layers[-1], start_channels * (2 ** (depth - 1 - i)), layers[-2*i-1]))\n",
        "      \n",
        "#     l = BatchNormalization()(layers[-1])\n",
        "#     out = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(l)\n",
        "    \n",
        "#     return Model(inputs=inp, outputs=out)\n",
        "\n",
        "def UNet(inputs, channels, depth):\n",
        "    layers = [inputs]\n",
        "    \n",
        "    for i in range(depth):\n",
        "      layers.append(res_block(layers[-1], channels))\n",
        "      \n",
        "      \n",
        "    for i in range(depth):\n",
        "      layers.append(res_block_r(layers[-1], channels, layers[-2*i-1]))\n",
        "    \n",
        "    return layers[-1]\n",
        "    \n",
        "    \n",
        "\n",
        "def build_model(channels=64, depth=6):\n",
        "    inp = keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "    l = BatchNormalization()(inp)\n",
        "    l = Conv2D(channels, 3, strides=1, padding='same', activation='relu')(l)\n",
        "    \n",
        "    l = UNet(l, channels, depth)\n",
        "    l = UNet(l, channels, depth)\n",
        "    \n",
        "    l = BatchNormalization()(l)\n",
        "    out = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(l)\n",
        "    \n",
        "    return keras.Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "  \n",
        "build_model().summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 128, 128, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 128, 128, 1)  4           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 128, 128, 64) 640         batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 128, 128, 64) 0           conv2d_2[0][0]                   \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 64, 64, 64)   0           conv2d_5[0][0]                   \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           conv2d_8[0][0]                   \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36928       batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           conv2d_11[0][0]                  \n",
            "                                                                 batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 64)     0           conv2d_14[0][0]                  \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 64)     256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 4, 4, 64)     256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 4, 4, 64)     0           conv2d_17[0][0]                  \n",
            "                                                                 batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 4, 4, 64)     256         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 2, 2, 64)     36928       batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 2, 2, 64)     0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 2, 2, 64)     256         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 2, 2, 64)     36928       batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 2, 2, 64)     256         conv2d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 2, 2, 64)     36928       batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 2, 2, 64)     0           conv2d_transpose_1[0][0]         \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 2, 2, 64)     256         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 4, 4, 64)     0           conv2d_transpose_2[0][0]         \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4, 4, 64)     256         add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 64)     256         conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 4, 4, 64)     0           conv2d_transpose_4[0][0]         \n",
            "                                                                 batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 4, 4, 64)     256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 8, 8, 64)     0           conv2d_transpose_5[0][0]         \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 64)     256         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 64)     256         conv2d_transpose_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 8, 8, 64)     0           conv2d_transpose_7[0][0]         \n",
            "                                                                 batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 64)     256         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 16, 16, 64)   36928       batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_8[0][0]         \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 64)   256         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTrans (None, 16, 16, 64)   36928       batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 64)   256         conv2d_transpose_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DTran (None, 16, 16, 64)   36928       batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_10[0][0]        \n",
            "                                                                 batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 64)   256         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_11 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 32, 32, 64)   0           conv2d_transpose_11[0][0]        \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 32, 64)   256         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 64)   256         conv2d_transpose_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 32, 32, 64)   0           conv2d_transpose_13[0][0]        \n",
            "                                                                 batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 64)   256         add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_14 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 64, 64, 64)   0           conv2d_transpose_14[0][0]        \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 64, 64, 64)   256         add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_15 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 64, 64, 64)   256         conv2d_transpose_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_16 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 64, 64, 64)   0           conv2d_transpose_16[0][0]        \n",
            "                                                                 batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 64, 64, 64)   256         add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_17 (Conv2DTran (None, 128, 128, 64) 36928       batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 128, 128, 64) 256         conv2d_transpose_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 128, 128, 64) 36928       batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 128, 128, 64) 256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 128, 128, 64) 36928       batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 128, 128, 64) 0           conv2d_20[0][0]                  \n",
            "                                                                 batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 128, 128, 64) 256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 64, 64, 64)   36928       batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 64, 64, 64)   256         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 64, 64, 64)   36928       batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 64, 64, 64)   256         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 64, 64, 64)   36928       batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 64, 64, 64)   0           conv2d_23[0][0]                  \n",
            "                                                                 batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 64, 64, 64)   256         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 32, 32, 64)   36928       batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 32, 32, 64)   256         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 32, 32, 64)   36928       batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 32, 32, 64)   256         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 32, 32, 64)   36928       batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 32, 32, 64)   0           conv2d_26[0][0]                  \n",
            "                                                                 batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 32, 32, 64)   256         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 64)   256         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 16, 16, 64)   256         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 16, 16, 64)   0           conv2d_29[0][0]                  \n",
            "                                                                 batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 16, 16, 64)   256         add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 64)     256         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 64)     256         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 8, 8, 64)     0           conv2d_32[0][0]                  \n",
            "                                                                 batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 64)     256         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 4, 64)     256         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 4, 4, 64)     256         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 4, 4, 64)     0           conv2d_35[0][0]                  \n",
            "                                                                 batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 4, 4, 64)     256         add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 2, 2, 64)     36928       batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 2, 2, 64)     0           conv2d_36[0][0]                  \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 2, 2, 64)     256         add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_18 (Conv2DTran (None, 2, 2, 64)     36928       batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 2, 2, 64)     256         conv2d_transpose_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_19 (Conv2DTran (None, 2, 2, 64)     36928       batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 2, 2, 64)     0           conv2d_transpose_19[0][0]        \n",
            "                                                                 batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 2, 2, 64)     256         add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_20 (Conv2DTran (None, 4, 4, 64)     36928       batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 4, 4, 64)     0           conv2d_transpose_20[0][0]        \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 4, 4, 64)     256         add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_21 (Conv2DTran (None, 4, 4, 64)     36928       batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 4, 4, 64)     256         conv2d_transpose_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_22 (Conv2DTran (None, 4, 4, 64)     36928       batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 4, 4, 64)     0           conv2d_transpose_22[0][0]        \n",
            "                                                                 batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 4, 4, 64)     256         add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_23 (Conv2DTran (None, 8, 8, 64)     36928       batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 8, 8, 64)     0           conv2d_transpose_23[0][0]        \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 8, 8, 64)     256         add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_24 (Conv2DTran (None, 8, 8, 64)     36928       batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 8, 8, 64)     256         conv2d_transpose_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_25 (Conv2DTran (None, 8, 8, 64)     36928       batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 8, 8, 64)     0           conv2d_transpose_25[0][0]        \n",
            "                                                                 batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 8, 8, 64)     256         add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_26 (Conv2DTran (None, 16, 16, 64)   36928       batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_26[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 16, 16, 64)   256         add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_27 (Conv2DTran (None, 16, 16, 64)   36928       batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 16, 16, 64)   256         conv2d_transpose_27[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_28 (Conv2DTran (None, 16, 16, 64)   36928       batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_28[0][0]        \n",
            "                                                                 batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 16, 16, 64)   256         add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_29 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 32, 32, 64)   0           conv2d_transpose_29[0][0]        \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 32, 32, 64)   256         add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_30 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 32, 32, 64)   256         conv2d_transpose_30[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_31 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 32, 32, 64)   0           conv2d_transpose_31[0][0]        \n",
            "                                                                 batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 32, 32, 64)   256         add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_32 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_34 (Add)                    (None, 64, 64, 64)   0           conv2d_transpose_32[0][0]        \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 64, 64, 64)   256         add_34[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_33 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 64, 64, 64)   256         conv2d_transpose_33[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_34 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_35 (Add)                    (None, 64, 64, 64)   0           conv2d_transpose_34[0][0]        \n",
            "                                                                 batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 64, 64, 64)   256         add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_35 (Conv2DTran (None, 128, 128, 64) 36928       batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 128, 128, 64) 256         conv2d_transpose_35[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 128, 128, 1)  65          batch_normalization_73[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 2,678,213\n",
            "Trainable params: 2,668,867\n",
            "Non-trainable params: 9,346\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PjE-J85UBkVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2362
        },
        "outputId": "4ca728d7-8bb2-400d-a1aa-da4a810c2ecb"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def build_train_generator(X_train, y_train, batch_size=32):\n",
        "    # Ref: https://keras.io/preprocessing/image/\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(\n",
        "#       rotation_range=360.,\n",
        "#       width_shift_range=0.1,\n",
        "#       height_shift_range=0.1,\n",
        "      fill_mode='reflect',\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "    )\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
        "    seed = 1\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "    \n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "\n",
        "    # combine generators into one which yields image and masks\n",
        "    return zip(image_generator, mask_generator)\n",
        "\n",
        "\n",
        "def cv(X_train, y_train, X_test, fold, fast=False):\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    \n",
        "    loss = []\n",
        "    metric = []\n",
        "    pred_test = np.zeros(X_test.shape)\n",
        "    fold_index = 0\n",
        "    \n",
        "    start_ch = 32\n",
        "    depth = 5\n",
        "         \n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train):      \n",
        "#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\n",
        "      model = build_model()\n",
        "  \n",
        "      resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "      strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "      model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "      session_master = resolver.master()\n",
        "  \n",
        "      model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[mean_iou, \"binary_crossentropy\"])\n",
        "    \n",
        "      EPOCHS = 20000\n",
        "      BATCH_SIZE = 32\n",
        "\n",
        "      \n",
        "      def train_data_fn():\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((X_train[train_index], y_train[train_index]))\n",
        "        dataset = dataset.shuffle(1000).repeat(EPOCHS).batch(BATCH_SIZE, drop_remainder=True)\n",
        "        return dataset\n",
        "      \n",
        "      def val_data_fn():\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((X_train[test_index], y_train[test_index]))\n",
        "        dataset = dataset.shuffle(1000).repeat(EPOCHS).batch(BATCH_SIZE, drop_remainder=True)\n",
        "        return dataset\n",
        "\n",
        "      early_stopping = EarlyStopping(patience=10, verbose=1)\n",
        "      model_checkpoint = ModelCheckpoint(\"./model/keras_%d.model\"%fold_index, save_best_only=True, verbose=1)\n",
        "      reduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "      history = model.fit(\n",
        "          train_data_fn,\n",
        "          steps_per_epoch=int(np.ceil(len(X_train[train_index]) / BATCH_SIZE)),\n",
        "          validation_data=val_data_fn,\n",
        "          validation_steps=int(np.ceil(len(X_train[test_index]) / BATCH_SIZE)),\n",
        "          epochs=EPOCHS,\n",
        "#           callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
        "          callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "      \n",
        "      best_idx = np.argmin(history.history['val_loss'])\n",
        "      val_loss = history.history['val_loss'][best_idx]\n",
        "      val_mean_iou = history.history['val_mean_iou'][best_idx]\n",
        "      \n",
        "      print(\"Best val_loss: %.4f, val_mean_iou: %.4f\"%(val_loss, val_mean_iou))\n",
        "      loss.append(val_loss)\n",
        "      metric.append(val_mean_iou)\n",
        "      \n",
        "      model = keras.models.load_model('model/keras_%d.model'%fold_index, custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss, 'dice_loss': dice_loss})\n",
        "      pred_test += model.predict(X_test) / fold\n",
        "      fold_index = fold_index + 1\n",
        "      \n",
        "      del model\n",
        "      gc.collect()\n",
        "      \n",
        "      if fast:\n",
        "        break\n",
        "      \n",
        "      \n",
        "    print('%d fold val_loss: %.4f, val_mean_iou: %.4f'%(fold, np.mean(loss), np.mean(metric)))\n",
        "    return pred_test, np.mean(loss), np.mean(metric)\n",
        "\n",
        "  \n",
        "pred_test, val_loss, val_mean_iou = cv(X_train, y_train, X_test, 5, fast=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 9419556912317607972)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17935243833086008971)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 1135501846389402646)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13782674558594628466)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2579903752271564830)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 2583214268528571970)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11436533436562884019)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2059104656241905838)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 16827265744955053934)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10847863157422769084)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3836213607464506273)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2640410074856331644)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Epoch 1/20000\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(32, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 128, 128, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_4\n",
            "INFO:tensorflow:Cloning Adam {'amsgrad': False, 'beta_1': 0.8999999761581421, 'decay': 0.0, 'beta_2': 0.9990000128746033, 'epsilon': 1e-07, 'lr': 0.0010000000474974513}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss_3/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 108.55012083053589 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " 99/100 [============================>.] - ETA: 2s - loss: 0.3675 - mean_iou: 0.3875 - binary_crossentropy: 0.3675INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(32, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 128, 128, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_4\n",
            "INFO:tensorflow:Cloning Adam {'amsgrad': False, 'beta_1': 0.8999999761581421, 'decay': 0.0, 'beta_2': 0.9990000128746033, 'epsilon': 1e-07, 'lr': 0.0010000000474974513}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 64.47917532920837 secs\n",
            "100/100 [==============================] - 324s 3s/step - loss: 0.3673 - mean_iou: 0.3875 - binary_crossentropy: 0.3673 - val_loss: 1.9163 - val_mean_iou: 0.0275 - val_binary_crossentropy: 1.9163\n",
            "Epoch 2/20000\n",
            "100/100 [==============================] - 43s 427ms/step - loss: 0.1859 - mean_iou: 0.6033 - binary_crossentropy: 0.1859 - val_loss: 0.9364 - val_mean_iou: 0.1420 - val_binary_crossentropy: 0.9364\n",
            "Epoch 3/20000\n",
            "100/100 [==============================] - 43s 432ms/step - loss: 0.1418 - mean_iou: 0.6685 - binary_crossentropy: 0.1418 - val_loss: 1.8688 - val_mean_iou: 0.3391 - val_binary_crossentropy: 1.8688\n",
            "Epoch 4/20000\n",
            " 10/100 [==>...........................] - ETA: 37s - loss: 0.1428 - mean_iou: 0.6431 - binary_crossentropy: 0.1428"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8a4339b323f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-8a4339b323f0>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(X_train, y_train, X_test, fold, fast)\u001b[0m\n\u001b[1;32m     77\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#           callbacks=[early_stopping, model_checkpoint, reduce_lr],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m       )\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1487\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0;31m# Sample-wise fit loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_step_wise\u001b[0;34m(self, ins, callbacks, steps_per_epoch, epochs, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m           \u001b[0mnext_step_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         outs = f.pipeline_run(cur_step_inputs=ins,\n\u001b[0;32m-> 1631\u001b[0;31m                               next_step_inputs=next_step_inputs)\n\u001b[0m\u001b[1;32m   1632\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         logging.warning('Your dataset iterator ran out of data; '\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mpipeline_run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0mnext_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         ], infeed_dict)\n\u001b[0m\u001b[1;32m   1089\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uf9lolkBBkSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6fwJZzEBkQl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uk1SXFM3BkNT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YywfFfHsBkKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1cd1b79c-0070-4c45-a4af-7c773917d325"
      },
      "cell_type": "code",
      "source": [
        "int(np.ceil(100/10))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "yQXxqr0MAqoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tFBYhgB5NEjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "ef7eacf1-bd31-4fd2-9928-47e64f30f8db"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def test_model():\n",
        "  x = tf.keras.layers.Input(shape=(101,101,1), dtype=np.float)\n",
        "  y = tf.keras.layers.Conv2D(1,1, padding='same', activation='sigmoid', dtype=np.float)(x)\n",
        "  m = tf.keras.Model(x, y)\n",
        "  return m\n",
        "\n",
        "model = test_model()\n",
        "model.summary()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 101, 101, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 101, 101, 1)       2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4tHvN3g6PA92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "d61b4a4d-cffd-4f0f-e562-97b591d47470"
      },
      "cell_type": "code",
      "source": [
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "session_master = resolver.master()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 9419556912317607972)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17935243833086008971)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 1135501846389402646)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13782674558594628466)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2579903752271564830)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 2583214268528571970)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11436533436562884019)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2059104656241905838)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 16827265744955053934)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10847863157422769084)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3836213607464506273)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2640410074856331644)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: grpc://10.240.1.2:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LIyUjH87qLvJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2034
        },
        "outputId": "e268077f-d2ad-4929-c2a8-eef43e5e8190"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def input_fn():\n",
        "  images = tf.random_uniform([1024, 101, 101, 1])\n",
        "  masks = tf.random_uniform([1024, 101, 101, 1])\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
        "  dataset = dataset.shuffle(1000).repeat(100000).batch(32, drop_remainder=True)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01),\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['binary_crossentropy'])\n",
        "\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    input_fn,\n",
        "    validation_data=input_fn,\n",
        "    validation_steps=100,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=20)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 12.161664724349976 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7134 - binary_crossentropy: 0.7134INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 12.039937734603882 secs\n",
            "100/100 [==============================] - 30s 298ms/step - loss: 0.7132 - binary_crossentropy: 0.7132 - val_loss: 0.7087 - val_binary_crossentropy: 0.7087\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 5s 48ms/step - loss: 0.7056 - binary_crossentropy: 0.7056 - val_loss: 0.7031 - val_binary_crossentropy: 0.7031\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 5s 49ms/step - loss: 0.7014 - binary_crossentropy: 0.7014 - val_loss: 0.7000 - val_binary_crossentropy: 0.7000\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 5s 49ms/step - loss: 0.6992 - binary_crossentropy: 0.6992 - val_loss: 0.6984 - val_binary_crossentropy: 0.6984\n",
            "Epoch 5/20\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.6979 - binary_crossentropy: 0.6979"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0db386ba2026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     epochs=20)\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1487\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0;31m# Sample-wise fit loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_step_wise\u001b[0;34m(self, ins, callbacks, steps_per_epoch, epochs, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                                            \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                                            \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                                            verbose=0)\n\u001b[0m\u001b[1;32m   1657\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    402\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1006\u001b[0m           \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m           \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       ], infeed_dict)\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6XdIwEcAqor5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2241
        },
        "outputId": "6f555f74-f366-4eb1-bd79-7687c211c77e"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef expected inputs 'float' do not match 2 inputs specified; Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\"](training/TFOptimizer/gradients/tpu_model_140369801514736/conv2d_1/Conv2D_grad/tuple/control_dependency_1, training/TFOptimizer/CrossReplicaSum/group_assignment)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-959ee3a79e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_sample_wise\u001b[0;34m(self, ins, callbacks, index_array, shuffle, batch_size, num_training_samples, indices_for_conversion_to_dense, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m       outs = f.pipeline_run(cur_step_inputs=ins_last_batch,\n\u001b[0;32m-> 1558\u001b[0;31m                             next_step_inputs=ins_batch)\n\u001b[0m\u001b[1;32m   1559\u001b[0m       \u001b[0mins_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mpipeline_run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1075\u001b[0m           next_input_tensors)\n\u001b[1;32m   1076\u001b[0m       next_tpu_model_ops = self._tpu_model_ops_for_input_specs(\n\u001b[0;32m-> 1077\u001b[0;31m           next_input_specs, next_step_infeed_manager)\n\u001b[0m\u001b[1;32m   1078\u001b[0m       \u001b[0minfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_infeed_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_tpu_model_ops_for_input_specs\u001b[0;34m(self, input_specs, infeed_manager)\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mshape_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_specs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m         logging.info('New input shapes; (re-)compiling: mode=%s, %s',\n\u001b[1;32m    915\u001b[0m                      self.execution_mode, input_specs)\n",
            "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mtpu_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[0;34m\"\"\"Yields a TPU session and sets it as the default Keras session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m       \u001b[0mdefault_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1812\u001b[0m       \u001b[0;31m# N.B. We have to call `K.set_session()` AND set our session as the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m       \u001b[0;31m# TF default. `K.get_session()` surprisingly does not return the value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 710\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef expected inputs 'float' do not match 2 inputs specified; Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\"](training/TFOptimizer/gradients/tpu_model_140369801514736/conv2d_1/Conv2D_grad/tuple/control_dependency_1, training/TFOptimizer/CrossReplicaSum/group_assignment)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ibenmWsXzMNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7fe904b0-88ba-4e00-f9a6-1dc9f27aecbc"
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"ResNet-50 implemented with Keras running on Cloud TPUs.\n",
        "This file shows how you can run ResNet-50 on a Cloud TPU using the TensorFlow\n",
        "Keras support. This is configured for ImageNet (e.g. 1000 classes), but you can\n",
        "easily adapt to your own datasets by changing the code appropriately.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import imagenet_input\n",
        "\n",
        "try:\n",
        "  import h5py as _  # pylint: disable=g-import-not-at-top\n",
        "  HAS_H5PY = True\n",
        "except ImportError:\n",
        "  logging.warning('`h5py` is not installed. Please consider installing it '\n",
        "                  'to save weights for long-running training.')\n",
        "  HAS_H5PY = False\n",
        "\n",
        "\n",
        "flags.DEFINE_bool('use_tpu', True, 'Use TPU model instead of CPU.')\n",
        "flags.DEFINE_string('tpu', None, 'Name of the TPU to use.')\n",
        "flags.DEFINE_string('data', None, 'Path to training and testing data.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "PER_CORE_BATCH_SIZE = 128\n",
        "NUM_CLASSES = 1000\n",
        "IMAGE_SIZE = 224\n",
        "EPOCHS = 90  # Standard imagenet training regime.\n",
        "APPROX_IMAGENET_TRAINING_IMAGES = 1280000  # Approximate number of images.\n",
        "APPROX_IMAGENET_TEST_IMAGES = 48000  # Approximate number of images.\n",
        "\n",
        "WEIGHTS_TXT = '/tmp/resnet50_weights.h5'\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "  logging.info('Building Keras ResNet-50 model.')\n",
        "  model = tf.keras.applications.resnet50.ResNet50(\n",
        "      include_top=True,\n",
        "      weights=None,\n",
        "      input_tensor=None,\n",
        "      input_shape=None,\n",
        "      pooling=None,\n",
        "      classes=NUM_CLASSES)\n",
        "\n",
        "  num_cores = 8\n",
        "  batch_size = PER_CORE_BATCH_SIZE * num_cores\n",
        "\n",
        "  if FLAGS.use_tpu:\n",
        "    logging.info('Converting from CPU to TPU model.')\n",
        "    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "    session_master = resolver.master()\n",
        "  else:\n",
        "    session_master = ''\n",
        "\n",
        "  logging.info('Compiling model.')\n",
        "  model.compile(\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "  if FLAGS.data is None:\n",
        "    training_images = np.random.randn(\n",
        "        batch_size, IMAGE_SIZE, IMAGE_SIZE, 3).astype(np.float32)\n",
        "    training_labels = np.random.randint(NUM_CLASSES, size=batch_size,\n",
        "                                        dtype=np.int32)\n",
        "    logging.info('Training model using synthetica data.')\n",
        "    model.fit(training_images, training_labels, epochs=EPOCHS,\n",
        "              batch_size=batch_size)\n",
        "    logging.info('Evaluating the model on synthetic data.')\n",
        "    model.evaluate(training_images, training_labels, verbose=0)\n",
        "  else:\n",
        "    imagenet_train, imagenet_eval = [imagenet_input.ImageNetInput(\n",
        "        is_training=is_training,\n",
        "        data_dir=FLAGS.data,\n",
        "        per_core_batch_size=PER_CORE_BATCH_SIZE)\n",
        "                                     for is_training in [True, False]]\n",
        "    logging.info('Training model using real data in directory \"%s\".',\n",
        "                 FLAGS.data)\n",
        "    model.fit(imagenet_train.input_fn,\n",
        "              epochs=EPOCHS,\n",
        "              steps_per_epoch=int(APPROX_IMAGENET_TRAINING_IMAGES / batch_size))\n",
        "\n",
        "    if HAS_H5PY:\n",
        "      logging.info('Save weights into %s', WEIGHTS_TXT)\n",
        "      model.save_weights(WEIGHTS_TXT, overwrite=True)\n",
        "\n",
        "    logging.info('Evaluating the model on the validation dataset.')\n",
        "    score = model.evaluate(\n",
        "        imagenet_eval.input_fn,\n",
        "        steps=int(APPROX_IMAGENET_TEST_IMAGES // batch_size),\n",
        "        verbose=1)\n",
        "    print('Evaluation score', score)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c62c05d6ad63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: No module named 'imagenet_input'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "r9DlwwbXq6o_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import dotenv\n",
        "# import keras\n",
        "import os\n",
        "dotenv.load_dotenv('.env')\n",
        "\n",
        "!mkdir model\n",
        "# !gsutil rsync gs://{os.environ['GCP_BUCKET']}/model model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoL4Y1yXqtaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5cb32500-ba58-4b15-c239-2fd8bf232040"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('raw/train.csv').set_index('id')\n",
        "test = pd.read_csv('raw/sample_submission.csv').set_index('id')\n",
        "depths = pd.read_csv('raw/depths.csv').set_index('id')\n",
        "\n",
        "train = pd.merge(train, depths, left_index=True, right_index=True, how='inner')\n",
        "train.shape, test.shape, depths.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 2), (18000, 1), (22000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "6TALjxylQlox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_size_target = 101"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6nC41yXq8j0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "73d65f68-7590-4b43-f4b5-f712fd457b61"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "\n",
        "train[\"image\"] = [np.array(load_img(\"./raw/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train.index)]\n",
        "train[\"mask\"] = [np.array(load_img(\"./raw/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train.index)]\n",
        "\n",
        "X_train = np.array(train['image'].tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
        "y_train = np.array(train['mask'].tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
        "\n",
        "X_test = np.array([np.array(load_img(\"./raw/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(test.index)]).reshape(-1, img_size_target, img_size_target, 1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "  0%|          | 0/4000 [00:00<?, ?it/s]/home/Hoiy/.local/lib/python3.5/site-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n",
            "100%|██████████| 4000/4000 [00:02<00:00, 1517.39it/s]\n",
            "100%|██████████| 4000/4000 [00:01<00:00, 3034.27it/s]\n",
            "100%|██████████| 18000/18000 [00:11<00:00, 1533.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ns-p0XzLKHQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16f71cf5-ebcf-44e8-9fa0-96bb485f9f53"
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32', copy=False)\n",
        "y_train = y_train.astype('float32', copy=False)\n",
        "\n",
        "X_test = X_test.astype('float32', copy=False)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 101, 101, 1), (4000, 101, 101, 1), (18000, 101, 101, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "EKDfbbaX_1WY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_config\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_estimator\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWtHOnEjhZs5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def sweep_thresholds_mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "def metric_fn(y_true, y_pred):\n",
        "  return {\n",
        "    'sweep_thresholds_mean_iou': sweep_thresholds_mean_iou(y_true, y_pred)\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIPW4QCxJEYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def host_call_fn(gs, loss, lr, ce):\n",
        "#   \"\"\"Training host call. Creates scalar summaries for training metrics.\n",
        "#   This function is executed on the CPU and should not directly reference\n",
        "#   any Tensors in the rest of the `model_fn`. To pass Tensors from the\n",
        "#   model to the `metric_fn`, provide as part of the `host_call`. See\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "#   for more information.\n",
        "#   Arguments should match the list of `Tensor` objects passed as the second\n",
        "#   element in the tuple passed to `host_call`.\n",
        "#   Args:\n",
        "#     gs: `Tensor with shape `[batch]` for the global_step\n",
        "#     loss: `Tensor` with shape `[batch]` for the training loss.\n",
        "#     lr: `Tensor` with shape `[batch]` for the learning_rate.\n",
        "#     ce: `Tensor` with shape `[batch]` for the current_epoch.\n",
        "#   Returns:\n",
        "#     List of summary ops to run on the CPU host.\n",
        "#   \"\"\"\n",
        "#   gs = gs[0]\n",
        "#   # Host call fns are executed FLAGS.iterations_per_loop times after one\n",
        "#   # TPU loop is finished, setting max_queue value to the same as number of\n",
        "#   # iterations will make the summary writer only flush the data to storage\n",
        "#   # once per loop.\n",
        "#   with summary.create_file_writer(\n",
        "#       FLAGS.model_dir, max_queue=FLAGS.iterations_per_loop).as_default():\n",
        "#     with summary.always_record_summaries():\n",
        "#       summary.scalar('loss', loss[0], step=gs)\n",
        "#       summary.scalar('learning_rate', lr[0], step=gs)\n",
        "#       summary.scalar('current_epoch', ce[0], step=gs)\n",
        "\n",
        "#       return summary.all_summary_ops()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sIa5GsA-_0rb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def test_model_fn(features, labels, mode, params):\n",
        "#     ###################################################################\n",
        "#     # The Keras LSTM layer that causes an error unless it is unrolled #\n",
        "#     ###################################################################\n",
        "#     predictions = tf.keras.layers.Conv2D(filters=1, kernel_size=1)(features)\n",
        "\n",
        "#     # Create ops for the TPUEstimatorSpec\n",
        "#     loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n",
        "#     optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
        "# #     if FLAGS.use_tpu:\n",
        "#     optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)\n",
        "  \n",
        "#     global_step = tf.train.get_global_step()\n",
        "#     train_op = optimizer.minimize(loss, global_step)\n",
        "\n",
        "#     # Return the TPUEstimatorSpec\n",
        "#     return tpu_estimator.TPUEstimatorSpec(\n",
        "#         mode=mode,\n",
        "#         loss=loss,\n",
        "#         train_op=train_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpUjJCzRBNUy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # https://github.com/tensorflow/tpu/blob/master/models/experimental/cifar_keras/cifar_keras.py\n",
        "\n",
        "# def model_fn(features, labels, mode, config, params):\n",
        "# #   print(features)\n",
        "#   v = tf.keras.layers.Input(tensor=features)\n",
        "#   pred = tf.keras.layers.Conv2D(\n",
        "#     filters=1,\n",
        "#     kernel_size=3,\n",
        "#     padding='same',\n",
        "#     activation='relu'\n",
        "#   )(v)\n",
        "  \n",
        "# #   print(pred)\n",
        "\n",
        "# #   pred=features\n",
        "  \n",
        "# #   if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "#     # Compute the current epoch and associated learning rate from global_step.\n",
        "#   global_step = tf.train.get_global_step()\n",
        "\n",
        "#   loss = tf.keras.losses.binary_crossentropy(labels, pred)  \n",
        "\n",
        "#   optimizer = tf.train.AdamOptimizer()\n",
        "#   optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "#   # Batch normalization requires UPDATE_OPS to be added as a dependency to\n",
        "#   # the train operation.\n",
        "#   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "#   with tf.control_dependencies(update_ops):\n",
        "#     train_op = optimizer.minimize(loss, global_step)\n",
        "      \n",
        "# #     model = tf.keras.Model(inputs=features, outputs=pred)\n",
        "    \n",
        "#   return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "#     mode=mode,\n",
        "#     loss=loss,\n",
        "#     train_op=train_op,\n",
        "#     host_call=None,\n",
        "#     eval_metrics=(metric_fn, [labels, pred]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oz4OuUomhxzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def main(unused_argv=None):\n",
        "#     tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver('kaggle-tpu')\n",
        "\n",
        "    \n",
        "#     config = tf.contrib.tpu.RunConfig(\n",
        "#       cluster=tpu_cluster_resolver,\n",
        "#       model_dir='gs://kaggle-195720-tgs-salt-identification-challenge/temp',\n",
        "# #       save_checkpoints_secs=3600,\n",
        "# #       session_config=tf.ConfigProto(\n",
        "# #           allow_soft_placement=True, log_device_placement=True),\n",
        "# #       tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "# #           iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "# #           num_shards=FLAGS.num_shards),\n",
        "#   )\n",
        "    \n",
        "    \n",
        "# #     config = tpu_config.RunConfig(\n",
        "# #         master=master_url,\n",
        "# #         model_dir='gs://kaggle-195720-tgs-salt-identification-challenge/temp')\n",
        "\n",
        "#     # Create the TPUEstimator\n",
        "#     estimator = tpu_estimator.TPUEstimator(\n",
        "#       use_tpu=True,\n",
        "#       model_fn=model_fn,\n",
        "#       config=config,\n",
        "#       train_batch_size=1024)\n",
        "\n",
        "#     # Train the estimator for 10 steps\n",
        "#     estimator.train(input_fn, max_steps=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXICpUNGitvo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def input_fn(params):\n",
        "#     dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "# #     dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(params['batch_size']))\n",
        "    \n",
        "#     print(dataset)\n",
        "\n",
        "#     # Make input_fn for the TPUEstimator train step\n",
        "# #     dataset_fn = dataset.make_one_shot_iterator().get_next()\n",
        "#     dataset = dataset.shuffle(1000).repeat().batch(params['batch_size'])\n",
        "    \n",
        "# #     dataset = dataset.prefetch(1)\n",
        "#     return dataset\n",
        "  \n",
        "  \n",
        "# # dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "# # dataset\n",
        "# # dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(128))\n",
        "\n",
        "# # print(dataset)\n",
        "\n",
        "# # # Make input_fn for the TPUEstimator train step\n",
        "# # dataset_fn = dataset.make_one_shot_iterator().get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFjqvXl4ijH9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myc4wE2NdCN-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.contrib import tpu\n",
        "# from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
        "\n",
        "# def axy_computation(a, x, y):\n",
        "#   return a * x + y\n",
        "\n",
        "# inputs = [\n",
        "#     3.0,\n",
        "#     tf.ones([3, 3], tf.float32),\n",
        "#     tf.ones([3, 3], tf.float32),\n",
        "# ]\n",
        "\n",
        "# tpu_computation = tpu.rewrite(axy_computation, inputs)\n",
        "\n",
        "# tpu_grpc_url = TPUClusterResolver(\n",
        "#     tpu=['kaggle-tpu']).get_master()\n",
        "\n",
        "# with tf.Session(tpu_grpc_url) as sess:\n",
        "#   sess.run(tpu.initialize_system())\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "#   output = sess.run(tpu_computation)\n",
        "#   print(output)\n",
        "#   sess.run(tpu.shutdown_system())\n",
        "\n",
        "# print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16XOGte5q_0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import keras\n",
        "# from keras import Model\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "# from keras.models import load_model\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, SpatialDropout2D, UpSampling2D\n",
        "\n",
        "# from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "# def dice_loss(y_true, y_pred):\n",
        "#     smooth = 1.\n",
        "#     y_true_f = K.flatten(y_true)\n",
        "#     y_pred_f = K.flatten(y_pred)\n",
        "#     intersection = y_true_f * y_pred_f\n",
        "#     score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "#     return 1. - score\n",
        "  \n",
        "  \n",
        "\n",
        "# # define iou or jaccard loss function\n",
        "# def jaccard_loss(y_true, y_pred):\n",
        "#     y_true = tf.reshape(y_true, [-1])\n",
        "#     y_pred = tf.reshape(y_pred, [-1])\n",
        "#     intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    \n",
        "#     epsilon = tf.keras.backend.epsilon()\n",
        "    \n",
        "#     score = (intersection + epsilon) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + epsilon)\n",
        "#     return 1 - score\n",
        "\n",
        "\n",
        "  \n",
        "# def iou_bce_loss(y_true, y_pred):\n",
        "#     return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.to_float(tf.greater(tf.round(y_true), y_pred)))\n",
        "    \n",
        "\n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     i = tf.round(y_pred) * tf.round(y_true)\n",
        "#     o = tf.round(y_pred) * tf.round(y_true)\n",
        "#     mean_iou = tf.metrics.mean_iou(y_true, tf.round(y_pred), 1)\n",
        "#     print(mean_iou)\n",
        "#     print(tf.tile(mean_iou, 10))\n",
        "#     print(mean_iou[0])\n",
        "#     return mean_iou[0]\n",
        "#     return tf.reduce_mean(tf.tile(mean_iou, 10))\n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     prec = []\n",
        "#     for t in np.arange(0.5, 1.0, 0.05):\n",
        "#         y_pred_ = tf.to_int32(y_pred > t)\n",
        "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         with tf.control_dependencies([up_opt]):\n",
        "#             score = tf.identity(score)\n",
        "#         prec.append(score)\n",
        "#     return K.mean(K.stack(prec), axis=0)\n",
        "\n",
        "\n",
        "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
        "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if activation == True:\n",
        "        x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "  \n",
        "def residual_block(blockInput, num_filters=16):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = convolution_block(x, num_filters, (3,3) )\n",
        "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
        "    x = Add()([x, blockInput])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "def conv_block(m, dim, acti, bn, res, do=0.5):\n",
        "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
        "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
        "    n = tf.keras.layers.SpatialDropout2D(do)(n) if do else n\n",
        "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
        "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
        "    n = tf.keras.layers.Concatenate()([m, n]) if res else n\n",
        "    return n\n",
        "\n",
        "  \n",
        "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "    if depth > 0:\n",
        "        n = conv_block(m, dim, acti, bn, res)\n",
        "        m = tf.keras.layers.MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "        m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
        "        if up:\n",
        "            m = tf.keras.layers.UpSampling2D()(m)\n",
        "            m = tf.keras.layers.Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "        else:\n",
        "#             padding = ['valid', 'same','same','valid','same','valid']\n",
        "            padding = ['same','same','valid','same','valid']\n",
        "            m = tf.keras.layers.Conv2DTranspose(dim, 3, strides=2, activation=acti, padding=padding[depth-1])(m)\n",
        "        n = tf.keras.layers.Concatenate()([n, m])\n",
        "        m = conv_block(n, dim, acti, bn, res)\n",
        "    else:\n",
        "        m = conv_block(m, dim, acti, bn, res, do=do)\n",
        "    return m\n",
        "\n",
        "  \n",
        "# def UNet(img_shape, out_ch=1, start_ch=32, depth=5, inc_rate=2., activation='relu', \n",
        "#          dropout=False, batchnorm=True, maxpool=True, upconv=False, residual=True):\n",
        "#     i = tf.keras.layers.Input(shape=img_shape)\n",
        "#     o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
        "#     o = tf.keras.layers.Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
        "#     return tf.keras.Model(inputs=i, outputs=o)\n",
        "\n",
        "def UNet(img_shape, out_ch=1, start_ch=32, depth=5, inc_rate=2., activation='relu', \n",
        "         dropout=False, batchnorm=True, maxpool=True, upconv=False, residual=True):\n",
        "    i = tf.keras.layers.Input(shape=img_shape)\n",
        "    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
        "    o = tf.keras.layers.Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
        "    return tf.keras.Model(inputs=i, outputs=o)\n",
        "  \n",
        "  \n",
        "\n",
        "# def ResNet():\n",
        "#   i = Input(shape=(img_size_target, img_size_target))\n",
        "  \n",
        "#   o = Conv2D(filters=1, kernel_size=1, activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NDV_LZ8Ernrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2206
        },
        "outputId": "0a09936c-c198-4b0a-e043-a3a0c3a6b204"
      },
      "cell_type": "code",
      "source": [
        "def build_train_generator(X_train, y_train, batch_size=32):\n",
        "    # Ref: https://keras.io/preprocessing/image/\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(\n",
        "      rotation_range=360.,\n",
        "#       width_shift_range=0.1,\n",
        "#       height_shift_range=0.1,\n",
        "      fill_mode='reflect',\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "    )\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
        "    seed = 1\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "    \n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "\n",
        "    # combine generators into one which yields image and masks\n",
        "    return zip(image_generator, mask_generator)\n",
        "\n",
        "\n",
        "def cv(X_train, y_train, X_test, fold, fast=False):\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    \n",
        "    loss = []\n",
        "    metric = []\n",
        "    pred_test = np.zeros(X_test.shape)\n",
        "    fold_index = 0\n",
        "    \n",
        "    start_ch = 32\n",
        "    depth = 5\n",
        "         \n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train):      \n",
        "#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\n",
        "      model = test_model()\n",
        "      resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "      strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "      model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "      session_master = resolver.master()\n",
        "      \n",
        "      model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"binary_crossentropy\"])\n",
        "\n",
        "#       early_stopping = EarlyStopping(patience=20, verbose=1)\n",
        "#       model_checkpoint = ModelCheckpoint(\"./model/keras_%d.model\"%fold_index, save_best_only=True, verbose=1)\n",
        "#       reduce_lr = ReduceLROnPlateau(factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
        "\n",
        "      epochs = 20000\n",
        "      batch_size = 32\n",
        "\n",
        "      history = model.fit_generator(\n",
        "          build_train_generator(X_train[train_index], y_train[train_index], batch_size),\n",
        "          validation_data=(X_train[test_index], y_train[test_index]),\n",
        "          steps_per_epoch=np.ceil(len(X_train[train_index]) / batch_size), \n",
        "          epochs=epochs,\n",
        "#           callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "      )\n",
        "      \n",
        "      best_idx = np.argmin(history.history['val_loss'])\n",
        "      val_loss = history.history['val_loss'][best_idx]\n",
        "      val_mean_iou = history.history['val_mean_iou'][best_idx]\n",
        "      \n",
        "      print(\"Best val_loss: %.4f, val_mean_iou: %.4f\"%(val_loss, val_mean_iou))\n",
        "      loss.append(val_loss)\n",
        "      metric.append(val_mean_iou)\n",
        "      \n",
        "      model = keras.models.load_model('model/keras_%d.model'%fold_index, custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss, 'dice_loss': dice_loss})\n",
        "      pred_test += model.predict(X_test) / fold\n",
        "      fold_index = fold_index + 1\n",
        "      \n",
        "      if fast:\n",
        "        break\n",
        "      \n",
        "      \n",
        "    print('%d fold val_loss: %.4f, val_mean_iou: %.4f'%(fold, np.mean(loss), np.mean(metric)))\n",
        "    return pred_test, np.mean(loss), np.mean(metric)\n",
        "\n",
        "  \n",
        "pred_test, val_loss, val_mean_iou = cv(X_train, y_train, X_test, 5, fast=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 13486222805198966859)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9984220072123666814)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 18162437541401842557)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9211313139234737039)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4136755912537517626)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6532054137011805886)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3825718477947054890)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5276807511303573055)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5719668420551969614)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10390436815882700867)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15818039368216332404)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6864323238587904500)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: metrics/mean_iou/ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_BOOL, Truncate=false, _tpu_replicate=\"cluster\"](metrics/mean_iou/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0dea82b24951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-0dea82b24951>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(X_train, y_train, X_test, fold, fast)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kaggle-tpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUDistributionStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_to_tpu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0msession_master\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/framework/python/framework/experimental.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m'any time, and without warning.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         decorator_utils.get_qualified_name(func), func.__module__)\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m   new_func.__doc__ = _add_experimental_function_notice_to_docstring(\n\u001b[1;32m     66\u001b[0m       func.__doc__)\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mtpu_model\u001b[0;34m(model, strategy)\u001b[0m\n\u001b[1;32m   1888\u001b[0m   \"\"\"\n\u001b[1;32m   1889\u001b[0m   \u001b[0;31m# Force initialization of the CPU model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m       \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   2706\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 710\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: metrics/mean_iou/ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_BOOL, Truncate=false, _tpu_replicate=\"cluster\"](metrics/mean_iou/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HRemgbrVOpjK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # summarize history for loss\n",
        "# plt.plot(history.history['loss'][1:])\n",
        "# plt.plot(history.history['val_loss'][1:])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train','Validation'], loc='upper left')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNqZwFOebfzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# best_idx = np.argmin(history.history['val_loss'])\n",
        "# val_loss = history.history['val_loss'][best_idx]\n",
        "# val_mean_iou = history.history['val_mean_iou'][best_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgBEjwKHCn1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('model/keras.model', custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EqepTCQa1djX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RLenc(img, order='F', format=True):\n",
        "    \"\"\"\n",
        "    img is binary mask image, shape (r,c)\n",
        "    order is down-then-right, i.e. Fortran\n",
        "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
        "\n",
        "    returns run length as an array or string (if format is True)\n",
        "    \"\"\"\n",
        "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
        "    runs = []  ## list of run lengths\n",
        "    r = 0  ## the current run length\n",
        "    pos = 1  ## count starts from 1 per WK\n",
        "    for c in bytes:\n",
        "        if (c == 0):\n",
        "            if r != 0:\n",
        "                runs.append((pos, r))\n",
        "                pos += r\n",
        "                r = 0\n",
        "            pos += 1\n",
        "        else:\n",
        "            r += 1\n",
        "\n",
        "    # if last run is unsaved (i.e. data ends with 1)\n",
        "    if r != 0:\n",
        "        runs.append((pos, r))\n",
        "        pos += r\n",
        "        r = 0\n",
        "\n",
        "    if format:\n",
        "        z = ''\n",
        "\n",
        "        for rr in runs:\n",
        "            z += '{} {} '.format(rr[0], rr[1])    \n",
        "        return z[:-1]\n",
        "    else:\n",
        "        return runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5wnyMSkd50M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class FasterRle(object):\n",
        "#     \"\"\"Perform RLE in paralell.\"\"\"\n",
        "\n",
        "#     def __init__(self, num_consumers=2):\n",
        "#         \"\"\"Initialize class.\"\"\"\n",
        "#         self._tasks = multiprocessing.JoinableQueue()\n",
        "#         self._results = multiprocessing.Queue()\n",
        "#         self._n_consumers = num_consumers\n",
        "\n",
        "#         # Initialize consumers\n",
        "#         self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n",
        "#         for w in self._consumers:\n",
        "#             w.start()\n",
        "\n",
        "#     def add(self, array, startIndex):\n",
        "#         \"\"\"Add a task to perform.\"\"\"\n",
        "#         self._tasks.put(FasterTask(array, startIndex))\n",
        "\n",
        "#     def get_results(self):\n",
        "#         \"\"\"Close all tasks.\"\"\"\n",
        "#         # Provide poison pill\n",
        "#         [self._tasks.put(None) for _ in range(self._n_consumers)]\n",
        "#         # Wait for finish\n",
        "#         self._tasks.join()\n",
        "#         # Return results\n",
        "#         singles = []\n",
        "#         while not self._results.empty():\n",
        "#             singles.append(self._results.get())\n",
        "            \n",
        "#         resultDic = dict()\n",
        "#         for rles, start in singles:\n",
        "#             #print('start:', start)\n",
        "#             for i,rle in enumerate(rles):\n",
        "#                 #print('i:', i)\n",
        "#                 resultDic[str(start+i)] = rle\n",
        "#         return resultDic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7wRK-ZUEmzA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch(func):\n",
        "  def batch_func(b):\n",
        "    return np.array([func(e) for e in tqdm(b)])\n",
        "  return batch_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sy0eHi4-P5PP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preds_test = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hIcfQ4_pPlHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e52d1ce9-b9b2-4fdf-f039-e606626a790a"
      },
      "cell_type": "code",
      "source": [
        "test['rle_mask'] = batch(RLenc)(np.round(pred_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18000/18000 [00:58<00:00, 309.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ST2JPIMLP666",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7aaf49b7-1c59-4efc-cfb8-2bdd7e3a885f"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_FILE = 'keras_unet_sc32de5wbecjaccloss_%.4f_%.4f.csv'%(val_loss, val_mean_iou)\n",
        "test.to_csv(SUBMISSION_FILE)\n",
        "SUBMISSION_FILE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'keras_unet_sc32de5wbecjaccloss_0.1320_0.7477.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "7iJCa8ZqRppy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cc737439-5883-4295-c670-658d69aaff48"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_MESSAGE='\"Keras U-net iou_bce_loss: val_loss: %.4f, val_mean_iou: %.4f\"'%(val_loss, val_mean_iou)\n",
        "!kaggle competitions submit -f '{SUBMISSION_FILE}' -m '{SUBMISSION_MESSAGE}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using competition: tgs-salt-identification-challenge\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.4.6)\n",
            "Successfully submitted to TGS Salt Identification Challenge"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ql1uneNwTuEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "c9be94ee-85ec-4429-d351-98436cfaf45f"
      },
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv('.env')\n",
        "!gsutil rsync model gs://{os.environ['GCP_BUCKET']}/model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "Copying file://model/keras_0.model [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "Copying file://model/keras_1.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_2.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_3.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m -o ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying file://model/keras_4.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "Operation completed over 5 objects/751.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6UwBwalwuZpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "bad2519d-f469-4e15-ce5c-1a921ed92c84"
      },
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# def batch_iou(pred, truth):\n",
        "#     pred_ = tf.round(pred)\n",
        "#     truth_ = truth\n",
        "    \n",
        "#     i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "#     u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "#     epsilon = tf.keras.backend.epsilon()\n",
        "#     return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "  \n",
        "# def mean_iou(pred, truth):\n",
        "#     ious = batch_iou(pred, truth)\n",
        "#     ious = tf.reshape(ious, (-1,1))\n",
        "#     ious = tf.tile(ious, (1,10))\n",
        "#     thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "#     return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "# BATCH_SIZE = 100\n",
        "# ones = tf.constant(np.ones((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# zeros = tf.constant(np.zeros((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# half = tf.coonstant(np.zeros(BATCH_SIZE, ))\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "  \n",
        "#   print(mean_iou(ones, zeros).eval())\n",
        "#   print(mean_iou(ones, ones).eval())\n",
        "#   print(mean_iou(zeros, ones).eval())\n",
        "#   print(mean_iou(zeros, zeros).eval())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1AayNFWou9ms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2345
        },
        "outputId": "b0ad8070-0ad3-4ddc-d926-aae031e16852"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Standard Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 12\n",
        "\n",
        "# input image dimensions\n",
        "IMG_ROWS, IMG_COLS = 28, 28\n",
        "\n",
        "use_tpu=True\n",
        "\n",
        "\n",
        "def mnist_model(input_shape):\n",
        "  \"\"\"Creates a MNIST model.\"\"\"\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(\n",
        "      tf.keras.layers.Conv2D(\n",
        "          32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(tf.keras.layers.Dropout(0.25))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "\n",
        "def main(unused_dev):\n",
        "  use_tpu = True\n",
        "\n",
        "  print('Mode:', 'TPU' if use_tpu else 'CPU')\n",
        "\n",
        "  if True:\n",
        "    print('Using fake data')\n",
        "    x_train = np.random.random((128, IMG_ROWS, IMG_COLS))\n",
        "    y_train = np.zeros([128, 1], dtype=np.int32)\n",
        "    x_test, y_test = x_train, y_train\n",
        "  else:\n",
        "    # the data, split between train and test sets\n",
        "    print('Using real data')\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "  x_train = x_train.reshape(x_train.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
        "  input_shape = (IMG_ROWS, IMG_COLS, 1)\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  print('x_train shape:', x_train.shape)\n",
        "  print(x_train.shape[0], 'train samples')\n",
        "  print(x_test.shape[0], 'test samples')\n",
        "\n",
        "  # convert class vectors to binary class matrices\n",
        "  y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "  y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "  model = mnist_model(input_shape)\n",
        "\n",
        "  if use_tpu:\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "    )\n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05),\n",
        "      metrics=['accuracy'])\n",
        "\n",
        "  model.fit(\n",
        "      x_train,\n",
        "      y_train,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      epochs=EPOCHS,\n",
        "      verbose=1,\n",
        "      validation_data=(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Loss for final step:', score[0])\n",
        "  print('Accuracy ', score[1])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mode: TPU\n",
            "Using fake data\n",
            "x_train shape: (128, 28, 28, 1)\n",
            "128 train samples\n",
            "128 test samples\n",
            "INFO:tensorflow:Detected old TPUStrategy API.\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 128 samples, validate on 128 samples\n",
            "Epoch 1/12\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(16, 28, 28, 1), dtype=tf.float32, name='conv2d_4_input0'), TensorSpec(shape=(16, 10), dtype=tf.float32, name='dense_5_target0')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_4_input\n",
            "INFO:tensorflow:Started compiling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1260\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'group_assignment' not in Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\", group_assignment=[]](training/TFOptimizer/gradients/tpu_model_139793487692408/conv2d_4/Conv2D_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e8ddd031b353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-e8ddd031b353>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_dev)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1041\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    852\u001b[0m                                                    infeed_manager)\n\u001b[1;32m    853\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_model_compiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;31m# Initialize our TPU weights on the first compile.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_test_model_compiles\u001b[0;34m(self, tpu_model_ops)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m     \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpu_compilation_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompilationResultProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 668\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'group_assignment' not in Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\", group_assignment=[]](training/TFOptimizer/gradients/tpu_model_139793487692408/conv2d_4/Conv2D_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "R3b3873xfUM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}