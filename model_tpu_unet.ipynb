{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_tpu_unet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Hoiy/kaggle-tgs-salt-identification-challenge/blob/master/model_tpu_unet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "RgW68azy-I2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0758a2c4-7365-4c53-9745-d36c4636c565"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import dotenv\n",
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "\n",
        "dotenv.load_dotenv('.env')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "tQMYWUFxAqcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ff0ee389-af08-4efb-adf3-5465f8268a98"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('raw/train.csv').set_index('id')\n",
        "test = pd.read_csv('raw/sample_submission.csv').set_index('id')\n",
        "depths = pd.read_csv('raw/depths.csv').set_index('id')\n",
        "\n",
        "train = pd.merge(train, depths, left_index=True, right_index=True, how='inner')\n",
        "train.shape, test.shape, depths.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 2), (18000, 1), (22000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "RWGJQRBXAqxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Whe2VDUGAquf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "a5bcd00b-0b51-4e59-a25d-b4ab75f296a1"
      },
      "cell_type": "code",
      "source": [
        "X_train = [cv2.resize(cv2.imread(\"./raw/images/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(train.index)]\n",
        "X_train = np.expand_dims(np.array(X_train), -1)\n",
        "X_train = X_train / 255\n",
        "X_train = X_train.astype(np.float32)\n",
        "\n",
        "y_train = [cv2.resize(cv2.imread(\"./raw/masks/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(train.index)]\n",
        "y_train = np.expand_dims(np.array(y_train), -1)\n",
        "y_train = y_train / 255\n",
        "y_train = y_train.astype(np.float32)\n",
        "\n",
        "\n",
        "X_test = [cv2.resize(cv2.imread(\"./raw/images/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(test.index)]\n",
        "X_test = np.expand_dims(np.array(X_test), -1)\n",
        "X_test = X_test / 255\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4000/4000 [00:01<00:00, 2782.40it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 9413.59it/s]\n",
            "100%|██████████| 18000/18000 [00:06<00:00, 2801.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 128, 128, 1), (4000, 128, 128, 1), (18000, 128, 128, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "9D2pdhBiAqrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3731
        },
        "outputId": "08497dec-070b-40c9-f95f-28e94161aa5a"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "# from keras import Model\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "# from keras.models import load_model\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, SpatialDropout2D, UpSampling2D, Activation, Add\n",
        "\n",
        "# from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return 1. - score\n",
        "  \n",
        "  \n",
        "\n",
        "# define iou or jaccard loss function\n",
        "def jaccard_loss(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    \n",
        "    score = (intersection + epsilon) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + epsilon)\n",
        "    return 1 - score\n",
        "\n",
        "\n",
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    gts = tf.reduce_sum(gt_sorted)\n",
        "    intersection = gts - tf.cumsum(gt_sorted)\n",
        "    union = gts + tf.cumsum(1. - gt_sorted)\n",
        "    jaccard = 1. - intersection / union\n",
        "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
        "    return jaccard\n",
        "  \n",
        "  \n",
        "  \n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        def treat_image(log_lab):\n",
        "            log, lab = log_lab\n",
        "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
        "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
        "            return lovasz_hinge_flat(log, lab)\n",
        "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
        "        loss = tf.reduce_mean(losses)\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_loss():\n",
        "        labelsf = tf.cast(labels, logits.dtype)\n",
        "        signs = 2. * labelsf - 1.\n",
        "        errors = 1. - logits * tf.stop_gradient(signs)\n",
        "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
        "        gt_sorted = tf.gather(labelsf, perm)\n",
        "        grad = lovasz_grad(gt_sorted)\n",
        "        #loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        loss = tf.tensordot(tf.nn.elu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        return loss\n",
        "\n",
        "    # deal with the void prediction case (only void pixels)\n",
        "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
        "                   lambda: tf.reduce_sum(logits) * 0.,\n",
        "                   compute_loss,\n",
        "                   strict=True,\n",
        "                   name=\"loss\"\n",
        "                   )\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = tf.reshape(scores, (-1,))\n",
        "    labels = tf.reshape(labels, (-1,))\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = tf.not_equal(labels, ignore)\n",
        "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
        "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
        "    return vscores, vlabels\n",
        "\n",
        "def lovasz_loss(y_true, y_pred):\n",
        "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
        "    #logits = K.log(y_pred / (1. - y_pred))\n",
        "    logits = y_pred #Jiaxin\n",
        "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
        "    return loss\n",
        "  \n",
        "\n",
        "# def iou_bce_loss(y_true, y_pred):\n",
        "#     return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.to_float(tf.greater(tf.round(y_true), y_pred)))\n",
        "    \n",
        "\n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     i = tf.round(y_pred) * tf.round(y_true)\n",
        "#     o = tf.round(y_pred) * tf.round(y_true)\n",
        "#     mean_iou = tf.metrics.mean_iou(y_true, tf.round(y_pred), 1)\n",
        "#     print(mean_iou)\n",
        "#     print(tf.tile(mean_iou, 10))\n",
        "#     print(mean_iou[0])\n",
        "#     return mean_iou[0]\n",
        "#     return tf.reduce_mean(tf.tile(mean_iou, 10))\n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     prec = []\n",
        "#     for t in np.arange(0.5, 1.0, 0.05):\n",
        "#         y_pred_ = tf.to_int32(y_pred > t)\n",
        "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         with tf.control_dependencies([up_opt]):\n",
        "#             score = tf.identity(score)\n",
        "#         prec.append(score)\n",
        "#     return K.mean(K.stack(prec), axis=0)\n",
        "\n",
        "\n",
        "# def convolution_block(x, filters, size):\n",
        "#     return x\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# def residual_block(blockInput, filters):\n",
        "#     x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(blockInput)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(x)\n",
        "#     x = Add()([x, blockInput])\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     return x\n",
        "  \n",
        "  \n",
        "# def res_block(i, filters=16):\n",
        "#     n1 = Conv2D(filters, 1, activation='relu', padding='same')(i)\n",
        "#     n3 = Conv2D(filters, 3, activation='relu', padding='same')(i)\n",
        "#     n5 = Conv2D(filters, 5, activation='relu', padding='same')(i)\n",
        "#     n = Concatenate()([n1, n3, n5])\n",
        "#     n = BatchNormalization()(n)\n",
        "    \n",
        "#     n1 = Conv2D(filters, 1, activation='relu', padding='same')(n)\n",
        "#     n3 = Conv2D(filters, 3, activation='relu', padding='same')(n)\n",
        "#     n5 = Conv2D(filters, 5, activation='relu', padding='same')(n)\n",
        "#     n = Concatenate()([n1, n3, n5])\n",
        "#     n = BatchNormalization()(n)\n",
        "    \n",
        "# #     n = Conv2D(1, 1, activation='relu', padding='same')(n)\n",
        "# #     n = Concatenate()([i, n])\n",
        "# #     n = BatchNormalization()(n)\n",
        "#     return n\n",
        "\n",
        "  \n",
        "# def level_block(inp, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "#     if depth > 0:\n",
        "#         n = conv_block(m, dim, acti, bn, res)\n",
        "#         m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "#         m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
        "#         if up:\n",
        "#             m = UpSampling2D()(m)\n",
        "#             m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "#         else:\n",
        "# #             padding = ['valid', 'same','same','valid','same','valid']\n",
        "#             padding = ['same','same','valid','same','valid']\n",
        "#             m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding=padding[depth-1])(m)\n",
        "#         n = Concatenate()([n, m])\n",
        "#         m = conv_block(n, dim, acti, bn, res)\n",
        "#     else:\n",
        "#         m = conv_block(m, dim, acti, bn, res, do=do)\n",
        "#     return m\n",
        "\n",
        "  \n",
        "# def ResNet():\n",
        "#     filter_size = 8\n",
        "#     i = Input(shape=(img_size_target,img_size_target,1))\n",
        "#     o = Conv2D(filter_size, 1, strides=(1,1), padding='same', activation='relu')(i)\n",
        "#     for depth in tqdm(range(128)):\n",
        "#       o = residual_block(o, filter_size)\n",
        "#     o = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(o)\n",
        "#     return Model(inputs=i, outputs=o)\n",
        "  \n",
        "  \n",
        "# def ResNet_2():\n",
        "#     resnet = keras.applications.resnet50.ResNet50(include_top=False, pooling=None, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "#     o = Conv2D(1, 1, strides=(1,1), activation='sigmoid')(resnet.output)\n",
        "#     return Model(inputs=resnet.input, outputs=o)\n",
        "  \n",
        "def res_block(inputs, filters):\n",
        "    inputs = BatchNormalization()(inputs)\n",
        "    conv = Conv2D(filters, 3, strides=1, padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = Conv2D(filters, 3, strides=1, padding='same', activation='relu')(conv)\n",
        "    \n",
        "    conv = Add()([conv, inputs])\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    return Conv2D(filters, 3, strides=2, padding='same', activation='relu')(conv)\n",
        "  \n",
        "\n",
        "def res_block_r(inputs, filters, res):\n",
        "    inputs = Add()([inputs, res])\n",
        "    inputs = BatchNormalization()(inputs)\n",
        "    conv = Conv2DTranspose(filters, 3, strides=1, padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = Conv2DTranspose(filters, 3, strides=1, padding='same', activation='relu')(conv)\n",
        "    \n",
        "    conv = Add()([conv, inputs])\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    return Conv2DTranspose(filters, 3, strides=2, padding='same', activation='relu')(conv)\n",
        "  \n",
        "  \n",
        "# def build_model(start_channels=8, depth=6):\n",
        "#     inp = Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "#     l = BatchNormalization()(inp)\n",
        "#     l = Conv2D(start_channels, 3, strides=1, padding='same', activation='relu')(l)\n",
        "    \n",
        "#     layers = [l]\n",
        "    \n",
        "#     for i in range(depth):\n",
        "#       layers.append(res_block(layers[-1], start_channels * (2 ** i)))\n",
        "      \n",
        "      \n",
        "#     for i in range(depth):\n",
        "#       layers.append(res_block_r(layers[-1], start_channels * (2 ** (depth - 1 - i)), layers[-2*i-1]))\n",
        "      \n",
        "#     l = BatchNormalization()(layers[-1])\n",
        "#     out = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(l)\n",
        "    \n",
        "#     return Model(inputs=inp, outputs=out)\n",
        "\n",
        "def UNet(inputs, channels, depth):\n",
        "    layers = [inputs]\n",
        "    \n",
        "    for i in range(depth):\n",
        "      layers.append(res_block(layers[-1], channels))\n",
        "      \n",
        "      \n",
        "    for i in range(depth):\n",
        "      layers.append(res_block_r(layers[-1], channels, layers[-2*i-1]))\n",
        "    \n",
        "    return layers[-1]\n",
        "    \n",
        "    \n",
        "\n",
        "def build_model(channels=64, depth=6):\n",
        "    inp = keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "    l = BatchNormalization()(inp)\n",
        "    l = Conv2D(channels, 3, strides=1, padding='same', activation='relu')(l)\n",
        "    \n",
        "    l = UNet(l, channels, depth)\n",
        "#     l = UNet(l, channels, depth)\n",
        "    \n",
        "    l = BatchNormalization()(l)\n",
        "    out = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(l)\n",
        "    \n",
        "    return keras.Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "  \n",
        "build_model().summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 128, 128, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 128, 128, 1)  4           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 128, 128, 64) 640         batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 128, 128, 64) 0           conv2d_2[0][0]                   \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 64, 64, 64)   0           conv2d_5[0][0]                   \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           conv2d_8[0][0]                   \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36928       batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           conv2d_11[0][0]                  \n",
            "                                                                 batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 64)     0           conv2d_14[0][0]                  \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 64)     256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 4, 4, 64)     256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 4, 4, 64)     0           conv2d_17[0][0]                  \n",
            "                                                                 batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 4, 4, 64)     256         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 2, 2, 64)     36928       batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 2, 2, 64)     0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 2, 2, 64)     256         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 2, 2, 64)     36928       batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 2, 2, 64)     256         conv2d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 2, 2, 64)     36928       batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 2, 2, 64)     0           conv2d_transpose_1[0][0]         \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 2, 2, 64)     256         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 4, 4, 64)     0           conv2d_transpose_2[0][0]         \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4, 4, 64)     256         add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 64)     256         conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 4, 4, 64)     0           conv2d_transpose_4[0][0]         \n",
            "                                                                 batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 4, 4, 64)     256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 8, 8, 64)     0           conv2d_transpose_5[0][0]         \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 64)     256         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 64)     256         conv2d_transpose_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 8, 8, 64)     0           conv2d_transpose_7[0][0]         \n",
            "                                                                 batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 64)     256         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 16, 16, 64)   36928       batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_8[0][0]         \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 64)   256         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTrans (None, 16, 16, 64)   36928       batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 64)   256         conv2d_transpose_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DTran (None, 16, 16, 64)   36928       batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_10[0][0]        \n",
            "                                                                 batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 64)   256         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_11 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 32, 32, 64)   0           conv2d_transpose_11[0][0]        \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 32, 64)   256         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 64)   256         conv2d_transpose_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 32, 32, 64)   0           conv2d_transpose_13[0][0]        \n",
            "                                                                 batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 64)   256         add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_14 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 64, 64, 64)   0           conv2d_transpose_14[0][0]        \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 64, 64, 64)   256         add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_15 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 64, 64, 64)   256         conv2d_transpose_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_16 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 64, 64, 64)   0           conv2d_transpose_16[0][0]        \n",
            "                                                                 batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 64, 64, 64)   256         add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_17 (Conv2DTran (None, 128, 128, 64) 36928       batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 128, 128, 64) 256         conv2d_transpose_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 128, 128, 1)  65          batch_normalization_37[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 1,339,589\n",
            "Trainable params: 1,334,851\n",
            "Non-trainable params: 4,738\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q2kwPCzH776M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from albumentations import (\n",
        "    PadIfNeeded,\n",
        "    HorizontalFlip,\n",
        "    VerticalFlip,    \n",
        "    CenterCrop,    \n",
        "    Crop,\n",
        "    Compose,\n",
        "    Transpose,\n",
        "    RandomRotate90,\n",
        "    ElasticTransform,\n",
        "    GridDistortion, \n",
        "    OpticalDistortion,\n",
        "    RandomSizedCrop,\n",
        "    OneOf,\n",
        "    CLAHE,\n",
        "    RandomContrast,\n",
        "    RandomGamma,\n",
        "    RandomBrightness\n",
        ")\n",
        "\n",
        "def random_h_flip(image, mask):\n",
        "  aug = HorizontalFlip(p=0.5)\n",
        "  augmented = aug(image=image, mask=mask)\n",
        "  return augmented['image'], augmented['mask']\n",
        "\n",
        "\n",
        "def random_v_flip(image, mask):\n",
        "  aug = VerticalFlip(p=0.5)\n",
        "  augmented = aug(image=image, mask=mask)\n",
        "  return augmented['image'], augmented['mask']\n",
        "\n",
        "\n",
        "def random_rotate(image, mask):\n",
        "  aug = RandomRotate90(p=1)\n",
        "  augmented = aug(image=image, mask=mask)\n",
        "  return augmented['image'], augmented['mask']\n",
        "\n",
        "\n",
        "def random_aug(image, mask):\n",
        "  image, mask = random_h_flip(image, mask)\n",
        "  image, mask = random_v_flip(image, mask)\n",
        "  image, mask = random_rotate(image, mask)\n",
        "  return image, mask\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def _random_flip_left_right(image, mask):\n",
        "  image = tf.image.random_flip_left_right(image, seed=SEED)\n",
        "  mask = tf.image.random_flip_left_right(mask, seed=SEED)\n",
        "  return image, mask\n",
        "\n",
        "def _random_flip_up_down(image, mask):\n",
        "  image = tf.image.random_flip_up_down(image, seed=SEED)\n",
        "  mask = tf.image.random_flip_up_down(mask, seed=SEED)\n",
        "  return image, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PjE-J85UBkVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11637
        },
        "outputId": "93006a00-7134-4d46-9258-78e631acf8eb"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def build_train_generator(X_train, y_train, batch_size=32):\n",
        "    # Ref: https://keras.io/preprocessing/image/\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(\n",
        "#       rotation_range=360.,\n",
        "#       width_shift_range=0.1,\n",
        "#       height_shift_range=0.1,\n",
        "      fill_mode='reflect',\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "    )\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
        "    seed = 1\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "    \n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "\n",
        "    # combine generators into one which yields image and masks\n",
        "    return zip(image_generator, mask_generator)\n",
        "\n",
        "  \n",
        "\n",
        "def cv(X_train, y_train, X_test, fold, fast=False):\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    \n",
        "    loss = []\n",
        "    metric = []\n",
        "    pred_test = np.zeros(X_test.shape)\n",
        "    fold_index = 0\n",
        "    \n",
        "    start_ch = 32\n",
        "    depth = 5\n",
        "         \n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train):      \n",
        "#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\n",
        "      model = build_model(channels=32)\n",
        "  \n",
        "      resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "      strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "      model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "      session_master = resolver.master()\n",
        "  \n",
        "      model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[mean_iou, \"binary_crossentropy\"])\n",
        "    \n",
        "      EPOCHS = 20000\n",
        "      BATCH_SIZE = 128\n",
        "\n",
        "      def create_data_fn(index, training=False):\n",
        "        def data_fn():\n",
        "          dataset = tf.data.Dataset.from_tensor_slices((X_train[index], y_train[index]))\n",
        "          if training:\n",
        "            dataset = dataset.map(_random_flip_left_right).map(_random_flip_up_down)\n",
        "          dataset = dataset.shuffle(len(X_train[index])).repeat(EPOCHS).batch(BATCH_SIZE, drop_remainder=True)\n",
        "          return dataset\n",
        "        return data_fn\n",
        "          \n",
        "\n",
        "      early_stopping = EarlyStopping(patience=10, verbose=1)\n",
        "      model_checkpoint = ModelCheckpoint(\"./model/keras_%d.model\"%fold_index, save_best_only=True, verbose=1)\n",
        "      reduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "      history = model.fit(\n",
        "          create_data_fn(train_index, training=True),\n",
        "          steps_per_epoch=int(np.ceil(len(X_train[train_index]) / BATCH_SIZE)),\n",
        "          validation_data=create_data_fn(test_index, training=False),\n",
        "          validation_steps=int(np.ceil(len(X_train[test_index]) / BATCH_SIZE)),\n",
        "          epochs=EPOCHS\n",
        "#           callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
        "#           callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "      \n",
        "      best_idx = np.argmin(history.history['val_loss'])\n",
        "      val_loss = history.history['val_loss'][best_idx]\n",
        "      val_mean_iou = history.history['val_mean_iou'][best_idx]\n",
        "      \n",
        "      print(\"Best val_loss: %.4f, val_mean_iou: %.4f\"%(val_loss, val_mean_iou))\n",
        "      loss.append(val_loss)\n",
        "      metric.append(val_mean_iou)\n",
        "      \n",
        "      model = keras.models.load_model('model/keras_%d.model'%fold_index, custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss, 'dice_loss': dice_loss})\n",
        "      pred_test += model.predict(X_test) / fold\n",
        "      fold_index = fold_index + 1\n",
        "      \n",
        "      del model\n",
        "      gc.collect()\n",
        "      \n",
        "      if fast:\n",
        "        break\n",
        "      \n",
        "      \n",
        "    print('%d fold val_loss: %.4f, val_mean_iou: %.4f'%(fold, np.mean(loss), np.mean(metric)))\n",
        "    return pred_test, np.mean(loss), np.mean(metric)\n",
        "\n",
        "  \n",
        "pred_test, val_loss, val_mean_iou = cv(X_train, y_train, X_test, 5, fast=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6931466752683336584)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2319523528817134981)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2621229120846880910)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13017604127097533172)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13684817192547276644)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15939364587101623240)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6777068171437765544)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 677236183219516705)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 17055041862000479642)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13317328517731664289)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7926127133639364078)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9787759974722640898)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: grpc://10.240.1.2:8470\n",
            "Epoch 1/20000\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(128, 128, 128, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Cloning Adam {'beta_2': 0.9990000128746033, 'decay': 0.0, 'lr': 0.0010000000474974513, 'epsilon': 1e-07, 'beta_1': 0.8999999761581421, 'amsgrad': False}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 25.029783964157104 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "24/25 [===========================>..] - ETA: 2s - loss: 0.5997 - mean_iou: 0.1103 - binary_crossentropy: 0.5997INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(128, 128, 128, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Cloning Adam {'beta_2': 0.9990000128746033, 'decay': 0.0, 'lr': 0.0010000000474974513, 'epsilon': 1e-07, 'beta_1': 0.8999999761581421, 'amsgrad': False}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.359346866607666 secs\n",
            "25/25 [==============================] - 83s 3s/step - loss: 0.5958 - mean_iou: 0.1118 - binary_crossentropy: 0.5958 - val_loss: 0.6440 - val_mean_iou: 0.3862 - val_binary_crossentropy: 0.6440\n",
            "Epoch 2/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.3954 - mean_iou: 0.2319 - binary_crossentropy: 0.3954 - val_loss: 0.5778 - val_mean_iou: 0.3605 - val_binary_crossentropy: 0.5778\n",
            "Epoch 3/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.3019 - mean_iou: 0.3693 - binary_crossentropy: 0.3019 - val_loss: 1.1062 - val_mean_iou: 0.0975 - val_binary_crossentropy: 1.1062\n",
            "Epoch 4/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.2259 - mean_iou: 0.4662 - binary_crossentropy: 0.2259 - val_loss: 4.7892 - val_mean_iou: 0.1407 - val_binary_crossentropy: 4.7892\n",
            "Epoch 5/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.1747 - mean_iou: 0.5622 - binary_crossentropy: 0.1747 - val_loss: 7.6849 - val_mean_iou: 0.1163 - val_binary_crossentropy: 7.6849\n",
            "Epoch 6/20000\n",
            "25/25 [==============================] - 15s 615ms/step - loss: 0.1256 - mean_iou: 0.6223 - binary_crossentropy: 0.1256 - val_loss: 8.0910 - val_mean_iou: 0.1377 - val_binary_crossentropy: 8.0910\n",
            "Epoch 7/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.1016 - mean_iou: 0.6669 - binary_crossentropy: 0.1016 - val_loss: 9.0269 - val_mean_iou: 0.1390 - val_binary_crossentropy: 9.0269\n",
            "Epoch 8/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.1009 - mean_iou: 0.6593 - binary_crossentropy: 0.1009 - val_loss: 4.8818 - val_mean_iou: 0.1432 - val_binary_crossentropy: 4.8818\n",
            "Epoch 9/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0843 - mean_iou: 0.7120 - binary_crossentropy: 0.0843 - val_loss: 4.9597 - val_mean_iou: 0.1354 - val_binary_crossentropy: 4.9597\n",
            "Epoch 10/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0704 - mean_iou: 0.7254 - binary_crossentropy: 0.0704 - val_loss: 4.7108 - val_mean_iou: 0.1618 - val_binary_crossentropy: 4.7108\n",
            "Epoch 11/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0617 - mean_iou: 0.7397 - binary_crossentropy: 0.0617 - val_loss: 4.1021 - val_mean_iou: 0.1622 - val_binary_crossentropy: 4.1021\n",
            "Epoch 12/20000\n",
            "25/25 [==============================] - 15s 614ms/step - loss: 0.0601 - mean_iou: 0.7406 - binary_crossentropy: 0.0601 - val_loss: 2.4100 - val_mean_iou: 0.2140 - val_binary_crossentropy: 2.4100\n",
            "Epoch 13/20000\n",
            "25/25 [==============================] - 15s 594ms/step - loss: 0.0537 - mean_iou: 0.7670 - binary_crossentropy: 0.0537 - val_loss: 2.7089 - val_mean_iou: 0.2232 - val_binary_crossentropy: 2.7089\n",
            "Epoch 14/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0488 - mean_iou: 0.7861 - binary_crossentropy: 0.0488 - val_loss: 1.1669 - val_mean_iou: 0.3084 - val_binary_crossentropy: 1.1669\n",
            "Epoch 15/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0450 - mean_iou: 0.7981 - binary_crossentropy: 0.0450 - val_loss: 0.7311 - val_mean_iou: 0.4257 - val_binary_crossentropy: 0.7311\n",
            "Epoch 16/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0427 - mean_iou: 0.7982 - binary_crossentropy: 0.0427 - val_loss: 1.4312 - val_mean_iou: 0.3921 - val_binary_crossentropy: 1.4312\n",
            "Epoch 17/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0398 - mean_iou: 0.8017 - binary_crossentropy: 0.0398 - val_loss: 0.8011 - val_mean_iou: 0.4468 - val_binary_crossentropy: 0.8011\n",
            "Epoch 18/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0389 - mean_iou: 0.8010 - binary_crossentropy: 0.0389 - val_loss: 0.6596 - val_mean_iou: 0.4881 - val_binary_crossentropy: 0.6596\n",
            "Epoch 19/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0379 - mean_iou: 0.8133 - binary_crossentropy: 0.0379 - val_loss: 0.7109 - val_mean_iou: 0.4700 - val_binary_crossentropy: 0.7109\n",
            "Epoch 20/20000\n",
            "25/25 [==============================] - 14s 579ms/step - loss: 0.0526 - mean_iou: 0.7657 - binary_crossentropy: 0.0526 - val_loss: 0.4586 - val_mean_iou: 0.5080 - val_binary_crossentropy: 0.4586\n",
            "Epoch 21/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0463 - mean_iou: 0.7798 - binary_crossentropy: 0.0463 - val_loss: 0.3855 - val_mean_iou: 0.5429 - val_binary_crossentropy: 0.3855\n",
            "Epoch 22/20000\n",
            "25/25 [==============================] - 15s 580ms/step - loss: 0.0369 - mean_iou: 0.8153 - binary_crossentropy: 0.0369 - val_loss: 0.3939 - val_mean_iou: 0.5772 - val_binary_crossentropy: 0.3939\n",
            "Epoch 23/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0347 - mean_iou: 0.8195 - binary_crossentropy: 0.0347 - val_loss: 0.3485 - val_mean_iou: 0.6098 - val_binary_crossentropy: 0.3485\n",
            "Epoch 24/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0312 - mean_iou: 0.8254 - binary_crossentropy: 0.0312 - val_loss: 0.4039 - val_mean_iou: 0.6065 - val_binary_crossentropy: 0.4039\n",
            "Epoch 25/20000\n",
            "25/25 [==============================] - 15s 581ms/step - loss: 0.0361 - mean_iou: 0.8099 - binary_crossentropy: 0.0361 - val_loss: 0.3621 - val_mean_iou: 0.5660 - val_binary_crossentropy: 0.3621\n",
            "Epoch 26/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0361 - mean_iou: 0.8151 - binary_crossentropy: 0.0361 - val_loss: 0.4141 - val_mean_iou: 0.5854 - val_binary_crossentropy: 0.4141\n",
            "Epoch 27/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0308 - mean_iou: 0.8269 - binary_crossentropy: 0.0308 - val_loss: 0.4396 - val_mean_iou: 0.5878 - val_binary_crossentropy: 0.4396\n",
            "Epoch 28/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0294 - mean_iou: 0.8384 - binary_crossentropy: 0.0294 - val_loss: 0.3909 - val_mean_iou: 0.5982 - val_binary_crossentropy: 0.3909\n",
            "Epoch 29/20000\n",
            "25/25 [==============================] - 15s 590ms/step - loss: 0.0326 - mean_iou: 0.8343 - binary_crossentropy: 0.0326 - val_loss: 0.3327 - val_mean_iou: 0.6196 - val_binary_crossentropy: 0.3327\n",
            "Epoch 30/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0311 - mean_iou: 0.8234 - binary_crossentropy: 0.0311 - val_loss: 0.3928 - val_mean_iou: 0.5776 - val_binary_crossentropy: 0.3928\n",
            "Epoch 31/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0275 - mean_iou: 0.8529 - binary_crossentropy: 0.0275 - val_loss: 0.4037 - val_mean_iou: 0.5876 - val_binary_crossentropy: 0.4037\n",
            "Epoch 32/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0237 - mean_iou: 0.8706 - binary_crossentropy: 0.0237 - val_loss: 0.4200 - val_mean_iou: 0.5922 - val_binary_crossentropy: 0.4200\n",
            "Epoch 33/20000\n",
            "25/25 [==============================] - 15s 606ms/step - loss: 0.0238 - mean_iou: 0.8646 - binary_crossentropy: 0.0238 - val_loss: 0.4526 - val_mean_iou: 0.5748 - val_binary_crossentropy: 0.4526\n",
            "Epoch 34/20000\n",
            "25/25 [==============================] - 15s 598ms/step - loss: 0.0221 - mean_iou: 0.8777 - binary_crossentropy: 0.0221 - val_loss: 0.4592 - val_mean_iou: 0.6189 - val_binary_crossentropy: 0.4592\n",
            "Epoch 35/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0217 - mean_iou: 0.8778 - binary_crossentropy: 0.0217 - val_loss: 0.4528 - val_mean_iou: 0.5603 - val_binary_crossentropy: 0.4528\n",
            "Epoch 36/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0211 - mean_iou: 0.8700 - binary_crossentropy: 0.0211 - val_loss: 0.3685 - val_mean_iou: 0.5607 - val_binary_crossentropy: 0.3685\n",
            "Epoch 37/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0199 - mean_iou: 0.8738 - binary_crossentropy: 0.0199 - val_loss: 0.4511 - val_mean_iou: 0.5674 - val_binary_crossentropy: 0.4511\n",
            "Epoch 38/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0192 - mean_iou: 0.8910 - binary_crossentropy: 0.0192 - val_loss: 0.4230 - val_mean_iou: 0.5901 - val_binary_crossentropy: 0.4230\n",
            "Epoch 39/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0185 - mean_iou: 0.8872 - binary_crossentropy: 0.0185 - val_loss: 0.4914 - val_mean_iou: 0.5720 - val_binary_crossentropy: 0.4914\n",
            "Epoch 40/20000\n",
            "25/25 [==============================] - 15s 590ms/step - loss: 0.0179 - mean_iou: 0.8885 - binary_crossentropy: 0.0179 - val_loss: 0.4054 - val_mean_iou: 0.5908 - val_binary_crossentropy: 0.4054\n",
            "Epoch 41/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0178 - mean_iou: 0.8980 - binary_crossentropy: 0.0178 - val_loss: 0.3738 - val_mean_iou: 0.5912 - val_binary_crossentropy: 0.3738\n",
            "Epoch 42/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0168 - mean_iou: 0.8964 - binary_crossentropy: 0.0168 - val_loss: 0.4596 - val_mean_iou: 0.5588 - val_binary_crossentropy: 0.4596\n",
            "Epoch 43/20000\n",
            "25/25 [==============================] - 14s 578ms/step - loss: 0.0169 - mean_iou: 0.8994 - binary_crossentropy: 0.0169 - val_loss: 0.4883 - val_mean_iou: 0.5498 - val_binary_crossentropy: 0.4883\n",
            "Epoch 44/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0177 - mean_iou: 0.9015 - binary_crossentropy: 0.0177 - val_loss: 0.4626 - val_mean_iou: 0.5843 - val_binary_crossentropy: 0.4626\n",
            "Epoch 45/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0166 - mean_iou: 0.9026 - binary_crossentropy: 0.0166 - val_loss: 0.4954 - val_mean_iou: 0.5685 - val_binary_crossentropy: 0.4954\n",
            "Epoch 46/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0160 - mean_iou: 0.9061 - binary_crossentropy: 0.0160 - val_loss: 0.4517 - val_mean_iou: 0.5642 - val_binary_crossentropy: 0.4517\n",
            "Epoch 47/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0174 - mean_iou: 0.8941 - binary_crossentropy: 0.0174 - val_loss: 0.5435 - val_mean_iou: 0.5664 - val_binary_crossentropy: 0.5435\n",
            "Epoch 48/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0163 - mean_iou: 0.8969 - binary_crossentropy: 0.0163 - val_loss: 0.4538 - val_mean_iou: 0.5933 - val_binary_crossentropy: 0.4538\n",
            "Epoch 49/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0156 - mean_iou: 0.9102 - binary_crossentropy: 0.0156 - val_loss: 0.5200 - val_mean_iou: 0.5441 - val_binary_crossentropy: 0.5200\n",
            "Epoch 50/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0148 - mean_iou: 0.9059 - binary_crossentropy: 0.0148 - val_loss: 0.4507 - val_mean_iou: 0.5718 - val_binary_crossentropy: 0.4507\n",
            "Epoch 51/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0153 - mean_iou: 0.9001 - binary_crossentropy: 0.0153 - val_loss: 0.5246 - val_mean_iou: 0.5751 - val_binary_crossentropy: 0.5246\n",
            "Epoch 52/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0148 - mean_iou: 0.9071 - binary_crossentropy: 0.0148 - val_loss: 0.4605 - val_mean_iou: 0.5797 - val_binary_crossentropy: 0.4605\n",
            "Epoch 53/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0146 - mean_iou: 0.9071 - binary_crossentropy: 0.0146 - val_loss: 0.4834 - val_mean_iou: 0.5613 - val_binary_crossentropy: 0.4834\n",
            "Epoch 54/20000\n",
            "25/25 [==============================] - 15s 595ms/step - loss: 0.0142 - mean_iou: 0.9095 - binary_crossentropy: 0.0142 - val_loss: 0.5567 - val_mean_iou: 0.5410 - val_binary_crossentropy: 0.5567\n",
            "Epoch 55/20000\n",
            "25/25 [==============================] - 15s 607ms/step - loss: 0.0178 - mean_iou: 0.8979 - binary_crossentropy: 0.0178 - val_loss: 0.4775 - val_mean_iou: 0.5676 - val_binary_crossentropy: 0.4775\n",
            "Epoch 56/20000\n",
            "25/25 [==============================] - 15s 591ms/step - loss: 0.0178 - mean_iou: 0.8851 - binary_crossentropy: 0.0178 - val_loss: 0.6088 - val_mean_iou: 0.5631 - val_binary_crossentropy: 0.6088\n",
            "Epoch 57/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0173 - mean_iou: 0.8925 - binary_crossentropy: 0.0173 - val_loss: 0.4420 - val_mean_iou: 0.5770 - val_binary_crossentropy: 0.4420\n",
            "Epoch 58/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0183 - mean_iou: 0.8932 - binary_crossentropy: 0.0183 - val_loss: 0.4547 - val_mean_iou: 0.5875 - val_binary_crossentropy: 0.4547\n",
            "Epoch 59/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0156 - mean_iou: 0.9020 - binary_crossentropy: 0.0156 - val_loss: 0.5415 - val_mean_iou: 0.5616 - val_binary_crossentropy: 0.5415\n",
            "Epoch 60/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0215 - mean_iou: 0.8715 - binary_crossentropy: 0.0215 - val_loss: 0.5161 - val_mean_iou: 0.5268 - val_binary_crossentropy: 0.5161\n",
            "Epoch 61/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0848 - mean_iou: 0.7303 - binary_crossentropy: 0.0848 - val_loss: 0.7954 - val_mean_iou: 0.1606 - val_binary_crossentropy: 0.7954\n",
            "Epoch 62/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.1365 - mean_iou: 0.6316 - binary_crossentropy: 0.1365 - val_loss: 3.6842 - val_mean_iou: 0.3393 - val_binary_crossentropy: 3.6842\n",
            "Epoch 63/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0680 - mean_iou: 0.7460 - binary_crossentropy: 0.0680 - val_loss: 1.9491 - val_mean_iou: 0.3029 - val_binary_crossentropy: 1.9491\n",
            "Epoch 64/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0481 - mean_iou: 0.7855 - binary_crossentropy: 0.0481 - val_loss: 1.0925 - val_mean_iou: 0.3657 - val_binary_crossentropy: 1.0925\n",
            "Epoch 65/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0348 - mean_iou: 0.8201 - binary_crossentropy: 0.0348 - val_loss: 0.6743 - val_mean_iou: 0.4895 - val_binary_crossentropy: 0.6743\n",
            "Epoch 66/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0289 - mean_iou: 0.8464 - binary_crossentropy: 0.0289 - val_loss: 0.5591 - val_mean_iou: 0.5504 - val_binary_crossentropy: 0.5591\n",
            "Epoch 67/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0252 - mean_iou: 0.8522 - binary_crossentropy: 0.0252 - val_loss: 0.5097 - val_mean_iou: 0.5643 - val_binary_crossentropy: 0.5097\n",
            "Epoch 68/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0242 - mean_iou: 0.8556 - binary_crossentropy: 0.0242 - val_loss: 0.3809 - val_mean_iou: 0.5820 - val_binary_crossentropy: 0.3809\n",
            "Epoch 69/20000\n",
            "25/25 [==============================] - 15s 581ms/step - loss: 0.0223 - mean_iou: 0.8748 - binary_crossentropy: 0.0223 - val_loss: 0.4978 - val_mean_iou: 0.5854 - val_binary_crossentropy: 0.4978\n",
            "Epoch 70/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0205 - mean_iou: 0.8814 - binary_crossentropy: 0.0205 - val_loss: 0.4880 - val_mean_iou: 0.5621 - val_binary_crossentropy: 0.4880\n",
            "Epoch 71/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0210 - mean_iou: 0.8840 - binary_crossentropy: 0.0210 - val_loss: 0.4412 - val_mean_iou: 0.5910 - val_binary_crossentropy: 0.4412\n",
            "Epoch 72/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0286 - mean_iou: 0.8350 - binary_crossentropy: 0.0286 - val_loss: 0.4949 - val_mean_iou: 0.5194 - val_binary_crossentropy: 0.4949\n",
            "Epoch 73/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0225 - mean_iou: 0.8678 - binary_crossentropy: 0.0225 - val_loss: 0.3800 - val_mean_iou: 0.6010 - val_binary_crossentropy: 0.3800\n",
            "Epoch 74/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0195 - mean_iou: 0.8914 - binary_crossentropy: 0.0195 - val_loss: 0.4635 - val_mean_iou: 0.5817 - val_binary_crossentropy: 0.4635\n",
            "Epoch 75/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0177 - mean_iou: 0.8965 - binary_crossentropy: 0.0177 - val_loss: 0.4247 - val_mean_iou: 0.5956 - val_binary_crossentropy: 0.4247\n",
            "Epoch 76/20000\n",
            "25/25 [==============================] - 15s 617ms/step - loss: 0.0168 - mean_iou: 0.8947 - binary_crossentropy: 0.0168 - val_loss: 0.4588 - val_mean_iou: 0.5950 - val_binary_crossentropy: 0.4588\n",
            "Epoch 77/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0167 - mean_iou: 0.8890 - binary_crossentropy: 0.0167 - val_loss: 0.4334 - val_mean_iou: 0.5963 - val_binary_crossentropy: 0.4334\n",
            "Epoch 78/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0159 - mean_iou: 0.9010 - binary_crossentropy: 0.0159 - val_loss: 0.4149 - val_mean_iou: 0.5860 - val_binary_crossentropy: 0.4149\n",
            "Epoch 79/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0154 - mean_iou: 0.9026 - binary_crossentropy: 0.0154 - val_loss: 0.4569 - val_mean_iou: 0.5794 - val_binary_crossentropy: 0.4569\n",
            "Epoch 80/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0159 - mean_iou: 0.9024 - binary_crossentropy: 0.0159 - val_loss: 0.3985 - val_mean_iou: 0.5846 - val_binary_crossentropy: 0.3985\n",
            "Epoch 81/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0158 - mean_iou: 0.9044 - binary_crossentropy: 0.0158 - val_loss: 0.5657 - val_mean_iou: 0.5821 - val_binary_crossentropy: 0.5657\n",
            "Epoch 82/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0152 - mean_iou: 0.9074 - binary_crossentropy: 0.0152 - val_loss: 0.4235 - val_mean_iou: 0.5999 - val_binary_crossentropy: 0.4235\n",
            "Epoch 83/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0149 - mean_iou: 0.8989 - binary_crossentropy: 0.0149 - val_loss: 0.5157 - val_mean_iou: 0.5669 - val_binary_crossentropy: 0.5157\n",
            "Epoch 84/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0142 - mean_iou: 0.9139 - binary_crossentropy: 0.0142 - val_loss: 0.4821 - val_mean_iou: 0.5698 - val_binary_crossentropy: 0.4821\n",
            "Epoch 85/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0143 - mean_iou: 0.9135 - binary_crossentropy: 0.0143 - val_loss: 0.5035 - val_mean_iou: 0.5691 - val_binary_crossentropy: 0.5035\n",
            "Epoch 86/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0135 - mean_iou: 0.9182 - binary_crossentropy: 0.0135 - val_loss: 0.5678 - val_mean_iou: 0.5384 - val_binary_crossentropy: 0.5678\n",
            "Epoch 87/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0128 - mean_iou: 0.9173 - binary_crossentropy: 0.0128 - val_loss: 0.4173 - val_mean_iou: 0.5855 - val_binary_crossentropy: 0.4173\n",
            "Epoch 88/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0132 - mean_iou: 0.9094 - binary_crossentropy: 0.0132 - val_loss: 0.4917 - val_mean_iou: 0.5896 - val_binary_crossentropy: 0.4917\n",
            "Epoch 89/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0129 - mean_iou: 0.9191 - binary_crossentropy: 0.0129 - val_loss: 0.4893 - val_mean_iou: 0.5790 - val_binary_crossentropy: 0.4893\n",
            "Epoch 90/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0123 - mean_iou: 0.9135 - binary_crossentropy: 0.0123 - val_loss: 0.5113 - val_mean_iou: 0.5717 - val_binary_crossentropy: 0.5113\n",
            "Epoch 91/20000\n",
            "25/25 [==============================] - 15s 581ms/step - loss: 0.0126 - mean_iou: 0.9195 - binary_crossentropy: 0.0126 - val_loss: 0.5524 - val_mean_iou: 0.5650 - val_binary_crossentropy: 0.5524\n",
            "Epoch 92/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0127 - mean_iou: 0.9149 - binary_crossentropy: 0.0127 - val_loss: 0.5472 - val_mean_iou: 0.5651 - val_binary_crossentropy: 0.5472\n",
            "Epoch 93/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0127 - mean_iou: 0.9231 - binary_crossentropy: 0.0127 - val_loss: 0.4653 - val_mean_iou: 0.5738 - val_binary_crossentropy: 0.4653\n",
            "Epoch 94/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0128 - mean_iou: 0.9198 - binary_crossentropy: 0.0128 - val_loss: 0.5658 - val_mean_iou: 0.5622 - val_binary_crossentropy: 0.5658\n",
            "Epoch 95/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0121 - mean_iou: 0.9265 - binary_crossentropy: 0.0121 - val_loss: 0.5979 - val_mean_iou: 0.5654 - val_binary_crossentropy: 0.5979\n",
            "Epoch 96/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0113 - mean_iou: 0.9294 - binary_crossentropy: 0.0113 - val_loss: 0.5162 - val_mean_iou: 0.5883 - val_binary_crossentropy: 0.5162\n",
            "Epoch 97/20000\n",
            "25/25 [==============================] - 16s 623ms/step - loss: 0.0117 - mean_iou: 0.9246 - binary_crossentropy: 0.0117 - val_loss: 0.5050 - val_mean_iou: 0.5739 - val_binary_crossentropy: 0.5050\n",
            "Epoch 98/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0119 - mean_iou: 0.9242 - binary_crossentropy: 0.0119 - val_loss: 0.4865 - val_mean_iou: 0.5618 - val_binary_crossentropy: 0.4865\n",
            "Epoch 99/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0113 - mean_iou: 0.9229 - binary_crossentropy: 0.0113 - val_loss: 0.5659 - val_mean_iou: 0.5560 - val_binary_crossentropy: 0.5659\n",
            "Epoch 100/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0130 - mean_iou: 0.9210 - binary_crossentropy: 0.0130 - val_loss: 0.4281 - val_mean_iou: 0.5752 - val_binary_crossentropy: 0.4281\n",
            "Epoch 101/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0125 - mean_iou: 0.9131 - binary_crossentropy: 0.0125 - val_loss: 0.5128 - val_mean_iou: 0.5777 - val_binary_crossentropy: 0.5128\n",
            "Epoch 102/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0492 - mean_iou: 0.7843 - binary_crossentropy: 0.0492 - val_loss: 0.9446 - val_mean_iou: 0.3983 - val_binary_crossentropy: 0.9446\n",
            "Epoch 103/20000\n",
            "25/25 [==============================] - 15s 590ms/step - loss: 0.0498 - mean_iou: 0.7729 - binary_crossentropy: 0.0498 - val_loss: 1.0236 - val_mean_iou: 0.3041 - val_binary_crossentropy: 1.0236\n",
            "Epoch 104/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0905 - mean_iou: 0.6983 - binary_crossentropy: 0.0905 - val_loss: 1.4787 - val_mean_iou: 0.3087 - val_binary_crossentropy: 1.4787\n",
            "Epoch 105/20000\n",
            "25/25 [==============================] - 15s 592ms/step - loss: 0.0460 - mean_iou: 0.7901 - binary_crossentropy: 0.0460 - val_loss: 0.7569 - val_mean_iou: 0.4404 - val_binary_crossentropy: 0.7569\n",
            "Epoch 106/20000\n",
            "25/25 [==============================] - 15s 593ms/step - loss: 0.0305 - mean_iou: 0.8267 - binary_crossentropy: 0.0305 - val_loss: 0.5797 - val_mean_iou: 0.5265 - val_binary_crossentropy: 0.5797\n",
            "Epoch 107/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0245 - mean_iou: 0.8618 - binary_crossentropy: 0.0245 - val_loss: 0.4901 - val_mean_iou: 0.5646 - val_binary_crossentropy: 0.4901\n",
            "Epoch 108/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0215 - mean_iou: 0.8784 - binary_crossentropy: 0.0215 - val_loss: 0.4899 - val_mean_iou: 0.5938 - val_binary_crossentropy: 0.4899\n",
            "Epoch 109/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0187 - mean_iou: 0.8917 - binary_crossentropy: 0.0187 - val_loss: 0.4492 - val_mean_iou: 0.5766 - val_binary_crossentropy: 0.4492\n",
            "Epoch 110/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0170 - mean_iou: 0.8893 - binary_crossentropy: 0.0170 - val_loss: 0.3822 - val_mean_iou: 0.6204 - val_binary_crossentropy: 0.3822\n",
            "Epoch 111/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0170 - mean_iou: 0.8892 - binary_crossentropy: 0.0170 - val_loss: 0.5320 - val_mean_iou: 0.5761 - val_binary_crossentropy: 0.5320\n",
            "Epoch 112/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0162 - mean_iou: 0.8993 - binary_crossentropy: 0.0162 - val_loss: 0.3819 - val_mean_iou: 0.5974 - val_binary_crossentropy: 0.3819\n",
            "Epoch 113/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0157 - mean_iou: 0.9063 - binary_crossentropy: 0.0157 - val_loss: 0.4333 - val_mean_iou: 0.6164 - val_binary_crossentropy: 0.4333\n",
            "Epoch 114/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0150 - mean_iou: 0.9060 - binary_crossentropy: 0.0150 - val_loss: 0.4549 - val_mean_iou: 0.6001 - val_binary_crossentropy: 0.4549\n",
            "Epoch 115/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0149 - mean_iou: 0.9059 - binary_crossentropy: 0.0149 - val_loss: 0.4303 - val_mean_iou: 0.6008 - val_binary_crossentropy: 0.4303\n",
            "Epoch 116/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0148 - mean_iou: 0.9169 - binary_crossentropy: 0.0148 - val_loss: 0.4605 - val_mean_iou: 0.5908 - val_binary_crossentropy: 0.4605\n",
            "Epoch 117/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0131 - mean_iou: 0.9168 - binary_crossentropy: 0.0131 - val_loss: 0.3882 - val_mean_iou: 0.6146 - val_binary_crossentropy: 0.3882\n",
            "Epoch 118/20000\n",
            "25/25 [==============================] - 16s 621ms/step - loss: 0.0133 - mean_iou: 0.9175 - binary_crossentropy: 0.0133 - val_loss: 0.4123 - val_mean_iou: 0.5977 - val_binary_crossentropy: 0.4123\n",
            "Epoch 119/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0137 - mean_iou: 0.9151 - binary_crossentropy: 0.0137 - val_loss: 0.4587 - val_mean_iou: 0.5833 - val_binary_crossentropy: 0.4587\n",
            "Epoch 120/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0135 - mean_iou: 0.9198 - binary_crossentropy: 0.0135 - val_loss: 0.4871 - val_mean_iou: 0.5798 - val_binary_crossentropy: 0.4871\n",
            "Epoch 121/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0124 - mean_iou: 0.9225 - binary_crossentropy: 0.0124 - val_loss: 0.5095 - val_mean_iou: 0.5975 - val_binary_crossentropy: 0.5095\n",
            "Epoch 122/20000\n",
            "25/25 [==============================] - 15s 592ms/step - loss: 0.0126 - mean_iou: 0.9217 - binary_crossentropy: 0.0126 - val_loss: 0.4744 - val_mean_iou: 0.5727 - val_binary_crossentropy: 0.4744\n",
            "Epoch 123/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0116 - mean_iou: 0.9259 - binary_crossentropy: 0.0116 - val_loss: 0.4765 - val_mean_iou: 0.6089 - val_binary_crossentropy: 0.4765\n",
            "Epoch 124/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0121 - mean_iou: 0.9254 - binary_crossentropy: 0.0121 - val_loss: 0.5344 - val_mean_iou: 0.5763 - val_binary_crossentropy: 0.5344\n",
            "Epoch 125/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0124 - mean_iou: 0.9224 - binary_crossentropy: 0.0124 - val_loss: 0.4619 - val_mean_iou: 0.5866 - val_binary_crossentropy: 0.4619\n",
            "Epoch 126/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0121 - mean_iou: 0.9233 - binary_crossentropy: 0.0121 - val_loss: 0.4779 - val_mean_iou: 0.5815 - val_binary_crossentropy: 0.4779\n",
            "Epoch 127/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0121 - mean_iou: 0.9326 - binary_crossentropy: 0.0121 - val_loss: 0.5233 - val_mean_iou: 0.5807 - val_binary_crossentropy: 0.5233\n",
            "Epoch 128/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0117 - mean_iou: 0.9267 - binary_crossentropy: 0.0117 - val_loss: 0.6215 - val_mean_iou: 0.5788 - val_binary_crossentropy: 0.6215\n",
            "Epoch 129/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0112 - mean_iou: 0.9312 - binary_crossentropy: 0.0112 - val_loss: 0.4944 - val_mean_iou: 0.6067 - val_binary_crossentropy: 0.4944\n",
            "Epoch 130/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0112 - mean_iou: 0.9269 - binary_crossentropy: 0.0112 - val_loss: 0.5294 - val_mean_iou: 0.5792 - val_binary_crossentropy: 0.5294\n",
            "Epoch 131/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0112 - mean_iou: 0.9295 - binary_crossentropy: 0.0112 - val_loss: 0.4522 - val_mean_iou: 0.5968 - val_binary_crossentropy: 0.4522\n",
            "Epoch 132/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0110 - mean_iou: 0.9264 - binary_crossentropy: 0.0110 - val_loss: 0.5119 - val_mean_iou: 0.5711 - val_binary_crossentropy: 0.5119\n",
            "Epoch 133/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0108 - mean_iou: 0.9238 - binary_crossentropy: 0.0108 - val_loss: 0.5255 - val_mean_iou: 0.5519 - val_binary_crossentropy: 0.5255\n",
            "Epoch 134/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0108 - mean_iou: 0.9280 - binary_crossentropy: 0.0108 - val_loss: 0.5555 - val_mean_iou: 0.5727 - val_binary_crossentropy: 0.5555\n",
            "Epoch 135/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0106 - mean_iou: 0.9343 - binary_crossentropy: 0.0106 - val_loss: 0.6562 - val_mean_iou: 0.5650 - val_binary_crossentropy: 0.6562\n",
            "Epoch 136/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0103 - mean_iou: 0.9290 - binary_crossentropy: 0.0103 - val_loss: 0.5608 - val_mean_iou: 0.5683 - val_binary_crossentropy: 0.5608\n",
            "Epoch 137/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0101 - mean_iou: 0.9349 - binary_crossentropy: 0.0101 - val_loss: 0.5501 - val_mean_iou: 0.5732 - val_binary_crossentropy: 0.5501\n",
            "Epoch 138/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0100 - mean_iou: 0.9336 - binary_crossentropy: 0.0100 - val_loss: 0.6148 - val_mean_iou: 0.5780 - val_binary_crossentropy: 0.6148\n",
            "Epoch 139/20000\n",
            "25/25 [==============================] - 16s 621ms/step - loss: 0.0101 - mean_iou: 0.9334 - binary_crossentropy: 0.0101 - val_loss: 0.5660 - val_mean_iou: 0.5858 - val_binary_crossentropy: 0.5660\n",
            "Epoch 140/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0102 - mean_iou: 0.9302 - binary_crossentropy: 0.0102 - val_loss: 0.5989 - val_mean_iou: 0.5926 - val_binary_crossentropy: 0.5989\n",
            "Epoch 141/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0107 - mean_iou: 0.9327 - binary_crossentropy: 0.0107 - val_loss: 0.5503 - val_mean_iou: 0.5682 - val_binary_crossentropy: 0.5503\n",
            "Epoch 142/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0101 - mean_iou: 0.9386 - binary_crossentropy: 0.0101 - val_loss: 0.5404 - val_mean_iou: 0.6078 - val_binary_crossentropy: 0.5404\n",
            "Epoch 143/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0096 - mean_iou: 0.9342 - binary_crossentropy: 0.0096 - val_loss: 0.5994 - val_mean_iou: 0.5890 - val_binary_crossentropy: 0.5994\n",
            "Epoch 144/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0102 - mean_iou: 0.9341 - binary_crossentropy: 0.0102 - val_loss: 0.5966 - val_mean_iou: 0.5772 - val_binary_crossentropy: 0.5966\n",
            "Epoch 145/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0101 - mean_iou: 0.9369 - binary_crossentropy: 0.0101 - val_loss: 0.5907 - val_mean_iou: 0.5949 - val_binary_crossentropy: 0.5907\n",
            "Epoch 146/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0099 - mean_iou: 0.9428 - binary_crossentropy: 0.0099 - val_loss: 0.6563 - val_mean_iou: 0.5667 - val_binary_crossentropy: 0.6563\n",
            "Epoch 147/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0097 - mean_iou: 0.9346 - binary_crossentropy: 0.0097 - val_loss: 0.5748 - val_mean_iou: 0.5671 - val_binary_crossentropy: 0.5748\n",
            "Epoch 148/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0095 - mean_iou: 0.9408 - binary_crossentropy: 0.0095 - val_loss: 0.4988 - val_mean_iou: 0.5875 - val_binary_crossentropy: 0.4988\n",
            "Epoch 149/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0101 - mean_iou: 0.9372 - binary_crossentropy: 0.0101 - val_loss: 0.5588 - val_mean_iou: 0.5663 - val_binary_crossentropy: 0.5588\n",
            "Epoch 150/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0098 - mean_iou: 0.9378 - binary_crossentropy: 0.0098 - val_loss: 0.6929 - val_mean_iou: 0.5723 - val_binary_crossentropy: 0.6929\n",
            "Epoch 151/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0096 - mean_iou: 0.9415 - binary_crossentropy: 0.0096 - val_loss: 0.6995 - val_mean_iou: 0.5581 - val_binary_crossentropy: 0.6995\n",
            "Epoch 152/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0094 - mean_iou: 0.9343 - binary_crossentropy: 0.0094 - val_loss: 0.5010 - val_mean_iou: 0.5663 - val_binary_crossentropy: 0.5010\n",
            "Epoch 153/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0096 - mean_iou: 0.9427 - binary_crossentropy: 0.0096 - val_loss: 0.6031 - val_mean_iou: 0.5846 - val_binary_crossentropy: 0.6031\n",
            "Epoch 154/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0091 - mean_iou: 0.9383 - binary_crossentropy: 0.0091 - val_loss: 0.5654 - val_mean_iou: 0.5487 - val_binary_crossentropy: 0.5654\n",
            "Epoch 155/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0097 - mean_iou: 0.9342 - binary_crossentropy: 0.0097 - val_loss: 0.5362 - val_mean_iou: 0.5756 - val_binary_crossentropy: 0.5362\n",
            "Epoch 156/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0089 - mean_iou: 0.9385 - binary_crossentropy: 0.0089 - val_loss: 0.5679 - val_mean_iou: 0.5470 - val_binary_crossentropy: 0.5679\n",
            "Epoch 157/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0088 - mean_iou: 0.9426 - binary_crossentropy: 0.0088 - val_loss: 0.7123 - val_mean_iou: 0.5635 - val_binary_crossentropy: 0.7123\n",
            "Epoch 158/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0090 - mean_iou: 0.9415 - binary_crossentropy: 0.0090 - val_loss: 0.5953 - val_mean_iou: 0.5866 - val_binary_crossentropy: 0.5953\n",
            "Epoch 159/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0095 - mean_iou: 0.9353 - binary_crossentropy: 0.0095 - val_loss: 0.4999 - val_mean_iou: 0.5985 - val_binary_crossentropy: 0.4999\n",
            "Epoch 160/20000\n",
            "25/25 [==============================] - 15s 614ms/step - loss: 0.0110 - mean_iou: 0.9334 - binary_crossentropy: 0.0110 - val_loss: 0.6867 - val_mean_iou: 0.5137 - val_binary_crossentropy: 0.6867\n",
            "Epoch 161/20000\n",
            "25/25 [==============================] - 15s 593ms/step - loss: 0.0296 - mean_iou: 0.8406 - binary_crossentropy: 0.0296 - val_loss: 0.6167 - val_mean_iou: 0.5201 - val_binary_crossentropy: 0.6167\n",
            "Epoch 162/20000\n",
            "25/25 [==============================] - 15s 590ms/step - loss: 0.0242 - mean_iou: 0.8667 - binary_crossentropy: 0.0242 - val_loss: 0.5703 - val_mean_iou: 0.5554 - val_binary_crossentropy: 0.5703\n",
            "Epoch 163/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0216 - mean_iou: 0.8629 - binary_crossentropy: 0.0216 - val_loss: 0.6040 - val_mean_iou: 0.5342 - val_binary_crossentropy: 0.6040\n",
            "Epoch 164/20000\n",
            "25/25 [==============================] - 15s 581ms/step - loss: 0.0200 - mean_iou: 0.8761 - binary_crossentropy: 0.0200 - val_loss: 0.4837 - val_mean_iou: 0.6161 - val_binary_crossentropy: 0.4837\n",
            "Epoch 165/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0146 - mean_iou: 0.9076 - binary_crossentropy: 0.0146 - val_loss: 0.4637 - val_mean_iou: 0.6335 - val_binary_crossentropy: 0.4637\n",
            "Epoch 166/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0125 - mean_iou: 0.9219 - binary_crossentropy: 0.0125 - val_loss: 0.6382 - val_mean_iou: 0.5905 - val_binary_crossentropy: 0.6382\n",
            "Epoch 167/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0114 - mean_iou: 0.9209 - binary_crossentropy: 0.0114 - val_loss: 0.5399 - val_mean_iou: 0.5776 - val_binary_crossentropy: 0.5399\n",
            "Epoch 168/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0114 - mean_iou: 0.9220 - binary_crossentropy: 0.0114 - val_loss: 0.4928 - val_mean_iou: 0.5911 - val_binary_crossentropy: 0.4928\n",
            "Epoch 169/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0107 - mean_iou: 0.9258 - binary_crossentropy: 0.0107 - val_loss: 0.5653 - val_mean_iou: 0.5821 - val_binary_crossentropy: 0.5653\n",
            "Epoch 170/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0104 - mean_iou: 0.9334 - binary_crossentropy: 0.0104 - val_loss: 0.5778 - val_mean_iou: 0.5728 - val_binary_crossentropy: 0.5778\n",
            "Epoch 171/20000\n",
            "25/25 [==============================] - 14s 580ms/step - loss: 0.0104 - mean_iou: 0.9327 - binary_crossentropy: 0.0104 - val_loss: 0.5260 - val_mean_iou: 0.6042 - val_binary_crossentropy: 0.5260\n",
            "Epoch 172/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0101 - mean_iou: 0.9344 - binary_crossentropy: 0.0101 - val_loss: 0.5025 - val_mean_iou: 0.5867 - val_binary_crossentropy: 0.5025\n",
            "Epoch 173/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0098 - mean_iou: 0.9390 - binary_crossentropy: 0.0098 - val_loss: 0.5639 - val_mean_iou: 0.5742 - val_binary_crossentropy: 0.5639\n",
            "Epoch 174/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0098 - mean_iou: 0.9354 - binary_crossentropy: 0.0098 - val_loss: 0.4991 - val_mean_iou: 0.5717 - val_binary_crossentropy: 0.4991\n",
            "Epoch 175/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0100 - mean_iou: 0.9336 - binary_crossentropy: 0.0100 - val_loss: 0.5435 - val_mean_iou: 0.5787 - val_binary_crossentropy: 0.5435\n",
            "Epoch 176/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0091 - mean_iou: 0.9456 - binary_crossentropy: 0.0091 - val_loss: 0.6005 - val_mean_iou: 0.5727 - val_binary_crossentropy: 0.6005\n",
            "Epoch 177/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0089 - mean_iou: 0.9474 - binary_crossentropy: 0.0089 - val_loss: 0.5900 - val_mean_iou: 0.5691 - val_binary_crossentropy: 0.5900\n",
            "Epoch 178/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0090 - mean_iou: 0.9456 - binary_crossentropy: 0.0090 - val_loss: 0.6211 - val_mean_iou: 0.5772 - val_binary_crossentropy: 0.6211\n",
            "Epoch 179/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0091 - mean_iou: 0.9420 - binary_crossentropy: 0.0091 - val_loss: 0.5621 - val_mean_iou: 0.5741 - val_binary_crossentropy: 0.5621\n",
            "Epoch 180/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0092 - mean_iou: 0.9445 - binary_crossentropy: 0.0092 - val_loss: 0.5404 - val_mean_iou: 0.5963 - val_binary_crossentropy: 0.5404\n",
            "Epoch 181/20000\n",
            "25/25 [==============================] - 15s 609ms/step - loss: 0.0089 - mean_iou: 0.9447 - binary_crossentropy: 0.0089 - val_loss: 0.5019 - val_mean_iou: 0.5830 - val_binary_crossentropy: 0.5019\n",
            "Epoch 182/20000\n",
            "25/25 [==============================] - 15s 599ms/step - loss: 0.0087 - mean_iou: 0.9458 - binary_crossentropy: 0.0087 - val_loss: 0.6393 - val_mean_iou: 0.5840 - val_binary_crossentropy: 0.6393\n",
            "Epoch 183/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0090 - mean_iou: 0.9445 - binary_crossentropy: 0.0090 - val_loss: 0.5826 - val_mean_iou: 0.5604 - val_binary_crossentropy: 0.5826\n",
            "Epoch 184/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0088 - mean_iou: 0.9441 - binary_crossentropy: 0.0088 - val_loss: 0.6129 - val_mean_iou: 0.5744 - val_binary_crossentropy: 0.6129\n",
            "Epoch 185/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0091 - mean_iou: 0.9452 - binary_crossentropy: 0.0091 - val_loss: 0.6145 - val_mean_iou: 0.5715 - val_binary_crossentropy: 0.6145\n",
            "Epoch 186/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0084 - mean_iou: 0.9406 - binary_crossentropy: 0.0084 - val_loss: 0.6926 - val_mean_iou: 0.5503 - val_binary_crossentropy: 0.6926\n",
            "Epoch 187/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0085 - mean_iou: 0.9418 - binary_crossentropy: 0.0085 - val_loss: 0.5792 - val_mean_iou: 0.5959 - val_binary_crossentropy: 0.5792\n",
            "Epoch 188/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0083 - mean_iou: 0.9472 - binary_crossentropy: 0.0083 - val_loss: 0.5455 - val_mean_iou: 0.5891 - val_binary_crossentropy: 0.5455\n",
            "Epoch 189/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0086 - mean_iou: 0.9471 - binary_crossentropy: 0.0086 - val_loss: 0.7433 - val_mean_iou: 0.5626 - val_binary_crossentropy: 0.7433\n",
            "Epoch 190/20000\n",
            "25/25 [==============================] - 15s 590ms/step - loss: 0.0083 - mean_iou: 0.9471 - binary_crossentropy: 0.0083 - val_loss: 0.6130 - val_mean_iou: 0.5831 - val_binary_crossentropy: 0.6130\n",
            "Epoch 191/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0081 - mean_iou: 0.9446 - binary_crossentropy: 0.0081 - val_loss: 0.6460 - val_mean_iou: 0.5671 - val_binary_crossentropy: 0.6460\n",
            "Epoch 192/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0083 - mean_iou: 0.9443 - binary_crossentropy: 0.0083 - val_loss: 0.5369 - val_mean_iou: 0.5674 - val_binary_crossentropy: 0.5369\n",
            "Epoch 193/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0083 - mean_iou: 0.9473 - binary_crossentropy: 0.0083 - val_loss: 0.5041 - val_mean_iou: 0.5811 - val_binary_crossentropy: 0.5041\n",
            "Epoch 194/20000\n",
            "25/25 [==============================] - 15s 580ms/step - loss: 0.0084 - mean_iou: 0.9548 - binary_crossentropy: 0.0084 - val_loss: 0.6052 - val_mean_iou: 0.5676 - val_binary_crossentropy: 0.6052\n",
            "Epoch 195/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0084 - mean_iou: 0.9465 - binary_crossentropy: 0.0084 - val_loss: 0.7085 - val_mean_iou: 0.5568 - val_binary_crossentropy: 0.7085\n",
            "Epoch 196/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0082 - mean_iou: 0.9517 - binary_crossentropy: 0.0082 - val_loss: 0.6696 - val_mean_iou: 0.5540 - val_binary_crossentropy: 0.6696\n",
            "Epoch 197/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0080 - mean_iou: 0.9463 - binary_crossentropy: 0.0080 - val_loss: 0.6572 - val_mean_iou: 0.5635 - val_binary_crossentropy: 0.6572\n",
            "Epoch 198/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0083 - mean_iou: 0.9462 - binary_crossentropy: 0.0083 - val_loss: 0.6870 - val_mean_iou: 0.5467 - val_binary_crossentropy: 0.6870\n",
            "Epoch 199/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0081 - mean_iou: 0.9458 - binary_crossentropy: 0.0081 - val_loss: 0.6177 - val_mean_iou: 0.5578 - val_binary_crossentropy: 0.6177\n",
            "Epoch 200/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0080 - mean_iou: 0.9481 - binary_crossentropy: 0.0080 - val_loss: 0.7839 - val_mean_iou: 0.5392 - val_binary_crossentropy: 0.7839\n",
            "Epoch 201/20000\n",
            "25/25 [==============================] - 15s 590ms/step - loss: 0.0085 - mean_iou: 0.9472 - binary_crossentropy: 0.0085 - val_loss: 0.6129 - val_mean_iou: 0.5773 - val_binary_crossentropy: 0.6129\n",
            "Epoch 202/20000\n",
            "25/25 [==============================] - 15s 605ms/step - loss: 0.0081 - mean_iou: 0.9501 - binary_crossentropy: 0.0081 - val_loss: 0.5363 - val_mean_iou: 0.6103 - val_binary_crossentropy: 0.5363\n",
            "Epoch 203/20000\n",
            "25/25 [==============================] - 15s 608ms/step - loss: 0.0080 - mean_iou: 0.9518 - binary_crossentropy: 0.0080 - val_loss: 0.6851 - val_mean_iou: 0.5805 - val_binary_crossentropy: 0.6851\n",
            "Epoch 204/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0083 - mean_iou: 0.9468 - binary_crossentropy: 0.0083 - val_loss: 0.7188 - val_mean_iou: 0.5414 - val_binary_crossentropy: 0.7188\n",
            "Epoch 205/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0079 - mean_iou: 0.9468 - binary_crossentropy: 0.0079 - val_loss: 0.5865 - val_mean_iou: 0.5840 - val_binary_crossentropy: 0.5865\n",
            "Epoch 206/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0081 - mean_iou: 0.9494 - binary_crossentropy: 0.0081 - val_loss: 0.6999 - val_mean_iou: 0.5800 - val_binary_crossentropy: 0.6999\n",
            "Epoch 207/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0079 - mean_iou: 0.9488 - binary_crossentropy: 0.0079 - val_loss: 0.6848 - val_mean_iou: 0.5814 - val_binary_crossentropy: 0.6848\n",
            "Epoch 208/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0080 - mean_iou: 0.9511 - binary_crossentropy: 0.0080 - val_loss: 0.6962 - val_mean_iou: 0.5478 - val_binary_crossentropy: 0.6962\n",
            "Epoch 209/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0079 - mean_iou: 0.9556 - binary_crossentropy: 0.0079 - val_loss: 0.6697 - val_mean_iou: 0.5759 - val_binary_crossentropy: 0.6697\n",
            "Epoch 210/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0080 - mean_iou: 0.9495 - binary_crossentropy: 0.0080 - val_loss: 0.6512 - val_mean_iou: 0.5570 - val_binary_crossentropy: 0.6512\n",
            "Epoch 211/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0078 - mean_iou: 0.9485 - binary_crossentropy: 0.0078 - val_loss: 0.7751 - val_mean_iou: 0.5635 - val_binary_crossentropy: 0.7751\n",
            "Epoch 212/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0079 - mean_iou: 0.9543 - binary_crossentropy: 0.0079 - val_loss: 0.7114 - val_mean_iou: 0.5525 - val_binary_crossentropy: 0.7114\n",
            "Epoch 213/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0081 - mean_iou: 0.9503 - binary_crossentropy: 0.0081 - val_loss: 0.6355 - val_mean_iou: 0.5742 - val_binary_crossentropy: 0.6355\n",
            "Epoch 214/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0079 - mean_iou: 0.9470 - binary_crossentropy: 0.0079 - val_loss: 0.6432 - val_mean_iou: 0.5761 - val_binary_crossentropy: 0.6432\n",
            "Epoch 215/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0080 - mean_iou: 0.9469 - binary_crossentropy: 0.0080 - val_loss: 0.6569 - val_mean_iou: 0.5903 - val_binary_crossentropy: 0.6569\n",
            "Epoch 216/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0079 - mean_iou: 0.9422 - binary_crossentropy: 0.0079 - val_loss: 0.6018 - val_mean_iou: 0.5561 - val_binary_crossentropy: 0.6018\n",
            "Epoch 217/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0074 - mean_iou: 0.9528 - binary_crossentropy: 0.0074 - val_loss: 0.6484 - val_mean_iou: 0.5580 - val_binary_crossentropy: 0.6484\n",
            "Epoch 218/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0075 - mean_iou: 0.9508 - binary_crossentropy: 0.0075 - val_loss: 0.5969 - val_mean_iou: 0.5664 - val_binary_crossentropy: 0.5969\n",
            "Epoch 219/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0076 - mean_iou: 0.9493 - binary_crossentropy: 0.0076 - val_loss: 0.6218 - val_mean_iou: 0.5637 - val_binary_crossentropy: 0.6218\n",
            "Epoch 220/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0079 - mean_iou: 0.9554 - binary_crossentropy: 0.0079 - val_loss: 0.7060 - val_mean_iou: 0.5496 - val_binary_crossentropy: 0.7060\n",
            "Epoch 221/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0079 - mean_iou: 0.9430 - binary_crossentropy: 0.0079 - val_loss: 0.7045 - val_mean_iou: 0.5648 - val_binary_crossentropy: 0.7045\n",
            "Epoch 222/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0076 - mean_iou: 0.9538 - binary_crossentropy: 0.0076 - val_loss: 0.6800 - val_mean_iou: 0.5473 - val_binary_crossentropy: 0.6800\n",
            "Epoch 223/20000\n",
            "25/25 [==============================] - 15s 599ms/step - loss: 0.0073 - mean_iou: 0.9517 - binary_crossentropy: 0.0073 - val_loss: 0.6292 - val_mean_iou: 0.5544 - val_binary_crossentropy: 0.6292\n",
            "Epoch 224/20000\n",
            "25/25 [==============================] - 15s 610ms/step - loss: 0.0077 - mean_iou: 0.9573 - binary_crossentropy: 0.0077 - val_loss: 0.6821 - val_mean_iou: 0.5619 - val_binary_crossentropy: 0.6821\n",
            "Epoch 225/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0074 - mean_iou: 0.9608 - binary_crossentropy: 0.0074 - val_loss: 0.6392 - val_mean_iou: 0.5435 - val_binary_crossentropy: 0.6392\n",
            "Epoch 226/20000\n",
            "25/25 [==============================] - 15s 590ms/step - loss: 0.0075 - mean_iou: 0.9515 - binary_crossentropy: 0.0075 - val_loss: 0.6406 - val_mean_iou: 0.5692 - val_binary_crossentropy: 0.6406\n",
            "Epoch 227/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0077 - mean_iou: 0.9556 - binary_crossentropy: 0.0077 - val_loss: 0.6750 - val_mean_iou: 0.5718 - val_binary_crossentropy: 0.6750\n",
            "Epoch 228/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0074 - mean_iou: 0.9543 - binary_crossentropy: 0.0074 - val_loss: 0.6565 - val_mean_iou: 0.5311 - val_binary_crossentropy: 0.6565\n",
            "Epoch 229/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0078 - mean_iou: 0.9528 - binary_crossentropy: 0.0078 - val_loss: 0.6197 - val_mean_iou: 0.5782 - val_binary_crossentropy: 0.6197\n",
            "Epoch 230/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0074 - mean_iou: 0.9510 - binary_crossentropy: 0.0074 - val_loss: 0.6453 - val_mean_iou: 0.5610 - val_binary_crossentropy: 0.6453\n",
            "Epoch 231/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0074 - mean_iou: 0.9533 - binary_crossentropy: 0.0074 - val_loss: 0.5317 - val_mean_iou: 0.5700 - val_binary_crossentropy: 0.5317\n",
            "Epoch 232/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0071 - mean_iou: 0.9503 - binary_crossentropy: 0.0071 - val_loss: 0.6418 - val_mean_iou: 0.5425 - val_binary_crossentropy: 0.6418\n",
            "Epoch 233/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0071 - mean_iou: 0.9537 - binary_crossentropy: 0.0071 - val_loss: 0.7063 - val_mean_iou: 0.5569 - val_binary_crossentropy: 0.7063\n",
            "Epoch 234/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0074 - mean_iou: 0.9536 - binary_crossentropy: 0.0074 - val_loss: 0.6576 - val_mean_iou: 0.5715 - val_binary_crossentropy: 0.6576\n",
            "Epoch 235/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.2604 - mean_iou: 0.5611 - binary_crossentropy: 0.2604 - val_loss: 3.8294 - val_mean_iou: 0.3606 - val_binary_crossentropy: 3.8294\n",
            "Epoch 236/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.1708 - mean_iou: 0.5673 - binary_crossentropy: 0.1708 - val_loss: 4.1177 - val_mean_iou: 0.3779 - val_binary_crossentropy: 4.1177\n",
            "Epoch 237/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0950 - mean_iou: 0.7033 - binary_crossentropy: 0.0950 - val_loss: 3.5709 - val_mean_iou: 0.3513 - val_binary_crossentropy: 3.5709\n",
            "Epoch 238/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0699 - mean_iou: 0.7313 - binary_crossentropy: 0.0699 - val_loss: 1.4687 - val_mean_iou: 0.3644 - val_binary_crossentropy: 1.4687\n",
            "Epoch 239/20000\n",
            "25/25 [==============================] - 15s 590ms/step - loss: 0.0572 - mean_iou: 0.7626 - binary_crossentropy: 0.0572 - val_loss: 0.6004 - val_mean_iou: 0.4946 - val_binary_crossentropy: 0.6004\n",
            "Epoch 240/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0517 - mean_iou: 0.7730 - binary_crossentropy: 0.0517 - val_loss: 0.4637 - val_mean_iou: 0.5792 - val_binary_crossentropy: 0.4637\n",
            "Epoch 241/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0415 - mean_iou: 0.8008 - binary_crossentropy: 0.0415 - val_loss: 0.4038 - val_mean_iou: 0.5808 - val_binary_crossentropy: 0.4038\n",
            "Epoch 242/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0369 - mean_iou: 0.8190 - binary_crossentropy: 0.0369 - val_loss: 0.3622 - val_mean_iou: 0.6420 - val_binary_crossentropy: 0.3622\n",
            "Epoch 243/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0313 - mean_iou: 0.8209 - binary_crossentropy: 0.0313 - val_loss: 0.3905 - val_mean_iou: 0.6205 - val_binary_crossentropy: 0.3905\n",
            "Epoch 244/20000\n",
            "25/25 [==============================] - 15s 594ms/step - loss: 0.0299 - mean_iou: 0.8437 - binary_crossentropy: 0.0299 - val_loss: 0.3932 - val_mean_iou: 0.5983 - val_binary_crossentropy: 0.3932\n",
            "Epoch 245/20000\n",
            "25/25 [==============================] - 15s 614ms/step - loss: 0.0271 - mean_iou: 0.8526 - binary_crossentropy: 0.0271 - val_loss: 0.3718 - val_mean_iou: 0.6021 - val_binary_crossentropy: 0.3718\n",
            "Epoch 246/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0268 - mean_iou: 0.8535 - binary_crossentropy: 0.0268 - val_loss: 0.4165 - val_mean_iou: 0.5564 - val_binary_crossentropy: 0.4165\n",
            "Epoch 247/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0240 - mean_iou: 0.8531 - binary_crossentropy: 0.0240 - val_loss: 0.3990 - val_mean_iou: 0.5913 - val_binary_crossentropy: 0.3990\n",
            "Epoch 248/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0232 - mean_iou: 0.8571 - binary_crossentropy: 0.0232 - val_loss: 0.3761 - val_mean_iou: 0.6136 - val_binary_crossentropy: 0.3761\n",
            "Epoch 249/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0211 - mean_iou: 0.8825 - binary_crossentropy: 0.0211 - val_loss: 0.4756 - val_mean_iou: 0.5981 - val_binary_crossentropy: 0.4756\n",
            "Epoch 250/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0221 - mean_iou: 0.8658 - binary_crossentropy: 0.0221 - val_loss: 0.3612 - val_mean_iou: 0.6276 - val_binary_crossentropy: 0.3612\n",
            "Epoch 251/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0205 - mean_iou: 0.8766 - binary_crossentropy: 0.0205 - val_loss: 0.4224 - val_mean_iou: 0.6088 - val_binary_crossentropy: 0.4224\n",
            "Epoch 252/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0188 - mean_iou: 0.8873 - binary_crossentropy: 0.0188 - val_loss: 0.4848 - val_mean_iou: 0.6334 - val_binary_crossentropy: 0.4848\n",
            "Epoch 253/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0192 - mean_iou: 0.8913 - binary_crossentropy: 0.0192 - val_loss: 0.4210 - val_mean_iou: 0.6140 - val_binary_crossentropy: 0.4210\n",
            "Epoch 254/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0188 - mean_iou: 0.8950 - binary_crossentropy: 0.0188 - val_loss: 0.4325 - val_mean_iou: 0.6009 - val_binary_crossentropy: 0.4325\n",
            "Epoch 255/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0170 - mean_iou: 0.8904 - binary_crossentropy: 0.0170 - val_loss: 0.4761 - val_mean_iou: 0.5970 - val_binary_crossentropy: 0.4761\n",
            "Epoch 256/20000\n",
            "25/25 [==============================] - 15s 582ms/step - loss: 0.0168 - mean_iou: 0.8886 - binary_crossentropy: 0.0168 - val_loss: 0.4742 - val_mean_iou: 0.6102 - val_binary_crossentropy: 0.4742\n",
            "Epoch 257/20000\n",
            "25/25 [==============================] - 15s 583ms/step - loss: 0.0173 - mean_iou: 0.8974 - binary_crossentropy: 0.0173 - val_loss: 0.5224 - val_mean_iou: 0.6104 - val_binary_crossentropy: 0.5224\n",
            "Epoch 258/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0166 - mean_iou: 0.9005 - binary_crossentropy: 0.0166 - val_loss: 0.4433 - val_mean_iou: 0.5980 - val_binary_crossentropy: 0.4433\n",
            "Epoch 259/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0159 - mean_iou: 0.9026 - binary_crossentropy: 0.0159 - val_loss: 0.5456 - val_mean_iou: 0.6025 - val_binary_crossentropy: 0.5456\n",
            "Epoch 260/20000\n",
            "25/25 [==============================] - 15s 587ms/step - loss: 0.0164 - mean_iou: 0.9010 - binary_crossentropy: 0.0164 - val_loss: 0.4894 - val_mean_iou: 0.5813 - val_binary_crossentropy: 0.4894\n",
            "Epoch 261/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0156 - mean_iou: 0.9075 - binary_crossentropy: 0.0156 - val_loss: 0.5052 - val_mean_iou: 0.5728 - val_binary_crossentropy: 0.5052\n",
            "Epoch 262/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0148 - mean_iou: 0.9088 - binary_crossentropy: 0.0148 - val_loss: 0.5591 - val_mean_iou: 0.6021 - val_binary_crossentropy: 0.5591\n",
            "Epoch 263/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0157 - mean_iou: 0.9028 - binary_crossentropy: 0.0157 - val_loss: 0.5218 - val_mean_iou: 0.5756 - val_binary_crossentropy: 0.5218\n",
            "Epoch 264/20000\n",
            "25/25 [==============================] - 15s 584ms/step - loss: 0.0143 - mean_iou: 0.9124 - binary_crossentropy: 0.0143 - val_loss: 0.4867 - val_mean_iou: 0.5941 - val_binary_crossentropy: 0.4867\n",
            "Epoch 265/20000\n",
            "25/25 [==============================] - 15s 589ms/step - loss: 0.0144 - mean_iou: 0.9119 - binary_crossentropy: 0.0144 - val_loss: 0.5083 - val_mean_iou: 0.5872 - val_binary_crossentropy: 0.5083\n",
            "Epoch 266/20000\n",
            "25/25 [==============================] - 15s 619ms/step - loss: 0.0144 - mean_iou: 0.9126 - binary_crossentropy: 0.0144 - val_loss: 0.4868 - val_mean_iou: 0.5791 - val_binary_crossentropy: 0.4868\n",
            "Epoch 267/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0137 - mean_iou: 0.9120 - binary_crossentropy: 0.0137 - val_loss: 0.5789 - val_mean_iou: 0.5680 - val_binary_crossentropy: 0.5789\n",
            "Epoch 268/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0140 - mean_iou: 0.9144 - binary_crossentropy: 0.0140 - val_loss: 0.5213 - val_mean_iou: 0.5801 - val_binary_crossentropy: 0.5213\n",
            "Epoch 269/20000\n",
            "25/25 [==============================] - 15s 586ms/step - loss: 0.0134 - mean_iou: 0.9116 - binary_crossentropy: 0.0134 - val_loss: 0.5247 - val_mean_iou: 0.5720 - val_binary_crossentropy: 0.5247\n",
            "Epoch 270/20000\n",
            "25/25 [==============================] - 15s 588ms/step - loss: 0.0127 - mean_iou: 0.9146 - binary_crossentropy: 0.0127 - val_loss: 0.5523 - val_mean_iou: 0.5816 - val_binary_crossentropy: 0.5523\n",
            "Epoch 271/20000\n",
            "25/25 [==============================] - 15s 585ms/step - loss: 0.0129 - mean_iou: 0.9218 - binary_crossentropy: 0.0129 - val_loss: 0.5110 - val_mean_iou: 0.5888 - val_binary_crossentropy: 0.5110\n",
            "Epoch 272/20000\n",
            "18/25 [====================>.........] - ETA: 4s - loss: 0.0126 - mean_iou: 0.9249 - binary_crossentropy: 0.0126"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ab248f3e4658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-ab248f3e4658>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(X_train, y_train, X_test, fold, fast)\u001b[0m\n\u001b[1;32m     75\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_data_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;31m#           callbacks=[early_stopping, model_checkpoint, reduce_lr],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#           callbacks=[early_stopping, reduce_lr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1487\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0;31m# Sample-wise fit loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_step_wise\u001b[0;34m(self, ins, callbacks, steps_per_epoch, epochs, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m           \u001b[0mnext_step_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         outs = f.pipeline_run(cur_step_inputs=ins,\n\u001b[0;32m-> 1631\u001b[0;31m                               next_step_inputs=next_step_inputs)\n\u001b[0m\u001b[1;32m   1632\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         logging.warning('Your dataset iterator ran out of data; '\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mpipeline_run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0mnext_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         ], infeed_dict)\n\u001b[0m\u001b[1;32m   1089\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uf9lolkBBkSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6fwJZzEBkQl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uk1SXFM3BkNT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yQXxqr0MAqoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tFBYhgB5NEjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "71db68f4-a00c-46db-c6d5-250cb187778d"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def test_model():\n",
        "  x = tf.keras.layers.Input(shape=(101,101,1), dtype=np.float)\n",
        "  y = tf.keras.layers.Conv2D(1,1, padding='same', activation='sigmoid', dtype=np.float)(x)\n",
        "  m = tf.keras.Model(x, y)\n",
        "  return m\n",
        "\n",
        "model = test_model()\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 101, 101, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 101, 101, 1)       2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4tHvN3g6PA92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "4d8bf063-37b5-4805-a9d5-452f8815d2af"
      },
      "cell_type": "code",
      "source": [
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "session_master = resolver.master()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6931466752683336584)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2319523528817134981)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2621229120846880910)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13017604127097533172)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13684817192547276644)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15939364587101623240)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6777068171437765544)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 677236183219516705)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 17055041862000479642)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13317328517731664289)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7926127133639364078)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9787759974722640898)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: grpc://10.240.1.2:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cXtj4ZEb85dv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIyUjH87qLvJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2068
        },
        "outputId": "ea6b8821-4837-4968-ea22-0ada27acd56a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def _set_shape(image, mask):\n",
        "  image.set_shape([101, 101, 1])\n",
        "  mask.set_shape([101, 101, 1])\n",
        "  return image, mask\n",
        "\n",
        "def input_fn():\n",
        "  images = tf.random_uniform([1024, 101, 101, 1])\n",
        "  masks = tf.random_uniform([1024, 101, 101, 1])\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
        "#   print(dataset)\n",
        "#   dataset = dataset.map(lambda image, mask: tuple(tf.py_func(random_h_flip, [image, mask], [tf.float32, tf.float32])))\n",
        "#   print(dataset)\n",
        "  dataset = dataset.map(_random_flip_left_right)\n",
        "#   print(dataset)\n",
        "  dataset = dataset.shuffle(1000).repeat(100000).batch(32, drop_remainder=True)\n",
        "#   print(dataset)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01),\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['binary_crossentropy'])\n",
        "\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    input_fn,\n",
        "    validation_data=input_fn,\n",
        "    validation_steps=100,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<MapDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<BatchDataset shapes: ((32, 101, 101, 1), (32, 101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<MapDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<BatchDataset shapes: ((32, 101, 101, 1), (32, 101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 11.059766054153442 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.6932 - binary_crossentropy: 0.6932INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 10.902057647705078 secs\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.6932 - binary_crossentropy: 0.6932 - val_loss: 0.6932 - val_binary_crossentropy: 0.6932\n",
            "Epoch 2/20\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.6932 - binary_crossentropy: 0.6932"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-292190155012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     epochs=20)\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1487\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0;31m# Sample-wise fit loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_step_wise\u001b[0;34m(self, ins, callbacks, steps_per_epoch, epochs, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                                            \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                                            \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                                            verbose=0)\n\u001b[0m\u001b[1;32m   1657\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    402\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1006\u001b[0m           \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m           \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       ], infeed_dict)\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6XdIwEcAqor5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2241
        },
        "outputId": "6f555f74-f366-4eb1-bd79-7687c211c77e"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef expected inputs 'float' do not match 2 inputs specified; Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\"](training/TFOptimizer/gradients/tpu_model_140369801514736/conv2d_1/Conv2D_grad/tuple/control_dependency_1, training/TFOptimizer/CrossReplicaSum/group_assignment)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-959ee3a79e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_sample_wise\u001b[0;34m(self, ins, callbacks, index_array, shuffle, batch_size, num_training_samples, indices_for_conversion_to_dense, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m       outs = f.pipeline_run(cur_step_inputs=ins_last_batch,\n\u001b[0;32m-> 1558\u001b[0;31m                             next_step_inputs=ins_batch)\n\u001b[0m\u001b[1;32m   1559\u001b[0m       \u001b[0mins_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mpipeline_run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1075\u001b[0m           next_input_tensors)\n\u001b[1;32m   1076\u001b[0m       next_tpu_model_ops = self._tpu_model_ops_for_input_specs(\n\u001b[0;32m-> 1077\u001b[0;31m           next_input_specs, next_step_infeed_manager)\n\u001b[0m\u001b[1;32m   1078\u001b[0m       \u001b[0minfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_infeed_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_tpu_model_ops_for_input_specs\u001b[0;34m(self, input_specs, infeed_manager)\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mshape_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_specs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m         logging.info('New input shapes; (re-)compiling: mode=%s, %s',\n\u001b[1;32m    915\u001b[0m                      self.execution_mode, input_specs)\n",
            "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mtpu_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[0;34m\"\"\"Yields a TPU session and sets it as the default Keras session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m       \u001b[0mdefault_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1812\u001b[0m       \u001b[0;31m# N.B. We have to call `K.set_session()` AND set our session as the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m       \u001b[0;31m# TF default. `K.get_session()` surprisingly does not return the value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 710\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef expected inputs 'float' do not match 2 inputs specified; Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\"](training/TFOptimizer/gradients/tpu_model_140369801514736/conv2d_1/Conv2D_grad/tuple/control_dependency_1, training/TFOptimizer/CrossReplicaSum/group_assignment)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ibenmWsXzMNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7fe904b0-88ba-4e00-f9a6-1dc9f27aecbc"
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"ResNet-50 implemented with Keras running on Cloud TPUs.\n",
        "This file shows how you can run ResNet-50 on a Cloud TPU using the TensorFlow\n",
        "Keras support. This is configured for ImageNet (e.g. 1000 classes), but you can\n",
        "easily adapt to your own datasets by changing the code appropriately.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import imagenet_input\n",
        "\n",
        "try:\n",
        "  import h5py as _  # pylint: disable=g-import-not-at-top\n",
        "  HAS_H5PY = True\n",
        "except ImportError:\n",
        "  logging.warning('`h5py` is not installed. Please consider installing it '\n",
        "                  'to save weights for long-running training.')\n",
        "  HAS_H5PY = False\n",
        "\n",
        "\n",
        "flags.DEFINE_bool('use_tpu', True, 'Use TPU model instead of CPU.')\n",
        "flags.DEFINE_string('tpu', None, 'Name of the TPU to use.')\n",
        "flags.DEFINE_string('data', None, 'Path to training and testing data.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "PER_CORE_BATCH_SIZE = 128\n",
        "NUM_CLASSES = 1000\n",
        "IMAGE_SIZE = 224\n",
        "EPOCHS = 90  # Standard imagenet training regime.\n",
        "APPROX_IMAGENET_TRAINING_IMAGES = 1280000  # Approximate number of images.\n",
        "APPROX_IMAGENET_TEST_IMAGES = 48000  # Approximate number of images.\n",
        "\n",
        "WEIGHTS_TXT = '/tmp/resnet50_weights.h5'\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "  logging.info('Building Keras ResNet-50 model.')\n",
        "  model = tf.keras.applications.resnet50.ResNet50(\n",
        "      include_top=True,\n",
        "      weights=None,\n",
        "      input_tensor=None,\n",
        "      input_shape=None,\n",
        "      pooling=None,\n",
        "      classes=NUM_CLASSES)\n",
        "\n",
        "  num_cores = 8\n",
        "  batch_size = PER_CORE_BATCH_SIZE * num_cores\n",
        "\n",
        "  if FLAGS.use_tpu:\n",
        "    logging.info('Converting from CPU to TPU model.')\n",
        "    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "    session_master = resolver.master()\n",
        "  else:\n",
        "    session_master = ''\n",
        "\n",
        "  logging.info('Compiling model.')\n",
        "  model.compile(\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "  if FLAGS.data is None:\n",
        "    training_images = np.random.randn(\n",
        "        batch_size, IMAGE_SIZE, IMAGE_SIZE, 3).astype(np.float32)\n",
        "    training_labels = np.random.randint(NUM_CLASSES, size=batch_size,\n",
        "                                        dtype=np.int32)\n",
        "    logging.info('Training model using synthetica data.')\n",
        "    model.fit(training_images, training_labels, epochs=EPOCHS,\n",
        "              batch_size=batch_size)\n",
        "    logging.info('Evaluating the model on synthetic data.')\n",
        "    model.evaluate(training_images, training_labels, verbose=0)\n",
        "  else:\n",
        "    imagenet_train, imagenet_eval = [imagenet_input.ImageNetInput(\n",
        "        is_training=is_training,\n",
        "        data_dir=FLAGS.data,\n",
        "        per_core_batch_size=PER_CORE_BATCH_SIZE)\n",
        "                                     for is_training in [True, False]]\n",
        "    logging.info('Training model using real data in directory \"%s\".',\n",
        "                 FLAGS.data)\n",
        "    model.fit(imagenet_train.input_fn,\n",
        "              epochs=EPOCHS,\n",
        "              steps_per_epoch=int(APPROX_IMAGENET_TRAINING_IMAGES / batch_size))\n",
        "\n",
        "    if HAS_H5PY:\n",
        "      logging.info('Save weights into %s', WEIGHTS_TXT)\n",
        "      model.save_weights(WEIGHTS_TXT, overwrite=True)\n",
        "\n",
        "    logging.info('Evaluating the model on the validation dataset.')\n",
        "    score = model.evaluate(\n",
        "        imagenet_eval.input_fn,\n",
        "        steps=int(APPROX_IMAGENET_TEST_IMAGES // batch_size),\n",
        "        verbose=1)\n",
        "    print('Evaluation score', score)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c62c05d6ad63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: No module named 'imagenet_input'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "r9DlwwbXq6o_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import dotenv\n",
        "# import keras\n",
        "import os\n",
        "dotenv.load_dotenv('.env')\n",
        "\n",
        "!mkdir model\n",
        "# !gsutil rsync gs://{os.environ['GCP_BUCKET']}/model model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoL4Y1yXqtaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5cb32500-ba58-4b15-c239-2fd8bf232040"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('raw/train.csv').set_index('id')\n",
        "test = pd.read_csv('raw/sample_submission.csv').set_index('id')\n",
        "depths = pd.read_csv('raw/depths.csv').set_index('id')\n",
        "\n",
        "train = pd.merge(train, depths, left_index=True, right_index=True, how='inner')\n",
        "train.shape, test.shape, depths.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 2), (18000, 1), (22000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "6TALjxylQlox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_size_target = 101"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6nC41yXq8j0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "73d65f68-7590-4b43-f4b5-f712fd457b61"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "\n",
        "train[\"image\"] = [np.array(load_img(\"./raw/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train.index)]\n",
        "train[\"mask\"] = [np.array(load_img(\"./raw/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train.index)]\n",
        "\n",
        "X_train = np.array(train['image'].tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
        "y_train = np.array(train['mask'].tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
        "\n",
        "X_test = np.array([np.array(load_img(\"./raw/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(test.index)]).reshape(-1, img_size_target, img_size_target, 1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "  0%|          | 0/4000 [00:00<?, ?it/s]/home/Hoiy/.local/lib/python3.5/site-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n",
            "100%|██████████| 4000/4000 [00:02<00:00, 1517.39it/s]\n",
            "100%|██████████| 4000/4000 [00:01<00:00, 3034.27it/s]\n",
            "100%|██████████| 18000/18000 [00:11<00:00, 1533.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ns-p0XzLKHQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16f71cf5-ebcf-44e8-9fa0-96bb485f9f53"
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32', copy=False)\n",
        "y_train = y_train.astype('float32', copy=False)\n",
        "\n",
        "X_test = X_test.astype('float32', copy=False)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 101, 101, 1), (4000, 101, 101, 1), (18000, 101, 101, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "EKDfbbaX_1WY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_config\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_estimator\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWtHOnEjhZs5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def sweep_thresholds_mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "def metric_fn(y_true, y_pred):\n",
        "  return {\n",
        "    'sweep_thresholds_mean_iou': sweep_thresholds_mean_iou(y_true, y_pred)\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIPW4QCxJEYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def host_call_fn(gs, loss, lr, ce):\n",
        "#   \"\"\"Training host call. Creates scalar summaries for training metrics.\n",
        "#   This function is executed on the CPU and should not directly reference\n",
        "#   any Tensors in the rest of the `model_fn`. To pass Tensors from the\n",
        "#   model to the `metric_fn`, provide as part of the `host_call`. See\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "#   for more information.\n",
        "#   Arguments should match the list of `Tensor` objects passed as the second\n",
        "#   element in the tuple passed to `host_call`.\n",
        "#   Args:\n",
        "#     gs: `Tensor with shape `[batch]` for the global_step\n",
        "#     loss: `Tensor` with shape `[batch]` for the training loss.\n",
        "#     lr: `Tensor` with shape `[batch]` for the learning_rate.\n",
        "#     ce: `Tensor` with shape `[batch]` for the current_epoch.\n",
        "#   Returns:\n",
        "#     List of summary ops to run on the CPU host.\n",
        "#   \"\"\"\n",
        "#   gs = gs[0]\n",
        "#   # Host call fns are executed FLAGS.iterations_per_loop times after one\n",
        "#   # TPU loop is finished, setting max_queue value to the same as number of\n",
        "#   # iterations will make the summary writer only flush the data to storage\n",
        "#   # once per loop.\n",
        "#   with summary.create_file_writer(\n",
        "#       FLAGS.model_dir, max_queue=FLAGS.iterations_per_loop).as_default():\n",
        "#     with summary.always_record_summaries():\n",
        "#       summary.scalar('loss', loss[0], step=gs)\n",
        "#       summary.scalar('learning_rate', lr[0], step=gs)\n",
        "#       summary.scalar('current_epoch', ce[0], step=gs)\n",
        "\n",
        "#       return summary.all_summary_ops()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sIa5GsA-_0rb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def test_model_fn(features, labels, mode, params):\n",
        "#     ###################################################################\n",
        "#     # The Keras LSTM layer that causes an error unless it is unrolled #\n",
        "#     ###################################################################\n",
        "#     predictions = tf.keras.layers.Conv2D(filters=1, kernel_size=1)(features)\n",
        "\n",
        "#     # Create ops for the TPUEstimatorSpec\n",
        "#     loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n",
        "#     optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
        "# #     if FLAGS.use_tpu:\n",
        "#     optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)\n",
        "  \n",
        "#     global_step = tf.train.get_global_step()\n",
        "#     train_op = optimizer.minimize(loss, global_step)\n",
        "\n",
        "#     # Return the TPUEstimatorSpec\n",
        "#     return tpu_estimator.TPUEstimatorSpec(\n",
        "#         mode=mode,\n",
        "#         loss=loss,\n",
        "#         train_op=train_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpUjJCzRBNUy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # https://github.com/tensorflow/tpu/blob/master/models/experimental/cifar_keras/cifar_keras.py\n",
        "\n",
        "# def model_fn(features, labels, mode, config, params):\n",
        "# #   print(features)\n",
        "#   v = tf.keras.layers.Input(tensor=features)\n",
        "#   pred = tf.keras.layers.Conv2D(\n",
        "#     filters=1,\n",
        "#     kernel_size=3,\n",
        "#     padding='same',\n",
        "#     activation='relu'\n",
        "#   )(v)\n",
        "  \n",
        "# #   print(pred)\n",
        "\n",
        "# #   pred=features\n",
        "  \n",
        "# #   if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "#     # Compute the current epoch and associated learning rate from global_step.\n",
        "#   global_step = tf.train.get_global_step()\n",
        "\n",
        "#   loss = tf.keras.losses.binary_crossentropy(labels, pred)  \n",
        "\n",
        "#   optimizer = tf.train.AdamOptimizer()\n",
        "#   optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "#   # Batch normalization requires UPDATE_OPS to be added as a dependency to\n",
        "#   # the train operation.\n",
        "#   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "#   with tf.control_dependencies(update_ops):\n",
        "#     train_op = optimizer.minimize(loss, global_step)\n",
        "      \n",
        "# #     model = tf.keras.Model(inputs=features, outputs=pred)\n",
        "    \n",
        "#   return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "#     mode=mode,\n",
        "#     loss=loss,\n",
        "#     train_op=train_op,\n",
        "#     host_call=None,\n",
        "#     eval_metrics=(metric_fn, [labels, pred]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oz4OuUomhxzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def main(unused_argv=None):\n",
        "#     tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver('kaggle-tpu')\n",
        "\n",
        "    \n",
        "#     config = tf.contrib.tpu.RunConfig(\n",
        "#       cluster=tpu_cluster_resolver,\n",
        "#       model_dir='gs://kaggle-195720-tgs-salt-identification-challenge/temp',\n",
        "# #       save_checkpoints_secs=3600,\n",
        "# #       session_config=tf.ConfigProto(\n",
        "# #           allow_soft_placement=True, log_device_placement=True),\n",
        "# #       tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "# #           iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "# #           num_shards=FLAGS.num_shards),\n",
        "#   )\n",
        "    \n",
        "    \n",
        "# #     config = tpu_config.RunConfig(\n",
        "# #         master=master_url,\n",
        "# #         model_dir='gs://kaggle-195720-tgs-salt-identification-challenge/temp')\n",
        "\n",
        "#     # Create the TPUEstimator\n",
        "#     estimator = tpu_estimator.TPUEstimator(\n",
        "#       use_tpu=True,\n",
        "#       model_fn=model_fn,\n",
        "#       config=config,\n",
        "#       train_batch_size=1024)\n",
        "\n",
        "#     # Train the estimator for 10 steps\n",
        "#     estimator.train(input_fn, max_steps=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXICpUNGitvo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def input_fn(params):\n",
        "#     dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "# #     dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(params['batch_size']))\n",
        "    \n",
        "#     print(dataset)\n",
        "\n",
        "#     # Make input_fn for the TPUEstimator train step\n",
        "# #     dataset_fn = dataset.make_one_shot_iterator().get_next()\n",
        "#     dataset = dataset.shuffle(1000).repeat().batch(params['batch_size'])\n",
        "    \n",
        "# #     dataset = dataset.prefetch(1)\n",
        "#     return dataset\n",
        "  \n",
        "  \n",
        "# # dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "# # dataset\n",
        "# # dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(128))\n",
        "\n",
        "# # print(dataset)\n",
        "\n",
        "# # # Make input_fn for the TPUEstimator train step\n",
        "# # dataset_fn = dataset.make_one_shot_iterator().get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFjqvXl4ijH9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myc4wE2NdCN-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.contrib import tpu\n",
        "# from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
        "\n",
        "# def axy_computation(a, x, y):\n",
        "#   return a * x + y\n",
        "\n",
        "# inputs = [\n",
        "#     3.0,\n",
        "#     tf.ones([3, 3], tf.float32),\n",
        "#     tf.ones([3, 3], tf.float32),\n",
        "# ]\n",
        "\n",
        "# tpu_computation = tpu.rewrite(axy_computation, inputs)\n",
        "\n",
        "# tpu_grpc_url = TPUClusterResolver(\n",
        "#     tpu=['kaggle-tpu']).get_master()\n",
        "\n",
        "# with tf.Session(tpu_grpc_url) as sess:\n",
        "#   sess.run(tpu.initialize_system())\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "#   output = sess.run(tpu_computation)\n",
        "#   print(output)\n",
        "#   sess.run(tpu.shutdown_system())\n",
        "\n",
        "# print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16XOGte5q_0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import keras\n",
        "# from keras import Model\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "# from keras.models import load_model\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, SpatialDropout2D, UpSampling2D\n",
        "\n",
        "# from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "# def dice_loss(y_true, y_pred):\n",
        "#     smooth = 1.\n",
        "#     y_true_f = K.flatten(y_true)\n",
        "#     y_pred_f = K.flatten(y_pred)\n",
        "#     intersection = y_true_f * y_pred_f\n",
        "#     score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "#     return 1. - score\n",
        "  \n",
        "  \n",
        "\n",
        "# # define iou or jaccard loss function\n",
        "# def jaccard_loss(y_true, y_pred):\n",
        "#     y_true = tf.reshape(y_true, [-1])\n",
        "#     y_pred = tf.reshape(y_pred, [-1])\n",
        "#     intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    \n",
        "#     epsilon = tf.keras.backend.epsilon()\n",
        "    \n",
        "#     score = (intersection + epsilon) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + epsilon)\n",
        "#     return 1 - score\n",
        "\n",
        "\n",
        "  \n",
        "# def iou_bce_loss(y_true, y_pred):\n",
        "#     return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.to_float(tf.greater(tf.round(y_true), y_pred)))\n",
        "    \n",
        "\n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     i = tf.round(y_pred) * tf.round(y_true)\n",
        "#     o = tf.round(y_pred) * tf.round(y_true)\n",
        "#     mean_iou = tf.metrics.mean_iou(y_true, tf.round(y_pred), 1)\n",
        "#     print(mean_iou)\n",
        "#     print(tf.tile(mean_iou, 10))\n",
        "#     print(mean_iou[0])\n",
        "#     return mean_iou[0]\n",
        "#     return tf.reduce_mean(tf.tile(mean_iou, 10))\n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     prec = []\n",
        "#     for t in np.arange(0.5, 1.0, 0.05):\n",
        "#         y_pred_ = tf.to_int32(y_pred > t)\n",
        "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         with tf.control_dependencies([up_opt]):\n",
        "#             score = tf.identity(score)\n",
        "#         prec.append(score)\n",
        "#     return K.mean(K.stack(prec), axis=0)\n",
        "\n",
        "\n",
        "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
        "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if activation == True:\n",
        "        x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "  \n",
        "def residual_block(blockInput, num_filters=16):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = convolution_block(x, num_filters, (3,3) )\n",
        "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
        "    x = Add()([x, blockInput])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "def conv_block(m, dim, acti, bn, res, do=0.5):\n",
        "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
        "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
        "    n = tf.keras.layers.SpatialDropout2D(do)(n) if do else n\n",
        "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
        "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
        "    n = tf.keras.layers.Concatenate()([m, n]) if res else n\n",
        "    return n\n",
        "\n",
        "  \n",
        "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "    if depth > 0:\n",
        "        n = conv_block(m, dim, acti, bn, res)\n",
        "        m = tf.keras.layers.MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "        m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
        "        if up:\n",
        "            m = tf.keras.layers.UpSampling2D()(m)\n",
        "            m = tf.keras.layers.Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "        else:\n",
        "#             padding = ['valid', 'same','same','valid','same','valid']\n",
        "            padding = ['same','same','valid','same','valid']\n",
        "            m = tf.keras.layers.Conv2DTranspose(dim, 3, strides=2, activation=acti, padding=padding[depth-1])(m)\n",
        "        n = tf.keras.layers.Concatenate()([n, m])\n",
        "        m = conv_block(n, dim, acti, bn, res)\n",
        "    else:\n",
        "        m = conv_block(m, dim, acti, bn, res, do=do)\n",
        "    return m\n",
        "\n",
        "  \n",
        "# def UNet(img_shape, out_ch=1, start_ch=32, depth=5, inc_rate=2., activation='relu', \n",
        "#          dropout=False, batchnorm=True, maxpool=True, upconv=False, residual=True):\n",
        "#     i = tf.keras.layers.Input(shape=img_shape)\n",
        "#     o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
        "#     o = tf.keras.layers.Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
        "#     return tf.keras.Model(inputs=i, outputs=o)\n",
        "\n",
        "def UNet(img_shape, out_ch=1, start_ch=32, depth=5, inc_rate=2., activation='relu', \n",
        "         dropout=False, batchnorm=True, maxpool=True, upconv=False, residual=True):\n",
        "    i = tf.keras.layers.Input(shape=img_shape)\n",
        "    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
        "    o = tf.keras.layers.Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
        "    return tf.keras.Model(inputs=i, outputs=o)\n",
        "  \n",
        "  \n",
        "\n",
        "# def ResNet():\n",
        "#   i = Input(shape=(img_size_target, img_size_target))\n",
        "  \n",
        "#   o = Conv2D(filters=1, kernel_size=1, activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NDV_LZ8Ernrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2206
        },
        "outputId": "0a09936c-c198-4b0a-e043-a3a0c3a6b204"
      },
      "cell_type": "code",
      "source": [
        "def build_train_generator(X_train, y_train, batch_size=32):\n",
        "    # Ref: https://keras.io/preprocessing/image/\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(\n",
        "      rotation_range=360.,\n",
        "#       width_shift_range=0.1,\n",
        "#       height_shift_range=0.1,\n",
        "      fill_mode='reflect',\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "    )\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
        "    seed = 1\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "    \n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "\n",
        "    # combine generators into one which yields image and masks\n",
        "    return zip(image_generator, mask_generator)\n",
        "\n",
        "\n",
        "def cv(X_train, y_train, X_test, fold, fast=False):\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    \n",
        "    loss = []\n",
        "    metric = []\n",
        "    pred_test = np.zeros(X_test.shape)\n",
        "    fold_index = 0\n",
        "    \n",
        "    start_ch = 32\n",
        "    depth = 5\n",
        "         \n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train):      \n",
        "#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\n",
        "      model = test_model()\n",
        "      resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "      strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "      model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "      session_master = resolver.master()\n",
        "      \n",
        "      model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"binary_crossentropy\"])\n",
        "\n",
        "#       early_stopping = EarlyStopping(patience=20, verbose=1)\n",
        "#       model_checkpoint = ModelCheckpoint(\"./model/keras_%d.model\"%fold_index, save_best_only=True, verbose=1)\n",
        "#       reduce_lr = ReduceLROnPlateau(factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
        "\n",
        "      epochs = 20000\n",
        "      batch_size = 32\n",
        "\n",
        "      history = model.fit_generator(\n",
        "          build_train_generator(X_train[train_index], y_train[train_index], batch_size),\n",
        "          validation_data=(X_train[test_index], y_train[test_index]),\n",
        "          steps_per_epoch=np.ceil(len(X_train[train_index]) / batch_size), \n",
        "          epochs=epochs,\n",
        "#           callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "      )\n",
        "      \n",
        "      best_idx = np.argmin(history.history['val_loss'])\n",
        "      val_loss = history.history['val_loss'][best_idx]\n",
        "      val_mean_iou = history.history['val_mean_iou'][best_idx]\n",
        "      \n",
        "      print(\"Best val_loss: %.4f, val_mean_iou: %.4f\"%(val_loss, val_mean_iou))\n",
        "      loss.append(val_loss)\n",
        "      metric.append(val_mean_iou)\n",
        "      \n",
        "      model = keras.models.load_model('model/keras_%d.model'%fold_index, custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss, 'dice_loss': dice_loss})\n",
        "      pred_test += model.predict(X_test) / fold\n",
        "      fold_index = fold_index + 1\n",
        "      \n",
        "      if fast:\n",
        "        break\n",
        "      \n",
        "      \n",
        "    print('%d fold val_loss: %.4f, val_mean_iou: %.4f'%(fold, np.mean(loss), np.mean(metric)))\n",
        "    return pred_test, np.mean(loss), np.mean(metric)\n",
        "\n",
        "  \n",
        "pred_test, val_loss, val_mean_iou = cv(X_train, y_train, X_test, 5, fast=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 13486222805198966859)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9984220072123666814)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 18162437541401842557)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9211313139234737039)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4136755912537517626)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6532054137011805886)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3825718477947054890)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5276807511303573055)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5719668420551969614)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10390436815882700867)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15818039368216332404)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6864323238587904500)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: metrics/mean_iou/ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_BOOL, Truncate=false, _tpu_replicate=\"cluster\"](metrics/mean_iou/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0dea82b24951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-0dea82b24951>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(X_train, y_train, X_test, fold, fast)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kaggle-tpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUDistributionStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_to_tpu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0msession_master\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/framework/python/framework/experimental.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m'any time, and without warning.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         decorator_utils.get_qualified_name(func), func.__module__)\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m   new_func.__doc__ = _add_experimental_function_notice_to_docstring(\n\u001b[1;32m     66\u001b[0m       func.__doc__)\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mtpu_model\u001b[0;34m(model, strategy)\u001b[0m\n\u001b[1;32m   1888\u001b[0m   \"\"\"\n\u001b[1;32m   1889\u001b[0m   \u001b[0;31m# Force initialization of the CPU model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m       \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   2706\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 710\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: metrics/mean_iou/ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_BOOL, Truncate=false, _tpu_replicate=\"cluster\"](metrics/mean_iou/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HRemgbrVOpjK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # summarize history for loss\n",
        "# plt.plot(history.history['loss'][1:])\n",
        "# plt.plot(history.history['val_loss'][1:])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train','Validation'], loc='upper left')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNqZwFOebfzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# best_idx = np.argmin(history.history['val_loss'])\n",
        "# val_loss = history.history['val_loss'][best_idx]\n",
        "# val_mean_iou = history.history['val_mean_iou'][best_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgBEjwKHCn1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('model/keras.model', custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EqepTCQa1djX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RLenc(img, order='F', format=True):\n",
        "    \"\"\"\n",
        "    img is binary mask image, shape (r,c)\n",
        "    order is down-then-right, i.e. Fortran\n",
        "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
        "\n",
        "    returns run length as an array or string (if format is True)\n",
        "    \"\"\"\n",
        "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
        "    runs = []  ## list of run lengths\n",
        "    r = 0  ## the current run length\n",
        "    pos = 1  ## count starts from 1 per WK\n",
        "    for c in bytes:\n",
        "        if (c == 0):\n",
        "            if r != 0:\n",
        "                runs.append((pos, r))\n",
        "                pos += r\n",
        "                r = 0\n",
        "            pos += 1\n",
        "        else:\n",
        "            r += 1\n",
        "\n",
        "    # if last run is unsaved (i.e. data ends with 1)\n",
        "    if r != 0:\n",
        "        runs.append((pos, r))\n",
        "        pos += r\n",
        "        r = 0\n",
        "\n",
        "    if format:\n",
        "        z = ''\n",
        "\n",
        "        for rr in runs:\n",
        "            z += '{} {} '.format(rr[0], rr[1])    \n",
        "        return z[:-1]\n",
        "    else:\n",
        "        return runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5wnyMSkd50M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class FasterRle(object):\n",
        "#     \"\"\"Perform RLE in paralell.\"\"\"\n",
        "\n",
        "#     def __init__(self, num_consumers=2):\n",
        "#         \"\"\"Initialize class.\"\"\"\n",
        "#         self._tasks = multiprocessing.JoinableQueue()\n",
        "#         self._results = multiprocessing.Queue()\n",
        "#         self._n_consumers = num_consumers\n",
        "\n",
        "#         # Initialize consumers\n",
        "#         self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n",
        "#         for w in self._consumers:\n",
        "#             w.start()\n",
        "\n",
        "#     def add(self, array, startIndex):\n",
        "#         \"\"\"Add a task to perform.\"\"\"\n",
        "#         self._tasks.put(FasterTask(array, startIndex))\n",
        "\n",
        "#     def get_results(self):\n",
        "#         \"\"\"Close all tasks.\"\"\"\n",
        "#         # Provide poison pill\n",
        "#         [self._tasks.put(None) for _ in range(self._n_consumers)]\n",
        "#         # Wait for finish\n",
        "#         self._tasks.join()\n",
        "#         # Return results\n",
        "#         singles = []\n",
        "#         while not self._results.empty():\n",
        "#             singles.append(self._results.get())\n",
        "            \n",
        "#         resultDic = dict()\n",
        "#         for rles, start in singles:\n",
        "#             #print('start:', start)\n",
        "#             for i,rle in enumerate(rles):\n",
        "#                 #print('i:', i)\n",
        "#                 resultDic[str(start+i)] = rle\n",
        "#         return resultDic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7wRK-ZUEmzA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch(func):\n",
        "  def batch_func(b):\n",
        "    return np.array([func(e) for e in tqdm(b)])\n",
        "  return batch_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sy0eHi4-P5PP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preds_test = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hIcfQ4_pPlHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e52d1ce9-b9b2-4fdf-f039-e606626a790a"
      },
      "cell_type": "code",
      "source": [
        "test['rle_mask'] = batch(RLenc)(np.round(pred_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18000/18000 [00:58<00:00, 309.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ST2JPIMLP666",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7aaf49b7-1c59-4efc-cfb8-2bdd7e3a885f"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_FILE = 'keras_unet_sc32de5wbecjaccloss_%.4f_%.4f.csv'%(val_loss, val_mean_iou)\n",
        "test.to_csv(SUBMISSION_FILE)\n",
        "SUBMISSION_FILE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'keras_unet_sc32de5wbecjaccloss_0.1320_0.7477.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "7iJCa8ZqRppy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cc737439-5883-4295-c670-658d69aaff48"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_MESSAGE='\"Keras U-net iou_bce_loss: val_loss: %.4f, val_mean_iou: %.4f\"'%(val_loss, val_mean_iou)\n",
        "!kaggle competitions submit -f '{SUBMISSION_FILE}' -m '{SUBMISSION_MESSAGE}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using competition: tgs-salt-identification-challenge\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.4.6)\n",
            "Successfully submitted to TGS Salt Identification Challenge"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ql1uneNwTuEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "c9be94ee-85ec-4429-d351-98436cfaf45f"
      },
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv('.env')\n",
        "!gsutil rsync model gs://{os.environ['GCP_BUCKET']}/model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "Copying file://model/keras_0.model [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "Copying file://model/keras_1.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_2.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_3.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m -o ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying file://model/keras_4.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "Operation completed over 5 objects/751.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6UwBwalwuZpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "bad2519d-f469-4e15-ce5c-1a921ed92c84"
      },
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# def batch_iou(pred, truth):\n",
        "#     pred_ = tf.round(pred)\n",
        "#     truth_ = truth\n",
        "    \n",
        "#     i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "#     u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "#     epsilon = tf.keras.backend.epsilon()\n",
        "#     return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "  \n",
        "# def mean_iou(pred, truth):\n",
        "#     ious = batch_iou(pred, truth)\n",
        "#     ious = tf.reshape(ious, (-1,1))\n",
        "#     ious = tf.tile(ious, (1,10))\n",
        "#     thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "#     return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "# BATCH_SIZE = 100\n",
        "# ones = tf.constant(np.ones((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# zeros = tf.constant(np.zeros((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# half = tf.coonstant(np.zeros(BATCH_SIZE, ))\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "  \n",
        "#   print(mean_iou(ones, zeros).eval())\n",
        "#   print(mean_iou(ones, ones).eval())\n",
        "#   print(mean_iou(zeros, ones).eval())\n",
        "#   print(mean_iou(zeros, zeros).eval())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1AayNFWou9ms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2345
        },
        "outputId": "b0ad8070-0ad3-4ddc-d926-aae031e16852"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Standard Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 12\n",
        "\n",
        "# input image dimensions\n",
        "IMG_ROWS, IMG_COLS = 28, 28\n",
        "\n",
        "use_tpu=True\n",
        "\n",
        "\n",
        "def mnist_model(input_shape):\n",
        "  \"\"\"Creates a MNIST model.\"\"\"\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(\n",
        "      tf.keras.layers.Conv2D(\n",
        "          32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(tf.keras.layers.Dropout(0.25))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "\n",
        "def main(unused_dev):\n",
        "  use_tpu = True\n",
        "\n",
        "  print('Mode:', 'TPU' if use_tpu else 'CPU')\n",
        "\n",
        "  if True:\n",
        "    print('Using fake data')\n",
        "    x_train = np.random.random((128, IMG_ROWS, IMG_COLS))\n",
        "    y_train = np.zeros([128, 1], dtype=np.int32)\n",
        "    x_test, y_test = x_train, y_train\n",
        "  else:\n",
        "    # the data, split between train and test sets\n",
        "    print('Using real data')\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "  x_train = x_train.reshape(x_train.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
        "  input_shape = (IMG_ROWS, IMG_COLS, 1)\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  print('x_train shape:', x_train.shape)\n",
        "  print(x_train.shape[0], 'train samples')\n",
        "  print(x_test.shape[0], 'test samples')\n",
        "\n",
        "  # convert class vectors to binary class matrices\n",
        "  y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "  y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "  model = mnist_model(input_shape)\n",
        "\n",
        "  if use_tpu:\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "    )\n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05),\n",
        "      metrics=['accuracy'])\n",
        "\n",
        "  model.fit(\n",
        "      x_train,\n",
        "      y_train,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      epochs=EPOCHS,\n",
        "      verbose=1,\n",
        "      validation_data=(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Loss for final step:', score[0])\n",
        "  print('Accuracy ', score[1])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mode: TPU\n",
            "Using fake data\n",
            "x_train shape: (128, 28, 28, 1)\n",
            "128 train samples\n",
            "128 test samples\n",
            "INFO:tensorflow:Detected old TPUStrategy API.\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 128 samples, validate on 128 samples\n",
            "Epoch 1/12\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(16, 28, 28, 1), dtype=tf.float32, name='conv2d_4_input0'), TensorSpec(shape=(16, 10), dtype=tf.float32, name='dense_5_target0')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_4_input\n",
            "INFO:tensorflow:Started compiling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1260\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'group_assignment' not in Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\", group_assignment=[]](training/TFOptimizer/gradients/tpu_model_139793487692408/conv2d_4/Conv2D_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e8ddd031b353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-e8ddd031b353>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_dev)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1041\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    852\u001b[0m                                                    infeed_manager)\n\u001b[1;32m    853\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_model_compiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;31m# Initialize our TPU weights on the first compile.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_test_model_compiles\u001b[0;34m(self, tpu_model_ops)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m     \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpu_compilation_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompilationResultProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 668\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'group_assignment' not in Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\", group_assignment=[]](training/TFOptimizer/gradients/tpu_model_139793487692408/conv2d_4/Conv2D_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "R3b3873xfUM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}