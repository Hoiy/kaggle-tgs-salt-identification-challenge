{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_tpu_unet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RgW68azy-I2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2973e9b7-259f-4258-a2b7-8f0d7faf5aba"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import dotenv\n",
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "\n",
        "dotenv.load_dotenv('.env')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "tQMYWUFxAqcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2189a9e-5715-4c3e-d1c9-349cd4724d62"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('raw/train.csv').set_index('id')\n",
        "test = pd.read_csv('raw/sample_submission.csv').set_index('id')\n",
        "depths = pd.read_csv('raw/depths.csv').set_index('id')\n",
        "\n",
        "train = pd.merge(train, depths, left_index=True, right_index=True, how='inner')\n",
        "train.shape, test.shape, depths.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 2), (18000, 1), (22000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "RWGJQRBXAqxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Whe2VDUGAquf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "68de5d25-a8d3-480a-a6f4-e2b1577810d0"
      },
      "cell_type": "code",
      "source": [
        "X_train = [cv2.resize(cv2.imread(\"./raw/images/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(train.index)]\n",
        "X_train = np.expand_dims(np.array(X_train), -1)\n",
        "X_train = X_train / 255\n",
        "X_train = X_train.astype(np.float32)\n",
        "\n",
        "y_train = [cv2.resize(cv2.imread(\"./raw/masks/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(train.index)]\n",
        "y_train = np.expand_dims(np.array(y_train), -1)\n",
        "y_train = y_train / 255\n",
        "y_train = y_train.astype(np.float32)\n",
        "\n",
        "\n",
        "X_test = [cv2.resize(cv2.imread(\"./raw/images/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(test.index)]\n",
        "X_test = np.expand_dims(np.array(X_test), -1)\n",
        "X_test = X_test / 255\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4000/4000 [00:01<00:00, 2673.70it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 9311.34it/s]\n",
            "100%|██████████| 18000/18000 [00:06<00:00, 2727.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 128, 128, 1), (4000, 128, 128, 1), (18000, 128, 128, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "9D2pdhBiAqrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3731
        },
        "outputId": "952b4680-8115-4150-ccea-6e09cb470de8"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "# from keras import Model\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "# from keras.models import load_model\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, SpatialDropout2D, UpSampling2D, Activation, Add\n",
        "\n",
        "# from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return 1. - score\n",
        "  \n",
        "  \n",
        "\n",
        "# define iou or jaccard loss function\n",
        "def jaccard_loss(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    \n",
        "    score = (intersection + epsilon) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + epsilon)\n",
        "    return 1 - score\n",
        "\n",
        "\n",
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    gts = tf.reduce_sum(gt_sorted)\n",
        "    intersection = gts - tf.cumsum(gt_sorted)\n",
        "    union = gts + tf.cumsum(1. - gt_sorted)\n",
        "    jaccard = 1. - intersection / union\n",
        "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
        "    return jaccard\n",
        "  \n",
        "  \n",
        "  \n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        def treat_image(log_lab):\n",
        "            log, lab = log_lab\n",
        "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
        "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
        "            return lovasz_hinge_flat(log, lab)\n",
        "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
        "        loss = tf.reduce_mean(losses)\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_loss():\n",
        "        labelsf = tf.cast(labels, logits.dtype)\n",
        "        signs = 2. * labelsf - 1.\n",
        "        errors = 1. - logits * tf.stop_gradient(signs)\n",
        "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
        "        gt_sorted = tf.gather(labelsf, perm)\n",
        "        grad = lovasz_grad(gt_sorted)\n",
        "        #loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        loss = tf.tensordot(tf.nn.elu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        return loss\n",
        "\n",
        "    # deal with the void prediction case (only void pixels)\n",
        "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
        "                   lambda: tf.reduce_sum(logits) * 0.,\n",
        "                   compute_loss,\n",
        "                   strict=True,\n",
        "                   name=\"loss\"\n",
        "                   )\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = tf.reshape(scores, (-1,))\n",
        "    labels = tf.reshape(labels, (-1,))\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = tf.not_equal(labels, ignore)\n",
        "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
        "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
        "    return vscores, vlabels\n",
        "\n",
        "def lovasz_loss(y_true, y_pred):\n",
        "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
        "    #logits = K.log(y_pred / (1. - y_pred))\n",
        "    logits = y_pred #Jiaxin\n",
        "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
        "    return loss\n",
        "  \n",
        "\n",
        "# def iou_bce_loss(y_true, y_pred):\n",
        "#     return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.to_float(tf.greater(tf.round(y_true), y_pred)))\n",
        "    \n",
        "\n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     i = tf.round(y_pred) * tf.round(y_true)\n",
        "#     o = tf.round(y_pred) * tf.round(y_true)\n",
        "#     mean_iou = tf.metrics.mean_iou(y_true, tf.round(y_pred), 1)\n",
        "#     print(mean_iou)\n",
        "#     print(tf.tile(mean_iou, 10))\n",
        "#     print(mean_iou[0])\n",
        "#     return mean_iou[0]\n",
        "#     return tf.reduce_mean(tf.tile(mean_iou, 10))\n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     prec = []\n",
        "#     for t in np.arange(0.5, 1.0, 0.05):\n",
        "#         y_pred_ = tf.to_int32(y_pred > t)\n",
        "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         with tf.control_dependencies([up_opt]):\n",
        "#             score = tf.identity(score)\n",
        "#         prec.append(score)\n",
        "#     return K.mean(K.stack(prec), axis=0)\n",
        "\n",
        "\n",
        "# def convolution_block(x, filters, size):\n",
        "#     return x\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# def residual_block(blockInput, filters):\n",
        "#     x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(blockInput)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(x)\n",
        "#     x = Add()([x, blockInput])\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     return x\n",
        "  \n",
        "  \n",
        "# def res_block(i, filters=16):\n",
        "#     n1 = Conv2D(filters, 1, activation='relu', padding='same')(i)\n",
        "#     n3 = Conv2D(filters, 3, activation='relu', padding='same')(i)\n",
        "#     n5 = Conv2D(filters, 5, activation='relu', padding='same')(i)\n",
        "#     n = Concatenate()([n1, n3, n5])\n",
        "#     n = BatchNormalization()(n)\n",
        "    \n",
        "#     n1 = Conv2D(filters, 1, activation='relu', padding='same')(n)\n",
        "#     n3 = Conv2D(filters, 3, activation='relu', padding='same')(n)\n",
        "#     n5 = Conv2D(filters, 5, activation='relu', padding='same')(n)\n",
        "#     n = Concatenate()([n1, n3, n5])\n",
        "#     n = BatchNormalization()(n)\n",
        "    \n",
        "# #     n = Conv2D(1, 1, activation='relu', padding='same')(n)\n",
        "# #     n = Concatenate()([i, n])\n",
        "# #     n = BatchNormalization()(n)\n",
        "#     return n\n",
        "\n",
        "  \n",
        "# def level_block(inp, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "#     if depth > 0:\n",
        "#         n = conv_block(m, dim, acti, bn, res)\n",
        "#         m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "#         m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
        "#         if up:\n",
        "#             m = UpSampling2D()(m)\n",
        "#             m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "#         else:\n",
        "# #             padding = ['valid', 'same','same','valid','same','valid']\n",
        "#             padding = ['same','same','valid','same','valid']\n",
        "#             m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding=padding[depth-1])(m)\n",
        "#         n = Concatenate()([n, m])\n",
        "#         m = conv_block(n, dim, acti, bn, res)\n",
        "#     else:\n",
        "#         m = conv_block(m, dim, acti, bn, res, do=do)\n",
        "#     return m\n",
        "\n",
        "  \n",
        "# def ResNet():\n",
        "#     filter_size = 8\n",
        "#     i = Input(shape=(img_size_target,img_size_target,1))\n",
        "#     o = Conv2D(filter_size, 1, strides=(1,1), padding='same', activation='relu')(i)\n",
        "#     for depth in tqdm(range(128)):\n",
        "#       o = residual_block(o, filter_size)\n",
        "#     o = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(o)\n",
        "#     return Model(inputs=i, outputs=o)\n",
        "  \n",
        "  \n",
        "# def ResNet_2():\n",
        "#     resnet = keras.applications.resnet50.ResNet50(include_top=False, pooling=None, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "#     o = Conv2D(1, 1, strides=(1,1), activation='sigmoid')(resnet.output)\n",
        "#     return Model(inputs=resnet.input, outputs=o)\n",
        "  \n",
        "def res_block(inputs, filters):\n",
        "    inputs = BatchNormalization()(inputs)\n",
        "    conv = Conv2D(filters, 3, strides=1, padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = Conv2D(filters, 3, strides=1, padding='same', activation='relu')(conv)\n",
        "    \n",
        "    conv = Add()([conv, inputs])\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    return Conv2D(filters, 3, strides=2, padding='same', activation='relu')(conv)\n",
        "  \n",
        "\n",
        "def res_block_r(inputs, filters, res):\n",
        "    inputs = Add()([inputs, res])\n",
        "    inputs = BatchNormalization()(inputs)\n",
        "    conv = Conv2DTranspose(filters, 3, strides=1, padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = Conv2DTranspose(filters, 3, strides=1, padding='same', activation='relu')(conv)\n",
        "    \n",
        "    conv = Add()([conv, inputs])\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    return Conv2DTranspose(filters, 3, strides=2, padding='same', activation='relu')(conv)\n",
        "  \n",
        "  \n",
        "# def build_model(start_channels=8, depth=6):\n",
        "#     inp = Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "#     l = BatchNormalization()(inp)\n",
        "#     l = Conv2D(start_channels, 3, strides=1, padding='same', activation='relu')(l)\n",
        "    \n",
        "#     layers = [l]\n",
        "    \n",
        "#     for i in range(depth):\n",
        "#       layers.append(res_block(layers[-1], start_channels * (2 ** i)))\n",
        "      \n",
        "      \n",
        "#     for i in range(depth):\n",
        "#       layers.append(res_block_r(layers[-1], start_channels * (2 ** (depth - 1 - i)), layers[-2*i-1]))\n",
        "      \n",
        "#     l = BatchNormalization()(layers[-1])\n",
        "#     out = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(l)\n",
        "    \n",
        "#     return Model(inputs=inp, outputs=out)\n",
        "\n",
        "def UNet(inputs, channels, depth):\n",
        "    layers = [inputs]\n",
        "    \n",
        "    for i in range(depth):\n",
        "      layers.append(res_block(layers[-1], channels))\n",
        "      \n",
        "      \n",
        "    for i in range(depth):\n",
        "      layers.append(res_block_r(layers[-1], channels, layers[-2*i-1]))\n",
        "    \n",
        "    return layers[-1]\n",
        "    \n",
        "    \n",
        "\n",
        "def build_model(channels=64, depth=6):\n",
        "    inp = keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "    l = BatchNormalization()(inp)\n",
        "    l = Conv2D(channels, 3, strides=1, padding='same', activation='relu')(l)\n",
        "    \n",
        "    l = UNet(l, channels, depth)\n",
        "#     l = UNet(l, channels, depth)\n",
        "    \n",
        "    l = BatchNormalization()(l)\n",
        "    out = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(l)\n",
        "    \n",
        "    return keras.Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "  \n",
        "build_model().summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 128, 128, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 128, 128, 1)  4           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 128, 128, 64) 640         batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 128, 128, 64) 0           conv2d_2[0][0]                   \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 64, 64, 64)   0           conv2d_5[0][0]                   \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           conv2d_8[0][0]                   \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36928       batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 64)   36928       batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           conv2d_11[0][0]                  \n",
            "                                                                 batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 64)     36928       batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 64)     0           conv2d_14[0][0]                  \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 64)     256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 4, 4, 64)     256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 4, 4, 64)     0           conv2d_17[0][0]                  \n",
            "                                                                 batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 4, 4, 64)     256         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 2, 2, 64)     36928       batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 2, 2, 64)     0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 2, 2, 64)     256         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 2, 2, 64)     36928       batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 2, 2, 64)     256         conv2d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 2, 2, 64)     36928       batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 2, 2, 64)     0           conv2d_transpose_1[0][0]         \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 2, 2, 64)     256         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 4, 4, 64)     0           conv2d_transpose_2[0][0]         \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4, 4, 64)     256         add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 64)     256         conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 4, 4, 64)     36928       batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 4, 4, 64)     0           conv2d_transpose_4[0][0]         \n",
            "                                                                 batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 4, 4, 64)     256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 8, 8, 64)     0           conv2d_transpose_5[0][0]         \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 64)     256         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 64)     256         conv2d_transpose_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 8, 8, 64)     36928       batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 8, 8, 64)     0           conv2d_transpose_7[0][0]         \n",
            "                                                                 batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 64)     256         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 16, 16, 64)   36928       batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_8[0][0]         \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 64)   256         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTrans (None, 16, 16, 64)   36928       batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 64)   256         conv2d_transpose_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DTran (None, 16, 16, 64)   36928       batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_10[0][0]        \n",
            "                                                                 batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 64)   256         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_11 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 32, 32, 64)   0           conv2d_transpose_11[0][0]        \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 32, 64)   256         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 64)   256         conv2d_transpose_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DTran (None, 32, 32, 64)   36928       batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 32, 32, 64)   0           conv2d_transpose_13[0][0]        \n",
            "                                                                 batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 64)   256         add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_14 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 64, 64, 64)   0           conv2d_transpose_14[0][0]        \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 64, 64, 64)   256         add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_15 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 64, 64, 64)   256         conv2d_transpose_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_16 (Conv2DTran (None, 64, 64, 64)   36928       batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 64, 64, 64)   0           conv2d_transpose_16[0][0]        \n",
            "                                                                 batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 64, 64, 64)   256         add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_17 (Conv2DTran (None, 128, 128, 64) 36928       batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 128, 128, 64) 256         conv2d_transpose_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 128, 128, 1)  65          batch_normalization_37[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 1,339,589\n",
            "Trainable params: 1,334,851\n",
            "Non-trainable params: 4,738\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q2kwPCzH776M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from albumentations import (\n",
        "    PadIfNeeded,\n",
        "    HorizontalFlip,\n",
        "    VerticalFlip,    \n",
        "    CenterCrop,    \n",
        "    Crop,\n",
        "    Compose,\n",
        "    Transpose,\n",
        "    RandomRotate90,\n",
        "    ElasticTransform,\n",
        "    GridDistortion, \n",
        "    OpticalDistortion,\n",
        "    RandomSizedCrop,\n",
        "    OneOf,\n",
        "    CLAHE,\n",
        "    RandomContrast,\n",
        "    RandomGamma,\n",
        "    RandomBrightness\n",
        ")\n",
        "\n",
        "def random_h_flip(image, mask):\n",
        "  aug = HorizontalFlip(p=0.5)\n",
        "  augmented = aug(image=image, mask=mask)\n",
        "  return augmented['image'], augmented['mask']\n",
        "\n",
        "\n",
        "def random_v_flip(image, mask):\n",
        "  aug = VerticalFlip(p=0.5)\n",
        "  augmented = aug(image=image, mask=mask)\n",
        "  return augmented['image'], augmented['mask']\n",
        "\n",
        "\n",
        "def random_rotate(image, mask):\n",
        "  aug = RandomRotate90(p=1)\n",
        "  augmented = aug(image=image, mask=mask)\n",
        "  return augmented['image'], augmented['mask']\n",
        "\n",
        "\n",
        "def random_aug(image, mask):\n",
        "  image, mask = random_h_flip(image, mask)\n",
        "  image, mask = random_v_flip(image, mask)\n",
        "  image, mask = random_rotate(image, mask)\n",
        "  return image, mask\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def _random_flip_left_right(image, mask):\n",
        "  image = tf.image.random_flip_left_right(image, seed=SEED)\n",
        "  mask = tf.image.random_flip_left_right(mask, seed=SEED)\n",
        "  return image, mask\n",
        "\n",
        "def _random_flip_up_down(image, mask):\n",
        "  image = tf.image.random_flip_up_down(image, seed=SEED)\n",
        "  mask = tf.image.random_flip_up_down(mask, seed=SEED)\n",
        "  return image, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PjE-J85UBkVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9115
        },
        "outputId": "904fc2ca-b22b-4441-d6a7-9df72dd74ea3"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def build_train_generator(X_train, y_train, batch_size=32):\n",
        "    # Ref: https://keras.io/preprocessing/image/\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(\n",
        "#       rotation_range=360.,\n",
        "#       width_shift_range=0.1,\n",
        "#       height_shift_range=0.1,\n",
        "      fill_mode='reflect',\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "    )\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
        "    seed = 1\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "    \n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "\n",
        "    # combine generators into one which yields image and masks\n",
        "    return zip(image_generator, mask_generator)\n",
        "\n",
        "  \n",
        "\n",
        "def cv(X_train, y_train, X_test, fold, fast=False):\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    \n",
        "    loss = []\n",
        "    metric = []\n",
        "    pred_test = np.zeros(X_test.shape)\n",
        "    fold_index = 0\n",
        "    \n",
        "    start_ch = 32\n",
        "    depth = 5\n",
        "         \n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train):      \n",
        "#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\n",
        "      model = build_model(channels=8)\n",
        "  \n",
        "      resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "      strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "      model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "      session_master = resolver.master()\n",
        "  \n",
        "      model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[mean_iou, \"binary_crossentropy\"])\n",
        "    \n",
        "      EPOCHS = 20000\n",
        "      BATCH_SIZE = 64\n",
        "\n",
        "      def create_data_fn(index, training=False):\n",
        "        def data_fn():\n",
        "          dataset = tf.data.Dataset.from_tensor_slices((X_train[index], y_train[index]))\n",
        "          if training:\n",
        "            dataset = dataset.map(_random_flip_left_right).map(_random_flip_up_down)\n",
        "          dataset = dataset.shuffle(len(X_train[index])).repeat(EPOCHS).batch(BATCH_SIZE, drop_remainder=True)\n",
        "          return dataset\n",
        "        return data_fn\n",
        "          \n",
        "\n",
        "      early_stopping = EarlyStopping(patience=10, verbose=1)\n",
        "      model_checkpoint = ModelCheckpoint(\"./model/keras_%d.model\"%fold_index, save_best_only=True, verbose=1)\n",
        "      reduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "      history = model.fit(\n",
        "          create_data_fn(train_index, training=True),\n",
        "          steps_per_epoch=int(np.ceil(len(X_train[train_index]) / BATCH_SIZE)),\n",
        "          validation_data=create_data_fn(test_index, training=False),\n",
        "          validation_steps=int(np.ceil(len(X_train[test_index]) / BATCH_SIZE)),\n",
        "          epochs=EPOCHS\n",
        "#           callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
        "#           callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "      \n",
        "      best_idx = np.argmin(history.history['val_loss'])\n",
        "      val_loss = history.history['val_loss'][best_idx]\n",
        "      val_mean_iou = history.history['val_mean_iou'][best_idx]\n",
        "      \n",
        "      print(\"Best val_loss: %.4f, val_mean_iou: %.4f\"%(val_loss, val_mean_iou))\n",
        "      loss.append(val_loss)\n",
        "      metric.append(val_mean_iou)\n",
        "      \n",
        "      model = keras.models.load_model('model/keras_%d.model'%fold_index, custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss, 'dice_loss': dice_loss})\n",
        "      pred_test += model.predict(X_test) / fold\n",
        "      fold_index = fold_index + 1\n",
        "      \n",
        "      del model\n",
        "      gc.collect()\n",
        "      \n",
        "      if fast:\n",
        "        break\n",
        "      \n",
        "      \n",
        "    print('%d fold val_loss: %.4f, val_mean_iou: %.4f'%(fold, np.mean(loss), np.mean(metric)))\n",
        "    return pred_test, np.mean(loss), np.mean(metric)\n",
        "\n",
        "  \n",
        "pred_test, val_loss, val_mean_iou = cv(X_train, y_train, X_test, 5, fast=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8986511690744612254)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17578464049263961541)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 1822323957473087955)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11699108356318831405)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15578942636750116165)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 4723819265577645578)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15658835048562211150)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14084991468488383314)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 13322110499786951539)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6306473378593074235)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5054784506900200386)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 18203919771645832346)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: grpc://10.240.1.2:8470\n",
            "Epoch 1/20000\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(128, 128, 128, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Cloning Adam {'decay': 0.0, 'amsgrad': False, 'lr': 0.0010000000474974513, 'epsilon': 1e-07, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 23.344770669937134 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "24/25 [===========================>..] - ETA: 2s - loss: 0.7338 - mean_iou: 0.0399 - binary_crossentropy: 0.7338INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(128, 128, 128, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Cloning Adam {'decay': 0.0, 'amsgrad': False, 'lr': 0.0010000000474974513, 'epsilon': 1e-07, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.238258600234985 secs\n",
            "25/25 [==============================] - 77s 3s/step - loss: 0.7303 - mean_iou: 0.0410 - binary_crossentropy: 0.7303 - val_loss: 0.6743 - val_mean_iou: 0.0714 - val_binary_crossentropy: 0.6743\n",
            "Epoch 2/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.5832 - mean_iou: 0.1038 - binary_crossentropy: 0.5832 - val_loss: 0.7024 - val_mean_iou: 0.0212 - val_binary_crossentropy: 0.7024\n",
            "Epoch 3/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.4581 - mean_iou: 0.1696 - binary_crossentropy: 0.4581 - val_loss: 0.9648 - val_mean_iou: 0.1151 - val_binary_crossentropy: 0.9648\n",
            "Epoch 4/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.3934 - mean_iou: 0.1982 - binary_crossentropy: 0.3934 - val_loss: 1.7010 - val_mean_iou: 0.1299 - val_binary_crossentropy: 1.7010\n",
            "Epoch 5/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.3390 - mean_iou: 0.2233 - binary_crossentropy: 0.3390 - val_loss: 2.0636 - val_mean_iou: 0.1330 - val_binary_crossentropy: 2.0636\n",
            "Epoch 6/20000\n",
            "25/25 [==============================] - 12s 489ms/step - loss: 0.3276 - mean_iou: 0.2244 - binary_crossentropy: 0.3276 - val_loss: 2.9277 - val_mean_iou: 0.1268 - val_binary_crossentropy: 2.9277\n",
            "Epoch 7/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.3014 - mean_iou: 0.2461 - binary_crossentropy: 0.3014 - val_loss: 3.3672 - val_mean_iou: 0.1354 - val_binary_crossentropy: 3.3672\n",
            "Epoch 8/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.2810 - mean_iou: 0.2875 - binary_crossentropy: 0.2810 - val_loss: 3.9008 - val_mean_iou: 0.1453 - val_binary_crossentropy: 3.9008\n",
            "Epoch 9/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.2643 - mean_iou: 0.3255 - binary_crossentropy: 0.2643 - val_loss: 3.3865 - val_mean_iou: 0.1573 - val_binary_crossentropy: 3.3865\n",
            "Epoch 10/20000\n",
            "25/25 [==============================] - 12s 489ms/step - loss: 0.2613 - mean_iou: 0.3838 - binary_crossentropy: 0.2613 - val_loss: 3.3949 - val_mean_iou: 0.1442 - val_binary_crossentropy: 3.3949\n",
            "Epoch 11/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.2310 - mean_iou: 0.4367 - binary_crossentropy: 0.2310 - val_loss: 3.3044 - val_mean_iou: 0.1388 - val_binary_crossentropy: 3.3044\n",
            "Epoch 12/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.2316 - mean_iou: 0.4943 - binary_crossentropy: 0.2316 - val_loss: 2.2220 - val_mean_iou: 0.1463 - val_binary_crossentropy: 2.2220\n",
            "Epoch 13/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.2034 - mean_iou: 0.5207 - binary_crossentropy: 0.2034 - val_loss: 1.7998 - val_mean_iou: 0.2094 - val_binary_crossentropy: 1.7998\n",
            "Epoch 14/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.1861 - mean_iou: 0.5430 - binary_crossentropy: 0.1861 - val_loss: 1.0798 - val_mean_iou: 0.3367 - val_binary_crossentropy: 1.0798\n",
            "Epoch 15/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.1860 - mean_iou: 0.5631 - binary_crossentropy: 0.1860 - val_loss: 0.6753 - val_mean_iou: 0.3480 - val_binary_crossentropy: 0.6753\n",
            "Epoch 16/20000\n",
            "25/25 [==============================] - 13s 525ms/step - loss: 0.1732 - mean_iou: 0.5751 - binary_crossentropy: 0.1732 - val_loss: 0.5043 - val_mean_iou: 0.4430 - val_binary_crossentropy: 0.5043\n",
            "Epoch 17/20000\n",
            "25/25 [==============================] - 12s 499ms/step - loss: 0.1629 - mean_iou: 0.5899 - binary_crossentropy: 0.1629 - val_loss: 0.6240 - val_mean_iou: 0.4140 - val_binary_crossentropy: 0.6240\n",
            "Epoch 18/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.1505 - mean_iou: 0.5980 - binary_crossentropy: 0.1505 - val_loss: 0.4491 - val_mean_iou: 0.4703 - val_binary_crossentropy: 0.4491\n",
            "Epoch 19/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.1458 - mean_iou: 0.6116 - binary_crossentropy: 0.1458 - val_loss: 0.4479 - val_mean_iou: 0.5081 - val_binary_crossentropy: 0.4479\n",
            "Epoch 20/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.1424 - mean_iou: 0.5943 - binary_crossentropy: 0.1424 - val_loss: 0.3264 - val_mean_iou: 0.4962 - val_binary_crossentropy: 0.3264\n",
            "Epoch 21/20000\n",
            "25/25 [==============================] - 12s 489ms/step - loss: 0.1406 - mean_iou: 0.5910 - binary_crossentropy: 0.1406 - val_loss: 0.3533 - val_mean_iou: 0.5219 - val_binary_crossentropy: 0.3533\n",
            "Epoch 22/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.1324 - mean_iou: 0.6238 - binary_crossentropy: 0.1324 - val_loss: 0.4473 - val_mean_iou: 0.5012 - val_binary_crossentropy: 0.4473\n",
            "Epoch 23/20000\n",
            "25/25 [==============================] - 12s 497ms/step - loss: 0.1271 - mean_iou: 0.6343 - binary_crossentropy: 0.1271 - val_loss: 0.3731 - val_mean_iou: 0.5541 - val_binary_crossentropy: 0.3731\n",
            "Epoch 24/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.1221 - mean_iou: 0.6349 - binary_crossentropy: 0.1221 - val_loss: 0.3247 - val_mean_iou: 0.5512 - val_binary_crossentropy: 0.3247\n",
            "Epoch 25/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.1252 - mean_iou: 0.6410 - binary_crossentropy: 0.1252 - val_loss: 0.3097 - val_mean_iou: 0.5172 - val_binary_crossentropy: 0.3097\n",
            "Epoch 26/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.1252 - mean_iou: 0.6397 - binary_crossentropy: 0.1252 - val_loss: 0.2911 - val_mean_iou: 0.5482 - val_binary_crossentropy: 0.2911\n",
            "Epoch 27/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.1091 - mean_iou: 0.6672 - binary_crossentropy: 0.1091 - val_loss: 0.3087 - val_mean_iou: 0.5374 - val_binary_crossentropy: 0.3087\n",
            "Epoch 28/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.1172 - mean_iou: 0.6369 - binary_crossentropy: 0.1172 - val_loss: 0.3274 - val_mean_iou: 0.5199 - val_binary_crossentropy: 0.3274\n",
            "Epoch 29/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.1069 - mean_iou: 0.6712 - binary_crossentropy: 0.1069 - val_loss: 0.3197 - val_mean_iou: 0.5456 - val_binary_crossentropy: 0.3197\n",
            "Epoch 30/20000\n",
            "25/25 [==============================] - 12s 496ms/step - loss: 0.1111 - mean_iou: 0.6451 - binary_crossentropy: 0.1111 - val_loss: 0.2915 - val_mean_iou: 0.5454 - val_binary_crossentropy: 0.2915\n",
            "Epoch 31/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.1055 - mean_iou: 0.6856 - binary_crossentropy: 0.1055 - val_loss: 0.2500 - val_mean_iou: 0.5744 - val_binary_crossentropy: 0.2500\n",
            "Epoch 32/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.0975 - mean_iou: 0.6881 - binary_crossentropy: 0.0975 - val_loss: 0.3169 - val_mean_iou: 0.5319 - val_binary_crossentropy: 0.3169\n",
            "Epoch 33/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.0962 - mean_iou: 0.6728 - binary_crossentropy: 0.0962 - val_loss: 0.2797 - val_mean_iou: 0.5627 - val_binary_crossentropy: 0.2797\n",
            "Epoch 34/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.0974 - mean_iou: 0.6951 - binary_crossentropy: 0.0974 - val_loss: 0.3422 - val_mean_iou: 0.5219 - val_binary_crossentropy: 0.3422\n",
            "Epoch 35/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.1004 - mean_iou: 0.6644 - binary_crossentropy: 0.1004 - val_loss: 0.3115 - val_mean_iou: 0.5340 - val_binary_crossentropy: 0.3115\n",
            "Epoch 36/20000\n",
            "25/25 [==============================] - 12s 497ms/step - loss: 0.1001 - mean_iou: 0.6714 - binary_crossentropy: 0.1001 - val_loss: 0.3254 - val_mean_iou: 0.5315 - val_binary_crossentropy: 0.3254\n",
            "Epoch 37/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.1024 - mean_iou: 0.6750 - binary_crossentropy: 0.1024 - val_loss: 0.2766 - val_mean_iou: 0.5339 - val_binary_crossentropy: 0.2766\n",
            "Epoch 38/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0981 - mean_iou: 0.6844 - binary_crossentropy: 0.0981 - val_loss: 0.3220 - val_mean_iou: 0.5398 - val_binary_crossentropy: 0.3220\n",
            "Epoch 39/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0965 - mean_iou: 0.6873 - binary_crossentropy: 0.0965 - val_loss: 0.2813 - val_mean_iou: 0.5519 - val_binary_crossentropy: 0.2813\n",
            "Epoch 40/20000\n",
            "25/25 [==============================] - 12s 487ms/step - loss: 0.0934 - mean_iou: 0.6839 - binary_crossentropy: 0.0934 - val_loss: 0.3514 - val_mean_iou: 0.5422 - val_binary_crossentropy: 0.3514\n",
            "Epoch 41/20000\n",
            "25/25 [==============================] - 13s 515ms/step - loss: 0.0931 - mean_iou: 0.6968 - binary_crossentropy: 0.0931 - val_loss: 0.3127 - val_mean_iou: 0.5355 - val_binary_crossentropy: 0.3127\n",
            "Epoch 42/20000\n",
            "25/25 [==============================] - 13s 505ms/step - loss: 0.0967 - mean_iou: 0.6804 - binary_crossentropy: 0.0967 - val_loss: 0.2856 - val_mean_iou: 0.5708 - val_binary_crossentropy: 0.2856\n",
            "Epoch 43/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.0914 - mean_iou: 0.7027 - binary_crossentropy: 0.0914 - val_loss: 0.2619 - val_mean_iou: 0.5444 - val_binary_crossentropy: 0.2619\n",
            "Epoch 44/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0926 - mean_iou: 0.6900 - binary_crossentropy: 0.0926 - val_loss: 0.3018 - val_mean_iou: 0.5442 - val_binary_crossentropy: 0.3018\n",
            "Epoch 45/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0910 - mean_iou: 0.6960 - binary_crossentropy: 0.0910 - val_loss: 0.3280 - val_mean_iou: 0.5583 - val_binary_crossentropy: 0.3280\n",
            "Epoch 46/20000\n",
            "25/25 [==============================] - 12s 486ms/step - loss: 0.0896 - mean_iou: 0.7204 - binary_crossentropy: 0.0896 - val_loss: 0.2903 - val_mean_iou: 0.5765 - val_binary_crossentropy: 0.2903\n",
            "Epoch 47/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.0896 - mean_iou: 0.6901 - binary_crossentropy: 0.0896 - val_loss: 0.2940 - val_mean_iou: 0.5652 - val_binary_crossentropy: 0.2940\n",
            "Epoch 48/20000\n",
            "25/25 [==============================] - 12s 489ms/step - loss: 0.0876 - mean_iou: 0.7041 - binary_crossentropy: 0.0876 - val_loss: 0.3032 - val_mean_iou: 0.5996 - val_binary_crossentropy: 0.3032\n",
            "Epoch 49/20000\n",
            "25/25 [==============================] - 12s 485ms/step - loss: 0.0831 - mean_iou: 0.7002 - binary_crossentropy: 0.0831 - val_loss: 0.3155 - val_mean_iou: 0.5838 - val_binary_crossentropy: 0.3155\n",
            "Epoch 50/20000\n",
            "25/25 [==============================] - 12s 487ms/step - loss: 0.0861 - mean_iou: 0.6991 - binary_crossentropy: 0.0861 - val_loss: 0.2872 - val_mean_iou: 0.5874 - val_binary_crossentropy: 0.2872\n",
            "Epoch 51/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0818 - mean_iou: 0.7035 - binary_crossentropy: 0.0818 - val_loss: 0.2907 - val_mean_iou: 0.5629 - val_binary_crossentropy: 0.2907\n",
            "Epoch 52/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0796 - mean_iou: 0.7124 - binary_crossentropy: 0.0796 - val_loss: 0.3302 - val_mean_iou: 0.5551 - val_binary_crossentropy: 0.3302\n",
            "Epoch 53/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0845 - mean_iou: 0.7008 - binary_crossentropy: 0.0845 - val_loss: 0.3048 - val_mean_iou: 0.5681 - val_binary_crossentropy: 0.3048\n",
            "Epoch 54/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0861 - mean_iou: 0.7071 - binary_crossentropy: 0.0861 - val_loss: 0.2815 - val_mean_iou: 0.5708 - val_binary_crossentropy: 0.2815\n",
            "Epoch 55/20000\n",
            "25/25 [==============================] - 12s 489ms/step - loss: 0.0837 - mean_iou: 0.7038 - binary_crossentropy: 0.0837 - val_loss: 0.3382 - val_mean_iou: 0.5791 - val_binary_crossentropy: 0.3382\n",
            "Epoch 56/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0827 - mean_iou: 0.7300 - binary_crossentropy: 0.0827 - val_loss: 0.3167 - val_mean_iou: 0.5794 - val_binary_crossentropy: 0.3167\n",
            "Epoch 57/20000\n",
            "25/25 [==============================] - 12s 496ms/step - loss: 0.0807 - mean_iou: 0.7114 - binary_crossentropy: 0.0807 - val_loss: 0.3135 - val_mean_iou: 0.5769 - val_binary_crossentropy: 0.3135\n",
            "Epoch 58/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0763 - mean_iou: 0.7233 - binary_crossentropy: 0.0763 - val_loss: 0.3207 - val_mean_iou: 0.5749 - val_binary_crossentropy: 0.3207\n",
            "Epoch 59/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0758 - mean_iou: 0.7306 - binary_crossentropy: 0.0758 - val_loss: 0.2584 - val_mean_iou: 0.5879 - val_binary_crossentropy: 0.2584\n",
            "Epoch 60/20000\n",
            "25/25 [==============================] - 12s 487ms/step - loss: 0.0752 - mean_iou: 0.7189 - binary_crossentropy: 0.0752 - val_loss: 0.3066 - val_mean_iou: 0.5720 - val_binary_crossentropy: 0.3066\n",
            "Epoch 61/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0771 - mean_iou: 0.7297 - binary_crossentropy: 0.0771 - val_loss: 0.3299 - val_mean_iou: 0.5596 - val_binary_crossentropy: 0.3299\n",
            "Epoch 62/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.0772 - mean_iou: 0.7237 - binary_crossentropy: 0.0772 - val_loss: 0.2768 - val_mean_iou: 0.5927 - val_binary_crossentropy: 0.2768\n",
            "Epoch 63/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0786 - mean_iou: 0.7205 - binary_crossentropy: 0.0786 - val_loss: 0.3373 - val_mean_iou: 0.5682 - val_binary_crossentropy: 0.3373\n",
            "Epoch 64/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0727 - mean_iou: 0.7299 - binary_crossentropy: 0.0727 - val_loss: 0.3225 - val_mean_iou: 0.5721 - val_binary_crossentropy: 0.3225\n",
            "Epoch 65/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0795 - mean_iou: 0.7134 - binary_crossentropy: 0.0795 - val_loss: 0.2932 - val_mean_iou: 0.5458 - val_binary_crossentropy: 0.2932\n",
            "Epoch 66/20000\n",
            "25/25 [==============================] - 13s 510ms/step - loss: 0.0765 - mean_iou: 0.7123 - binary_crossentropy: 0.0765 - val_loss: 0.3332 - val_mean_iou: 0.5850 - val_binary_crossentropy: 0.3332\n",
            "Epoch 67/20000\n",
            "25/25 [==============================] - 13s 514ms/step - loss: 0.0769 - mean_iou: 0.7263 - binary_crossentropy: 0.0769 - val_loss: 0.3484 - val_mean_iou: 0.5585 - val_binary_crossentropy: 0.3484\n",
            "Epoch 68/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0725 - mean_iou: 0.7393 - binary_crossentropy: 0.0725 - val_loss: 0.3587 - val_mean_iou: 0.5696 - val_binary_crossentropy: 0.3587\n",
            "Epoch 69/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0742 - mean_iou: 0.7244 - binary_crossentropy: 0.0742 - val_loss: 0.4042 - val_mean_iou: 0.5669 - val_binary_crossentropy: 0.4042\n",
            "Epoch 70/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0722 - mean_iou: 0.7278 - binary_crossentropy: 0.0722 - val_loss: 0.3708 - val_mean_iou: 0.6141 - val_binary_crossentropy: 0.3708\n",
            "Epoch 71/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0705 - mean_iou: 0.7249 - binary_crossentropy: 0.0705 - val_loss: 0.3415 - val_mean_iou: 0.5643 - val_binary_crossentropy: 0.3415\n",
            "Epoch 72/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0731 - mean_iou: 0.7164 - binary_crossentropy: 0.0731 - val_loss: 0.4215 - val_mean_iou: 0.5657 - val_binary_crossentropy: 0.4215\n",
            "Epoch 73/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.0721 - mean_iou: 0.7288 - binary_crossentropy: 0.0721 - val_loss: 0.3840 - val_mean_iou: 0.5773 - val_binary_crossentropy: 0.3840\n",
            "Epoch 74/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0733 - mean_iou: 0.7334 - binary_crossentropy: 0.0733 - val_loss: 0.3045 - val_mean_iou: 0.5869 - val_binary_crossentropy: 0.3045\n",
            "Epoch 75/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0720 - mean_iou: 0.7237 - binary_crossentropy: 0.0720 - val_loss: 0.3556 - val_mean_iou: 0.5902 - val_binary_crossentropy: 0.3556\n",
            "Epoch 76/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0724 - mean_iou: 0.7302 - binary_crossentropy: 0.0724 - val_loss: 0.3559 - val_mean_iou: 0.5662 - val_binary_crossentropy: 0.3559\n",
            "Epoch 77/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0712 - mean_iou: 0.7443 - binary_crossentropy: 0.0712 - val_loss: 0.3489 - val_mean_iou: 0.5693 - val_binary_crossentropy: 0.3489\n",
            "Epoch 78/20000\n",
            "25/25 [==============================] - 12s 498ms/step - loss: 0.0684 - mean_iou: 0.7367 - binary_crossentropy: 0.0684 - val_loss: 0.3098 - val_mean_iou: 0.5990 - val_binary_crossentropy: 0.3098\n",
            "Epoch 79/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0713 - mean_iou: 0.7419 - binary_crossentropy: 0.0713 - val_loss: 0.3127 - val_mean_iou: 0.5749 - val_binary_crossentropy: 0.3127\n",
            "Epoch 80/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0712 - mean_iou: 0.7129 - binary_crossentropy: 0.0712 - val_loss: 0.3516 - val_mean_iou: 0.5882 - val_binary_crossentropy: 0.3516\n",
            "Epoch 81/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0728 - mean_iou: 0.7284 - binary_crossentropy: 0.0728 - val_loss: 0.3625 - val_mean_iou: 0.5679 - val_binary_crossentropy: 0.3625\n",
            "Epoch 82/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0760 - mean_iou: 0.7251 - binary_crossentropy: 0.0760 - val_loss: 0.3257 - val_mean_iou: 0.5836 - val_binary_crossentropy: 0.3257\n",
            "Epoch 83/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0730 - mean_iou: 0.7177 - binary_crossentropy: 0.0730 - val_loss: 0.3277 - val_mean_iou: 0.5897 - val_binary_crossentropy: 0.3277\n",
            "Epoch 84/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0686 - mean_iou: 0.7270 - binary_crossentropy: 0.0686 - val_loss: 0.3378 - val_mean_iou: 0.5691 - val_binary_crossentropy: 0.3378\n",
            "Epoch 85/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0682 - mean_iou: 0.7331 - binary_crossentropy: 0.0682 - val_loss: 0.3797 - val_mean_iou: 0.5739 - val_binary_crossentropy: 0.3797\n",
            "Epoch 86/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0727 - mean_iou: 0.7362 - binary_crossentropy: 0.0727 - val_loss: 0.3760 - val_mean_iou: 0.5799 - val_binary_crossentropy: 0.3760\n",
            "Epoch 87/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0689 - mean_iou: 0.7298 - binary_crossentropy: 0.0689 - val_loss: 0.3654 - val_mean_iou: 0.5434 - val_binary_crossentropy: 0.3654\n",
            "Epoch 88/20000\n",
            "25/25 [==============================] - 12s 488ms/step - loss: 0.0723 - mean_iou: 0.7251 - binary_crossentropy: 0.0723 - val_loss: 0.3722 - val_mean_iou: 0.5835 - val_binary_crossentropy: 0.3722\n",
            "Epoch 89/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0686 - mean_iou: 0.7306 - binary_crossentropy: 0.0686 - val_loss: 0.3392 - val_mean_iou: 0.5891 - val_binary_crossentropy: 0.3392\n",
            "Epoch 90/20000\n",
            "25/25 [==============================] - 12s 487ms/step - loss: 0.0738 - mean_iou: 0.7285 - binary_crossentropy: 0.0738 - val_loss: 0.3609 - val_mean_iou: 0.5725 - val_binary_crossentropy: 0.3609\n",
            "Epoch 91/20000\n",
            "25/25 [==============================] - 13s 504ms/step - loss: 0.0694 - mean_iou: 0.7361 - binary_crossentropy: 0.0694 - val_loss: 0.4078 - val_mean_iou: 0.5621 - val_binary_crossentropy: 0.4078\n",
            "Epoch 92/20000\n",
            "25/25 [==============================] - 13s 519ms/step - loss: 0.0691 - mean_iou: 0.7477 - binary_crossentropy: 0.0691 - val_loss: 0.3442 - val_mean_iou: 0.5970 - val_binary_crossentropy: 0.3442\n",
            "Epoch 93/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0663 - mean_iou: 0.7474 - binary_crossentropy: 0.0663 - val_loss: 0.3453 - val_mean_iou: 0.5930 - val_binary_crossentropy: 0.3453\n",
            "Epoch 94/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0670 - mean_iou: 0.7378 - binary_crossentropy: 0.0670 - val_loss: 0.3594 - val_mean_iou: 0.5905 - val_binary_crossentropy: 0.3594\n",
            "Epoch 95/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0670 - mean_iou: 0.7407 - binary_crossentropy: 0.0670 - val_loss: 0.3464 - val_mean_iou: 0.5824 - val_binary_crossentropy: 0.3464\n",
            "Epoch 96/20000\n",
            "25/25 [==============================] - 12s 496ms/step - loss: 0.0726 - mean_iou: 0.7242 - binary_crossentropy: 0.0726 - val_loss: 0.3824 - val_mean_iou: 0.5886 - val_binary_crossentropy: 0.3824\n",
            "Epoch 97/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0663 - mean_iou: 0.7399 - binary_crossentropy: 0.0663 - val_loss: 0.3362 - val_mean_iou: 0.5875 - val_binary_crossentropy: 0.3362\n",
            "Epoch 98/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0638 - mean_iou: 0.7454 - binary_crossentropy: 0.0638 - val_loss: 0.3833 - val_mean_iou: 0.5427 - val_binary_crossentropy: 0.3833\n",
            "Epoch 99/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0675 - mean_iou: 0.7348 - binary_crossentropy: 0.0675 - val_loss: 0.3624 - val_mean_iou: 0.6044 - val_binary_crossentropy: 0.3624\n",
            "Epoch 100/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0654 - mean_iou: 0.7406 - binary_crossentropy: 0.0654 - val_loss: 0.3444 - val_mean_iou: 0.5696 - val_binary_crossentropy: 0.3444\n",
            "Epoch 101/20000\n",
            "25/25 [==============================] - 12s 489ms/step - loss: 0.0630 - mean_iou: 0.7363 - binary_crossentropy: 0.0630 - val_loss: 0.3787 - val_mean_iou: 0.5740 - val_binary_crossentropy: 0.3787\n",
            "Epoch 102/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0619 - mean_iou: 0.7361 - binary_crossentropy: 0.0619 - val_loss: 0.4212 - val_mean_iou: 0.5559 - val_binary_crossentropy: 0.4212\n",
            "Epoch 103/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0604 - mean_iou: 0.7563 - binary_crossentropy: 0.0604 - val_loss: 0.3801 - val_mean_iou: 0.5704 - val_binary_crossentropy: 0.3801\n",
            "Epoch 104/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0588 - mean_iou: 0.7504 - binary_crossentropy: 0.0588 - val_loss: 0.3748 - val_mean_iou: 0.5942 - val_binary_crossentropy: 0.3748\n",
            "Epoch 105/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0628 - mean_iou: 0.7441 - binary_crossentropy: 0.0628 - val_loss: 0.3757 - val_mean_iou: 0.5982 - val_binary_crossentropy: 0.3757\n",
            "Epoch 106/20000\n",
            "25/25 [==============================] - 12s 497ms/step - loss: 0.0606 - mean_iou: 0.7575 - binary_crossentropy: 0.0606 - val_loss: 0.3401 - val_mean_iou: 0.5816 - val_binary_crossentropy: 0.3401\n",
            "Epoch 107/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0615 - mean_iou: 0.7504 - binary_crossentropy: 0.0615 - val_loss: 0.3774 - val_mean_iou: 0.5981 - val_binary_crossentropy: 0.3774\n",
            "Epoch 108/20000\n",
            "25/25 [==============================] - 12s 489ms/step - loss: 0.0627 - mean_iou: 0.7513 - binary_crossentropy: 0.0627 - val_loss: 0.3622 - val_mean_iou: 0.5772 - val_binary_crossentropy: 0.3622\n",
            "Epoch 109/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0635 - mean_iou: 0.7434 - binary_crossentropy: 0.0635 - val_loss: 0.4025 - val_mean_iou: 0.5497 - val_binary_crossentropy: 0.4025\n",
            "Epoch 110/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0647 - mean_iou: 0.7489 - binary_crossentropy: 0.0647 - val_loss: 0.3822 - val_mean_iou: 0.5775 - val_binary_crossentropy: 0.3822\n",
            "Epoch 111/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0597 - mean_iou: 0.7572 - binary_crossentropy: 0.0597 - val_loss: 0.4247 - val_mean_iou: 0.5612 - val_binary_crossentropy: 0.4247\n",
            "Epoch 112/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0658 - mean_iou: 0.7447 - binary_crossentropy: 0.0658 - val_loss: 0.3761 - val_mean_iou: 0.5760 - val_binary_crossentropy: 0.3761\n",
            "Epoch 113/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0709 - mean_iou: 0.7450 - binary_crossentropy: 0.0709 - val_loss: 0.3172 - val_mean_iou: 0.5715 - val_binary_crossentropy: 0.3172\n",
            "Epoch 114/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0635 - mean_iou: 0.7404 - binary_crossentropy: 0.0635 - val_loss: 0.3868 - val_mean_iou: 0.5700 - val_binary_crossentropy: 0.3868\n",
            "Epoch 115/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0624 - mean_iou: 0.7651 - binary_crossentropy: 0.0624 - val_loss: 0.4467 - val_mean_iou: 0.5730 - val_binary_crossentropy: 0.4467\n",
            "Epoch 116/20000\n",
            "25/25 [==============================] - 12s 498ms/step - loss: 0.0608 - mean_iou: 0.7619 - binary_crossentropy: 0.0608 - val_loss: 0.4176 - val_mean_iou: 0.5854 - val_binary_crossentropy: 0.4176\n",
            "Epoch 117/20000\n",
            "25/25 [==============================] - 13s 523ms/step - loss: 0.0589 - mean_iou: 0.7619 - binary_crossentropy: 0.0589 - val_loss: 0.3420 - val_mean_iou: 0.5949 - val_binary_crossentropy: 0.3420\n",
            "Epoch 118/20000\n",
            "25/25 [==============================] - 12s 487ms/step - loss: 0.0597 - mean_iou: 0.7561 - binary_crossentropy: 0.0597 - val_loss: 0.3321 - val_mean_iou: 0.5872 - val_binary_crossentropy: 0.3321\n",
            "Epoch 119/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0600 - mean_iou: 0.7519 - binary_crossentropy: 0.0600 - val_loss: 0.3907 - val_mean_iou: 0.5874 - val_binary_crossentropy: 0.3907\n",
            "Epoch 120/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0597 - mean_iou: 0.7520 - binary_crossentropy: 0.0597 - val_loss: 0.4440 - val_mean_iou: 0.5761 - val_binary_crossentropy: 0.4440\n",
            "Epoch 121/20000\n",
            "25/25 [==============================] - 12s 489ms/step - loss: 0.0705 - mean_iou: 0.7445 - binary_crossentropy: 0.0705 - val_loss: 0.3444 - val_mean_iou: 0.5888 - val_binary_crossentropy: 0.3444\n",
            "Epoch 122/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0625 - mean_iou: 0.7463 - binary_crossentropy: 0.0625 - val_loss: 0.4073 - val_mean_iou: 0.5788 - val_binary_crossentropy: 0.4073\n",
            "Epoch 123/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0629 - mean_iou: 0.7592 - binary_crossentropy: 0.0629 - val_loss: 0.4031 - val_mean_iou: 0.5939 - val_binary_crossentropy: 0.4031\n",
            "Epoch 124/20000\n",
            "25/25 [==============================] - 12s 496ms/step - loss: 0.0596 - mean_iou: 0.7590 - binary_crossentropy: 0.0596 - val_loss: 0.3670 - val_mean_iou: 0.5787 - val_binary_crossentropy: 0.3670\n",
            "Epoch 125/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0595 - mean_iou: 0.7582 - binary_crossentropy: 0.0595 - val_loss: 0.3605 - val_mean_iou: 0.6011 - val_binary_crossentropy: 0.3605\n",
            "Epoch 126/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0618 - mean_iou: 0.7546 - binary_crossentropy: 0.0618 - val_loss: 0.4091 - val_mean_iou: 0.5843 - val_binary_crossentropy: 0.4091\n",
            "Epoch 127/20000\n",
            "25/25 [==============================] - 12s 500ms/step - loss: 0.0601 - mean_iou: 0.7614 - binary_crossentropy: 0.0601 - val_loss: 0.3695 - val_mean_iou: 0.5903 - val_binary_crossentropy: 0.3695\n",
            "Epoch 128/20000\n",
            "25/25 [==============================] - 12s 489ms/step - loss: 0.0578 - mean_iou: 0.7570 - binary_crossentropy: 0.0578 - val_loss: 0.3954 - val_mean_iou: 0.5802 - val_binary_crossentropy: 0.3954\n",
            "Epoch 129/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0574 - mean_iou: 0.7593 - binary_crossentropy: 0.0574 - val_loss: 0.4146 - val_mean_iou: 0.5703 - val_binary_crossentropy: 0.4146\n",
            "Epoch 130/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0589 - mean_iou: 0.7686 - binary_crossentropy: 0.0589 - val_loss: 0.4739 - val_mean_iou: 0.5844 - val_binary_crossentropy: 0.4739\n",
            "Epoch 131/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0590 - mean_iou: 0.7558 - binary_crossentropy: 0.0590 - val_loss: 0.4636 - val_mean_iou: 0.5590 - val_binary_crossentropy: 0.4636\n",
            "Epoch 132/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0576 - mean_iou: 0.7699 - binary_crossentropy: 0.0576 - val_loss: 0.3872 - val_mean_iou: 0.5663 - val_binary_crossentropy: 0.3872\n",
            "Epoch 133/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0579 - mean_iou: 0.7660 - binary_crossentropy: 0.0579 - val_loss: 0.4339 - val_mean_iou: 0.5888 - val_binary_crossentropy: 0.4339\n",
            "Epoch 134/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0625 - mean_iou: 0.7501 - binary_crossentropy: 0.0625 - val_loss: 0.4338 - val_mean_iou: 0.5563 - val_binary_crossentropy: 0.4338\n",
            "Epoch 135/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0622 - mean_iou: 0.7532 - binary_crossentropy: 0.0622 - val_loss: 0.4153 - val_mean_iou: 0.5996 - val_binary_crossentropy: 0.4153\n",
            "Epoch 136/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0569 - mean_iou: 0.7642 - binary_crossentropy: 0.0569 - val_loss: 0.4147 - val_mean_iou: 0.5479 - val_binary_crossentropy: 0.4147\n",
            "Epoch 137/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0598 - mean_iou: 0.7691 - binary_crossentropy: 0.0598 - val_loss: 0.4092 - val_mean_iou: 0.5643 - val_binary_crossentropy: 0.4092\n",
            "Epoch 138/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0706 - mean_iou: 0.7472 - binary_crossentropy: 0.0706 - val_loss: 0.3300 - val_mean_iou: 0.5449 - val_binary_crossentropy: 0.3300\n",
            "Epoch 139/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0687 - mean_iou: 0.7299 - binary_crossentropy: 0.0687 - val_loss: 0.4086 - val_mean_iou: 0.5488 - val_binary_crossentropy: 0.4086\n",
            "Epoch 140/20000\n",
            "25/25 [==============================] - 12s 497ms/step - loss: 0.0665 - mean_iou: 0.7418 - binary_crossentropy: 0.0665 - val_loss: 0.3998 - val_mean_iou: 0.5324 - val_binary_crossentropy: 0.3998\n",
            "Epoch 141/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0599 - mean_iou: 0.7503 - binary_crossentropy: 0.0599 - val_loss: 0.3885 - val_mean_iou: 0.5709 - val_binary_crossentropy: 0.3885\n",
            "Epoch 142/20000\n",
            "25/25 [==============================] - 13s 529ms/step - loss: 0.0562 - mean_iou: 0.7713 - binary_crossentropy: 0.0562 - val_loss: 0.4286 - val_mean_iou: 0.5530 - val_binary_crossentropy: 0.4286\n",
            "Epoch 143/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0584 - mean_iou: 0.7667 - binary_crossentropy: 0.0584 - val_loss: 0.3610 - val_mean_iou: 0.6010 - val_binary_crossentropy: 0.3610\n",
            "Epoch 144/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0592 - mean_iou: 0.7633 - binary_crossentropy: 0.0592 - val_loss: 0.4387 - val_mean_iou: 0.5770 - val_binary_crossentropy: 0.4387\n",
            "Epoch 145/20000\n",
            "25/25 [==============================] - 12s 496ms/step - loss: 0.0581 - mean_iou: 0.7589 - binary_crossentropy: 0.0581 - val_loss: 0.4342 - val_mean_iou: 0.5873 - val_binary_crossentropy: 0.4342\n",
            "Epoch 146/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0560 - mean_iou: 0.7662 - binary_crossentropy: 0.0560 - val_loss: 0.4054 - val_mean_iou: 0.5814 - val_binary_crossentropy: 0.4054\n",
            "Epoch 147/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0558 - mean_iou: 0.7714 - binary_crossentropy: 0.0558 - val_loss: 0.4488 - val_mean_iou: 0.5890 - val_binary_crossentropy: 0.4488\n",
            "Epoch 148/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0591 - mean_iou: 0.7623 - binary_crossentropy: 0.0591 - val_loss: 0.3962 - val_mean_iou: 0.5951 - val_binary_crossentropy: 0.3962\n",
            "Epoch 149/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0566 - mean_iou: 0.7766 - binary_crossentropy: 0.0566 - val_loss: 0.3922 - val_mean_iou: 0.5953 - val_binary_crossentropy: 0.3922\n",
            "Epoch 150/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0567 - mean_iou: 0.7763 - binary_crossentropy: 0.0567 - val_loss: 0.3680 - val_mean_iou: 0.5827 - val_binary_crossentropy: 0.3680\n",
            "Epoch 151/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0545 - mean_iou: 0.7679 - binary_crossentropy: 0.0545 - val_loss: 0.4097 - val_mean_iou: 0.5782 - val_binary_crossentropy: 0.4097\n",
            "Epoch 152/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0611 - mean_iou: 0.7631 - binary_crossentropy: 0.0611 - val_loss: 0.3746 - val_mean_iou: 0.5814 - val_binary_crossentropy: 0.3746\n",
            "Epoch 153/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0667 - mean_iou: 0.7480 - binary_crossentropy: 0.0667 - val_loss: 0.4039 - val_mean_iou: 0.5738 - val_binary_crossentropy: 0.4039\n",
            "Epoch 154/20000\n",
            "25/25 [==============================] - 12s 497ms/step - loss: 0.0588 - mean_iou: 0.7478 - binary_crossentropy: 0.0588 - val_loss: 0.3947 - val_mean_iou: 0.5680 - val_binary_crossentropy: 0.3947\n",
            "Epoch 155/20000\n",
            "25/25 [==============================] - 12s 496ms/step - loss: 0.0595 - mean_iou: 0.7631 - binary_crossentropy: 0.0595 - val_loss: 0.3719 - val_mean_iou: 0.5623 - val_binary_crossentropy: 0.3719\n",
            "Epoch 156/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0590 - mean_iou: 0.7628 - binary_crossentropy: 0.0590 - val_loss: 0.4399 - val_mean_iou: 0.5766 - val_binary_crossentropy: 0.4399\n",
            "Epoch 157/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0567 - mean_iou: 0.7704 - binary_crossentropy: 0.0567 - val_loss: 0.3819 - val_mean_iou: 0.5931 - val_binary_crossentropy: 0.3819\n",
            "Epoch 158/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0554 - mean_iou: 0.7744 - binary_crossentropy: 0.0554 - val_loss: 0.4581 - val_mean_iou: 0.5877 - val_binary_crossentropy: 0.4581\n",
            "Epoch 159/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0562 - mean_iou: 0.7676 - binary_crossentropy: 0.0562 - val_loss: 0.3834 - val_mean_iou: 0.5785 - val_binary_crossentropy: 0.3834\n",
            "Epoch 160/20000\n",
            "25/25 [==============================] - 12s 497ms/step - loss: 0.0568 - mean_iou: 0.7648 - binary_crossentropy: 0.0568 - val_loss: 0.3933 - val_mean_iou: 0.5634 - val_binary_crossentropy: 0.3933\n",
            "Epoch 161/20000\n",
            "25/25 [==============================] - 12s 497ms/step - loss: 0.0550 - mean_iou: 0.7783 - binary_crossentropy: 0.0550 - val_loss: 0.4265 - val_mean_iou: 0.5701 - val_binary_crossentropy: 0.4265\n",
            "Epoch 162/20000\n",
            "25/25 [==============================] - 12s 496ms/step - loss: 0.0534 - mean_iou: 0.7622 - binary_crossentropy: 0.0534 - val_loss: 0.4117 - val_mean_iou: 0.5833 - val_binary_crossentropy: 0.4117\n",
            "Epoch 163/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0534 - mean_iou: 0.7784 - binary_crossentropy: 0.0534 - val_loss: 0.4646 - val_mean_iou: 0.5446 - val_binary_crossentropy: 0.4646\n",
            "Epoch 164/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0562 - mean_iou: 0.7550 - binary_crossentropy: 0.0562 - val_loss: 0.4647 - val_mean_iou: 0.5608 - val_binary_crossentropy: 0.4647\n",
            "Epoch 165/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0548 - mean_iou: 0.7714 - binary_crossentropy: 0.0548 - val_loss: 0.4977 - val_mean_iou: 0.5560 - val_binary_crossentropy: 0.4977\n",
            "Epoch 166/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0549 - mean_iou: 0.7644 - binary_crossentropy: 0.0549 - val_loss: 0.4169 - val_mean_iou: 0.5670 - val_binary_crossentropy: 0.4169\n",
            "Epoch 167/20000\n",
            "25/25 [==============================] - 13s 531ms/step - loss: 0.0569 - mean_iou: 0.7753 - binary_crossentropy: 0.0569 - val_loss: 0.4080 - val_mean_iou: 0.5478 - val_binary_crossentropy: 0.4080\n",
            "Epoch 168/20000\n",
            "25/25 [==============================] - 12s 499ms/step - loss: 0.0554 - mean_iou: 0.7588 - binary_crossentropy: 0.0554 - val_loss: 0.3906 - val_mean_iou: 0.5603 - val_binary_crossentropy: 0.3906\n",
            "Epoch 169/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0613 - mean_iou: 0.7567 - binary_crossentropy: 0.0613 - val_loss: 0.3787 - val_mean_iou: 0.5643 - val_binary_crossentropy: 0.3787\n",
            "Epoch 170/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0593 - mean_iou: 0.7653 - binary_crossentropy: 0.0593 - val_loss: 0.4494 - val_mean_iou: 0.5597 - val_binary_crossentropy: 0.4494\n",
            "Epoch 171/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0554 - mean_iou: 0.7695 - binary_crossentropy: 0.0554 - val_loss: 0.3811 - val_mean_iou: 0.6195 - val_binary_crossentropy: 0.3811\n",
            "Epoch 172/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0570 - mean_iou: 0.7584 - binary_crossentropy: 0.0570 - val_loss: 0.4591 - val_mean_iou: 0.5753 - val_binary_crossentropy: 0.4591\n",
            "Epoch 173/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0516 - mean_iou: 0.7837 - binary_crossentropy: 0.0516 - val_loss: 0.4004 - val_mean_iou: 0.5828 - val_binary_crossentropy: 0.4004\n",
            "Epoch 174/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0530 - mean_iou: 0.7645 - binary_crossentropy: 0.0530 - val_loss: 0.4271 - val_mean_iou: 0.5907 - val_binary_crossentropy: 0.4271\n",
            "Epoch 175/20000\n",
            "25/25 [==============================] - 12s 492ms/step - loss: 0.0538 - mean_iou: 0.7629 - binary_crossentropy: 0.0538 - val_loss: 0.4200 - val_mean_iou: 0.5811 - val_binary_crossentropy: 0.4200\n",
            "Epoch 176/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0539 - mean_iou: 0.7704 - binary_crossentropy: 0.0539 - val_loss: 0.4662 - val_mean_iou: 0.5616 - val_binary_crossentropy: 0.4662\n",
            "Epoch 177/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0522 - mean_iou: 0.7776 - binary_crossentropy: 0.0522 - val_loss: 0.3933 - val_mean_iou: 0.5873 - val_binary_crossentropy: 0.3933\n",
            "Epoch 178/20000\n",
            "25/25 [==============================] - 12s 496ms/step - loss: 0.0542 - mean_iou: 0.7681 - binary_crossentropy: 0.0542 - val_loss: 0.4759 - val_mean_iou: 0.5693 - val_binary_crossentropy: 0.4759\n",
            "Epoch 179/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0546 - mean_iou: 0.7641 - binary_crossentropy: 0.0546 - val_loss: 0.3616 - val_mean_iou: 0.6004 - val_binary_crossentropy: 0.3616\n",
            "Epoch 180/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0563 - mean_iou: 0.7714 - binary_crossentropy: 0.0563 - val_loss: 0.3742 - val_mean_iou: 0.5938 - val_binary_crossentropy: 0.3742\n",
            "Epoch 181/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0547 - mean_iou: 0.7640 - binary_crossentropy: 0.0547 - val_loss: 0.4216 - val_mean_iou: 0.6112 - val_binary_crossentropy: 0.4216\n",
            "Epoch 182/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0558 - mean_iou: 0.7671 - binary_crossentropy: 0.0558 - val_loss: 0.4508 - val_mean_iou: 0.5657 - val_binary_crossentropy: 0.4508\n",
            "Epoch 183/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0562 - mean_iou: 0.7648 - binary_crossentropy: 0.0562 - val_loss: 0.3982 - val_mean_iou: 0.5941 - val_binary_crossentropy: 0.3982\n",
            "Epoch 184/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0586 - mean_iou: 0.7533 - binary_crossentropy: 0.0586 - val_loss: 0.3816 - val_mean_iou: 0.5938 - val_binary_crossentropy: 0.3816\n",
            "Epoch 185/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0541 - mean_iou: 0.7766 - binary_crossentropy: 0.0541 - val_loss: 0.4529 - val_mean_iou: 0.5791 - val_binary_crossentropy: 0.4529\n",
            "Epoch 186/20000\n",
            "25/25 [==============================] - 12s 496ms/step - loss: 0.0553 - mean_iou: 0.7686 - binary_crossentropy: 0.0553 - val_loss: 0.4299 - val_mean_iou: 0.5712 - val_binary_crossentropy: 0.4299\n",
            "Epoch 187/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0526 - mean_iou: 0.7628 - binary_crossentropy: 0.0526 - val_loss: 0.4389 - val_mean_iou: 0.5885 - val_binary_crossentropy: 0.4389\n",
            "Epoch 188/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0495 - mean_iou: 0.7760 - binary_crossentropy: 0.0495 - val_loss: 0.4086 - val_mean_iou: 0.6051 - val_binary_crossentropy: 0.4086\n",
            "Epoch 189/20000\n",
            "25/25 [==============================] - 12s 494ms/step - loss: 0.0517 - mean_iou: 0.7751 - binary_crossentropy: 0.0517 - val_loss: 0.4186 - val_mean_iou: 0.5744 - val_binary_crossentropy: 0.4186\n",
            "Epoch 190/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0515 - mean_iou: 0.7686 - binary_crossentropy: 0.0515 - val_loss: 0.4179 - val_mean_iou: 0.5690 - val_binary_crossentropy: 0.4179\n",
            "Epoch 191/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0538 - mean_iou: 0.7740 - binary_crossentropy: 0.0538 - val_loss: 0.4731 - val_mean_iou: 0.5744 - val_binary_crossentropy: 0.4731\n",
            "Epoch 192/20000\n",
            "25/25 [==============================] - 13s 532ms/step - loss: 0.0524 - mean_iou: 0.7696 - binary_crossentropy: 0.0524 - val_loss: 0.3812 - val_mean_iou: 0.5883 - val_binary_crossentropy: 0.3812\n",
            "Epoch 193/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0539 - mean_iou: 0.7648 - binary_crossentropy: 0.0539 - val_loss: 0.4298 - val_mean_iou: 0.5794 - val_binary_crossentropy: 0.4298\n",
            "Epoch 194/20000\n",
            "25/25 [==============================] - 12s 493ms/step - loss: 0.0814 - mean_iou: 0.7305 - binary_crossentropy: 0.0814 - val_loss: 0.4082 - val_mean_iou: 0.5327 - val_binary_crossentropy: 0.4082\n",
            "Epoch 195/20000\n",
            "25/25 [==============================] - 12s 496ms/step - loss: 0.0643 - mean_iou: 0.7455 - binary_crossentropy: 0.0643 - val_loss: 0.3859 - val_mean_iou: 0.5959 - val_binary_crossentropy: 0.3859\n",
            "Epoch 196/20000\n",
            "25/25 [==============================] - 12s 495ms/step - loss: 0.0554 - mean_iou: 0.7581 - binary_crossentropy: 0.0554 - val_loss: 0.3935 - val_mean_iou: 0.5807 - val_binary_crossentropy: 0.3935\n",
            "Epoch 197/20000\n",
            "25/25 [==============================] - 12s 490ms/step - loss: 0.0528 - mean_iou: 0.7718 - binary_crossentropy: 0.0528 - val_loss: 0.3319 - val_mean_iou: 0.5910 - val_binary_crossentropy: 0.3319\n",
            "Epoch 198/20000\n",
            "25/25 [==============================] - 12s 491ms/step - loss: 0.0522 - mean_iou: 0.7657 - binary_crossentropy: 0.0522 - val_loss: 0.4159 - val_mean_iou: 0.5772 - val_binary_crossentropy: 0.4159\n",
            "Epoch 199/20000\n",
            " 8/25 [========>.....................] - ETA: 8s - loss: 0.0503 - mean_iou: 0.7684 - binary_crossentropy: 0.0503 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f748f0fd8e37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-f748f0fd8e37>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(X_train, y_train, X_test, fold, fast)\u001b[0m\n\u001b[1;32m     75\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_data_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;31m#           callbacks=[early_stopping, model_checkpoint, reduce_lr],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#           callbacks=[early_stopping, reduce_lr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1487\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0;31m# Sample-wise fit loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_step_wise\u001b[0;34m(self, ins, callbacks, steps_per_epoch, epochs, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m           \u001b[0mnext_step_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         outs = f.pipeline_run(cur_step_inputs=ins,\n\u001b[0;32m-> 1631\u001b[0;31m                               next_step_inputs=next_step_inputs)\n\u001b[0m\u001b[1;32m   1632\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         logging.warning('Your dataset iterator ran out of data; '\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mpipeline_run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0mnext_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         ], infeed_dict)\n\u001b[0m\u001b[1;32m   1089\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uf9lolkBBkSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6fwJZzEBkQl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uk1SXFM3BkNT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yQXxqr0MAqoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tFBYhgB5NEjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "71db68f4-a00c-46db-c6d5-250cb187778d"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def test_model():\n",
        "  x = tf.keras.layers.Input(shape=(101,101,1), dtype=np.float)\n",
        "  y = tf.keras.layers.Conv2D(1,1, padding='same', activation='sigmoid', dtype=np.float)(x)\n",
        "  m = tf.keras.Model(x, y)\n",
        "  return m\n",
        "\n",
        "model = test_model()\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 101, 101, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 101, 101, 1)       2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4tHvN3g6PA92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "4d8bf063-37b5-4805-a9d5-452f8815d2af"
      },
      "cell_type": "code",
      "source": [
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "session_master = resolver.master()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6931466752683336584)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2319523528817134981)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2621229120846880910)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13017604127097533172)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13684817192547276644)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15939364587101623240)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6777068171437765544)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 677236183219516705)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 17055041862000479642)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13317328517731664289)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7926127133639364078)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9787759974722640898)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: grpc://10.240.1.2:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cXtj4ZEb85dv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIyUjH87qLvJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2068
        },
        "outputId": "ea6b8821-4837-4968-ea22-0ada27acd56a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def _set_shape(image, mask):\n",
        "  image.set_shape([101, 101, 1])\n",
        "  mask.set_shape([101, 101, 1])\n",
        "  return image, mask\n",
        "\n",
        "def input_fn():\n",
        "  images = tf.random_uniform([1024, 101, 101, 1])\n",
        "  masks = tf.random_uniform([1024, 101, 101, 1])\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
        "#   print(dataset)\n",
        "#   dataset = dataset.map(lambda image, mask: tuple(tf.py_func(random_h_flip, [image, mask], [tf.float32, tf.float32])))\n",
        "#   print(dataset)\n",
        "  dataset = dataset.map(_random_flip_left_right)\n",
        "#   print(dataset)\n",
        "  dataset = dataset.shuffle(1000).repeat(100000).batch(32, drop_remainder=True)\n",
        "#   print(dataset)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01),\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['binary_crossentropy'])\n",
        "\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    input_fn,\n",
        "    validation_data=input_fn,\n",
        "    validation_steps=100,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<MapDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<BatchDataset shapes: ((32, 101, 101, 1), (32, 101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<MapDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<BatchDataset shapes: ((32, 101, 101, 1), (32, 101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 11.059766054153442 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.6932 - binary_crossentropy: 0.6932INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 10.902057647705078 secs\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.6932 - binary_crossentropy: 0.6932 - val_loss: 0.6932 - val_binary_crossentropy: 0.6932\n",
            "Epoch 2/20\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.6932 - binary_crossentropy: 0.6932"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-292190155012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     epochs=20)\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1487\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0;31m# Sample-wise fit loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_step_wise\u001b[0;34m(self, ins, callbacks, steps_per_epoch, epochs, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                                            \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                                            \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                                            verbose=0)\n\u001b[0m\u001b[1;32m   1657\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    402\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1006\u001b[0m           \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m           \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       ], infeed_dict)\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6XdIwEcAqor5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2241
        },
        "outputId": "6f555f74-f366-4eb1-bd79-7687c211c77e"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef expected inputs 'float' do not match 2 inputs specified; Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\"](training/TFOptimizer/gradients/tpu_model_140369801514736/conv2d_1/Conv2D_grad/tuple/control_dependency_1, training/TFOptimizer/CrossReplicaSum/group_assignment)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-959ee3a79e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_sample_wise\u001b[0;34m(self, ins, callbacks, index_array, shuffle, batch_size, num_training_samples, indices_for_conversion_to_dense, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m       outs = f.pipeline_run(cur_step_inputs=ins_last_batch,\n\u001b[0;32m-> 1558\u001b[0;31m                             next_step_inputs=ins_batch)\n\u001b[0m\u001b[1;32m   1559\u001b[0m       \u001b[0mins_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mpipeline_run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1075\u001b[0m           next_input_tensors)\n\u001b[1;32m   1076\u001b[0m       next_tpu_model_ops = self._tpu_model_ops_for_input_specs(\n\u001b[0;32m-> 1077\u001b[0;31m           next_input_specs, next_step_infeed_manager)\n\u001b[0m\u001b[1;32m   1078\u001b[0m       \u001b[0minfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_infeed_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_tpu_model_ops_for_input_specs\u001b[0;34m(self, input_specs, infeed_manager)\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mshape_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_specs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m         logging.info('New input shapes; (re-)compiling: mode=%s, %s',\n\u001b[1;32m    915\u001b[0m                      self.execution_mode, input_specs)\n",
            "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mtpu_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[0;34m\"\"\"Yields a TPU session and sets it as the default Keras session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m       \u001b[0mdefault_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1812\u001b[0m       \u001b[0;31m# N.B. We have to call `K.set_session()` AND set our session as the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m       \u001b[0;31m# TF default. `K.get_session()` surprisingly does not return the value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 710\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef expected inputs 'float' do not match 2 inputs specified; Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\"](training/TFOptimizer/gradients/tpu_model_140369801514736/conv2d_1/Conv2D_grad/tuple/control_dependency_1, training/TFOptimizer/CrossReplicaSum/group_assignment)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ibenmWsXzMNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7fe904b0-88ba-4e00-f9a6-1dc9f27aecbc"
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"ResNet-50 implemented with Keras running on Cloud TPUs.\n",
        "This file shows how you can run ResNet-50 on a Cloud TPU using the TensorFlow\n",
        "Keras support. This is configured for ImageNet (e.g. 1000 classes), but you can\n",
        "easily adapt to your own datasets by changing the code appropriately.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import imagenet_input\n",
        "\n",
        "try:\n",
        "  import h5py as _  # pylint: disable=g-import-not-at-top\n",
        "  HAS_H5PY = True\n",
        "except ImportError:\n",
        "  logging.warning('`h5py` is not installed. Please consider installing it '\n",
        "                  'to save weights for long-running training.')\n",
        "  HAS_H5PY = False\n",
        "\n",
        "\n",
        "flags.DEFINE_bool('use_tpu', True, 'Use TPU model instead of CPU.')\n",
        "flags.DEFINE_string('tpu', None, 'Name of the TPU to use.')\n",
        "flags.DEFINE_string('data', None, 'Path to training and testing data.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "PER_CORE_BATCH_SIZE = 128\n",
        "NUM_CLASSES = 1000\n",
        "IMAGE_SIZE = 224\n",
        "EPOCHS = 90  # Standard imagenet training regime.\n",
        "APPROX_IMAGENET_TRAINING_IMAGES = 1280000  # Approximate number of images.\n",
        "APPROX_IMAGENET_TEST_IMAGES = 48000  # Approximate number of images.\n",
        "\n",
        "WEIGHTS_TXT = '/tmp/resnet50_weights.h5'\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "  logging.info('Building Keras ResNet-50 model.')\n",
        "  model = tf.keras.applications.resnet50.ResNet50(\n",
        "      include_top=True,\n",
        "      weights=None,\n",
        "      input_tensor=None,\n",
        "      input_shape=None,\n",
        "      pooling=None,\n",
        "      classes=NUM_CLASSES)\n",
        "\n",
        "  num_cores = 8\n",
        "  batch_size = PER_CORE_BATCH_SIZE * num_cores\n",
        "\n",
        "  if FLAGS.use_tpu:\n",
        "    logging.info('Converting from CPU to TPU model.')\n",
        "    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "    session_master = resolver.master()\n",
        "  else:\n",
        "    session_master = ''\n",
        "\n",
        "  logging.info('Compiling model.')\n",
        "  model.compile(\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "  if FLAGS.data is None:\n",
        "    training_images = np.random.randn(\n",
        "        batch_size, IMAGE_SIZE, IMAGE_SIZE, 3).astype(np.float32)\n",
        "    training_labels = np.random.randint(NUM_CLASSES, size=batch_size,\n",
        "                                        dtype=np.int32)\n",
        "    logging.info('Training model using synthetica data.')\n",
        "    model.fit(training_images, training_labels, epochs=EPOCHS,\n",
        "              batch_size=batch_size)\n",
        "    logging.info('Evaluating the model on synthetic data.')\n",
        "    model.evaluate(training_images, training_labels, verbose=0)\n",
        "  else:\n",
        "    imagenet_train, imagenet_eval = [imagenet_input.ImageNetInput(\n",
        "        is_training=is_training,\n",
        "        data_dir=FLAGS.data,\n",
        "        per_core_batch_size=PER_CORE_BATCH_SIZE)\n",
        "                                     for is_training in [True, False]]\n",
        "    logging.info('Training model using real data in directory \"%s\".',\n",
        "                 FLAGS.data)\n",
        "    model.fit(imagenet_train.input_fn,\n",
        "              epochs=EPOCHS,\n",
        "              steps_per_epoch=int(APPROX_IMAGENET_TRAINING_IMAGES / batch_size))\n",
        "\n",
        "    if HAS_H5PY:\n",
        "      logging.info('Save weights into %s', WEIGHTS_TXT)\n",
        "      model.save_weights(WEIGHTS_TXT, overwrite=True)\n",
        "\n",
        "    logging.info('Evaluating the model on the validation dataset.')\n",
        "    score = model.evaluate(\n",
        "        imagenet_eval.input_fn,\n",
        "        steps=int(APPROX_IMAGENET_TEST_IMAGES // batch_size),\n",
        "        verbose=1)\n",
        "    print('Evaluation score', score)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c62c05d6ad63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: No module named 'imagenet_input'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "r9DlwwbXq6o_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import dotenv\n",
        "# import keras\n",
        "import os\n",
        "dotenv.load_dotenv('.env')\n",
        "\n",
        "!mkdir model\n",
        "# !gsutil rsync gs://{os.environ['GCP_BUCKET']}/model model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoL4Y1yXqtaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5cb32500-ba58-4b15-c239-2fd8bf232040"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('raw/train.csv').set_index('id')\n",
        "test = pd.read_csv('raw/sample_submission.csv').set_index('id')\n",
        "depths = pd.read_csv('raw/depths.csv').set_index('id')\n",
        "\n",
        "train = pd.merge(train, depths, left_index=True, right_index=True, how='inner')\n",
        "train.shape, test.shape, depths.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 2), (18000, 1), (22000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "6TALjxylQlox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_size_target = 101"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6nC41yXq8j0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "73d65f68-7590-4b43-f4b5-f712fd457b61"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "\n",
        "train[\"image\"] = [np.array(load_img(\"./raw/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train.index)]\n",
        "train[\"mask\"] = [np.array(load_img(\"./raw/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train.index)]\n",
        "\n",
        "X_train = np.array(train['image'].tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
        "y_train = np.array(train['mask'].tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
        "\n",
        "X_test = np.array([np.array(load_img(\"./raw/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(test.index)]).reshape(-1, img_size_target, img_size_target, 1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "  0%|          | 0/4000 [00:00<?, ?it/s]/home/Hoiy/.local/lib/python3.5/site-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n",
            "100%|██████████| 4000/4000 [00:02<00:00, 1517.39it/s]\n",
            "100%|██████████| 4000/4000 [00:01<00:00, 3034.27it/s]\n",
            "100%|██████████| 18000/18000 [00:11<00:00, 1533.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ns-p0XzLKHQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16f71cf5-ebcf-44e8-9fa0-96bb485f9f53"
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32', copy=False)\n",
        "y_train = y_train.astype('float32', copy=False)\n",
        "\n",
        "X_test = X_test.astype('float32', copy=False)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 101, 101, 1), (4000, 101, 101, 1), (18000, 101, 101, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "EKDfbbaX_1WY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_config\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_estimator\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWtHOnEjhZs5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def sweep_thresholds_mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "def metric_fn(y_true, y_pred):\n",
        "  return {\n",
        "    'sweep_thresholds_mean_iou': sweep_thresholds_mean_iou(y_true, y_pred)\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIPW4QCxJEYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def host_call_fn(gs, loss, lr, ce):\n",
        "#   \"\"\"Training host call. Creates scalar summaries for training metrics.\n",
        "#   This function is executed on the CPU and should not directly reference\n",
        "#   any Tensors in the rest of the `model_fn`. To pass Tensors from the\n",
        "#   model to the `metric_fn`, provide as part of the `host_call`. See\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "#   for more information.\n",
        "#   Arguments should match the list of `Tensor` objects passed as the second\n",
        "#   element in the tuple passed to `host_call`.\n",
        "#   Args:\n",
        "#     gs: `Tensor with shape `[batch]` for the global_step\n",
        "#     loss: `Tensor` with shape `[batch]` for the training loss.\n",
        "#     lr: `Tensor` with shape `[batch]` for the learning_rate.\n",
        "#     ce: `Tensor` with shape `[batch]` for the current_epoch.\n",
        "#   Returns:\n",
        "#     List of summary ops to run on the CPU host.\n",
        "#   \"\"\"\n",
        "#   gs = gs[0]\n",
        "#   # Host call fns are executed FLAGS.iterations_per_loop times after one\n",
        "#   # TPU loop is finished, setting max_queue value to the same as number of\n",
        "#   # iterations will make the summary writer only flush the data to storage\n",
        "#   # once per loop.\n",
        "#   with summary.create_file_writer(\n",
        "#       FLAGS.model_dir, max_queue=FLAGS.iterations_per_loop).as_default():\n",
        "#     with summary.always_record_summaries():\n",
        "#       summary.scalar('loss', loss[0], step=gs)\n",
        "#       summary.scalar('learning_rate', lr[0], step=gs)\n",
        "#       summary.scalar('current_epoch', ce[0], step=gs)\n",
        "\n",
        "#       return summary.all_summary_ops()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sIa5GsA-_0rb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def test_model_fn(features, labels, mode, params):\n",
        "#     ###################################################################\n",
        "#     # The Keras LSTM layer that causes an error unless it is unrolled #\n",
        "#     ###################################################################\n",
        "#     predictions = tf.keras.layers.Conv2D(filters=1, kernel_size=1)(features)\n",
        "\n",
        "#     # Create ops for the TPUEstimatorSpec\n",
        "#     loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n",
        "#     optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
        "# #     if FLAGS.use_tpu:\n",
        "#     optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)\n",
        "  \n",
        "#     global_step = tf.train.get_global_step()\n",
        "#     train_op = optimizer.minimize(loss, global_step)\n",
        "\n",
        "#     # Return the TPUEstimatorSpec\n",
        "#     return tpu_estimator.TPUEstimatorSpec(\n",
        "#         mode=mode,\n",
        "#         loss=loss,\n",
        "#         train_op=train_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpUjJCzRBNUy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # https://github.com/tensorflow/tpu/blob/master/models/experimental/cifar_keras/cifar_keras.py\n",
        "\n",
        "# def model_fn(features, labels, mode, config, params):\n",
        "# #   print(features)\n",
        "#   v = tf.keras.layers.Input(tensor=features)\n",
        "#   pred = tf.keras.layers.Conv2D(\n",
        "#     filters=1,\n",
        "#     kernel_size=3,\n",
        "#     padding='same',\n",
        "#     activation='relu'\n",
        "#   )(v)\n",
        "  \n",
        "# #   print(pred)\n",
        "\n",
        "# #   pred=features\n",
        "  \n",
        "# #   if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "#     # Compute the current epoch and associated learning rate from global_step.\n",
        "#   global_step = tf.train.get_global_step()\n",
        "\n",
        "#   loss = tf.keras.losses.binary_crossentropy(labels, pred)  \n",
        "\n",
        "#   optimizer = tf.train.AdamOptimizer()\n",
        "#   optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "#   # Batch normalization requires UPDATE_OPS to be added as a dependency to\n",
        "#   # the train operation.\n",
        "#   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "#   with tf.control_dependencies(update_ops):\n",
        "#     train_op = optimizer.minimize(loss, global_step)\n",
        "      \n",
        "# #     model = tf.keras.Model(inputs=features, outputs=pred)\n",
        "    \n",
        "#   return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "#     mode=mode,\n",
        "#     loss=loss,\n",
        "#     train_op=train_op,\n",
        "#     host_call=None,\n",
        "#     eval_metrics=(metric_fn, [labels, pred]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oz4OuUomhxzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def main(unused_argv=None):\n",
        "#     tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver('kaggle-tpu')\n",
        "\n",
        "    \n",
        "#     config = tf.contrib.tpu.RunConfig(\n",
        "#       cluster=tpu_cluster_resolver,\n",
        "#       model_dir='gs://kaggle-195720-tgs-salt-identification-challenge/temp',\n",
        "# #       save_checkpoints_secs=3600,\n",
        "# #       session_config=tf.ConfigProto(\n",
        "# #           allow_soft_placement=True, log_device_placement=True),\n",
        "# #       tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "# #           iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "# #           num_shards=FLAGS.num_shards),\n",
        "#   )\n",
        "    \n",
        "    \n",
        "# #     config = tpu_config.RunConfig(\n",
        "# #         master=master_url,\n",
        "# #         model_dir='gs://kaggle-195720-tgs-salt-identification-challenge/temp')\n",
        "\n",
        "#     # Create the TPUEstimator\n",
        "#     estimator = tpu_estimator.TPUEstimator(\n",
        "#       use_tpu=True,\n",
        "#       model_fn=model_fn,\n",
        "#       config=config,\n",
        "#       train_batch_size=1024)\n",
        "\n",
        "#     # Train the estimator for 10 steps\n",
        "#     estimator.train(input_fn, max_steps=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXICpUNGitvo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def input_fn(params):\n",
        "#     dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "# #     dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(params['batch_size']))\n",
        "    \n",
        "#     print(dataset)\n",
        "\n",
        "#     # Make input_fn for the TPUEstimator train step\n",
        "# #     dataset_fn = dataset.make_one_shot_iterator().get_next()\n",
        "#     dataset = dataset.shuffle(1000).repeat().batch(params['batch_size'])\n",
        "    \n",
        "# #     dataset = dataset.prefetch(1)\n",
        "#     return dataset\n",
        "  \n",
        "  \n",
        "# # dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "# # dataset\n",
        "# # dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(128))\n",
        "\n",
        "# # print(dataset)\n",
        "\n",
        "# # # Make input_fn for the TPUEstimator train step\n",
        "# # dataset_fn = dataset.make_one_shot_iterator().get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFjqvXl4ijH9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myc4wE2NdCN-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.contrib import tpu\n",
        "# from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
        "\n",
        "# def axy_computation(a, x, y):\n",
        "#   return a * x + y\n",
        "\n",
        "# inputs = [\n",
        "#     3.0,\n",
        "#     tf.ones([3, 3], tf.float32),\n",
        "#     tf.ones([3, 3], tf.float32),\n",
        "# ]\n",
        "\n",
        "# tpu_computation = tpu.rewrite(axy_computation, inputs)\n",
        "\n",
        "# tpu_grpc_url = TPUClusterResolver(\n",
        "#     tpu=['kaggle-tpu']).get_master()\n",
        "\n",
        "# with tf.Session(tpu_grpc_url) as sess:\n",
        "#   sess.run(tpu.initialize_system())\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "#   output = sess.run(tpu_computation)\n",
        "#   print(output)\n",
        "#   sess.run(tpu.shutdown_system())\n",
        "\n",
        "# print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16XOGte5q_0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import keras\n",
        "# from keras import Model\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "# from keras.models import load_model\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, SpatialDropout2D, UpSampling2D\n",
        "\n",
        "# from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "# def dice_loss(y_true, y_pred):\n",
        "#     smooth = 1.\n",
        "#     y_true_f = K.flatten(y_true)\n",
        "#     y_pred_f = K.flatten(y_pred)\n",
        "#     intersection = y_true_f * y_pred_f\n",
        "#     score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "#     return 1. - score\n",
        "  \n",
        "  \n",
        "\n",
        "# # define iou or jaccard loss function\n",
        "# def jaccard_loss(y_true, y_pred):\n",
        "#     y_true = tf.reshape(y_true, [-1])\n",
        "#     y_pred = tf.reshape(y_pred, [-1])\n",
        "#     intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    \n",
        "#     epsilon = tf.keras.backend.epsilon()\n",
        "    \n",
        "#     score = (intersection + epsilon) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + epsilon)\n",
        "#     return 1 - score\n",
        "\n",
        "\n",
        "  \n",
        "# def iou_bce_loss(y_true, y_pred):\n",
        "#     return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.to_float(tf.greater(tf.round(y_true), y_pred)))\n",
        "    \n",
        "\n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     i = tf.round(y_pred) * tf.round(y_true)\n",
        "#     o = tf.round(y_pred) * tf.round(y_true)\n",
        "#     mean_iou = tf.metrics.mean_iou(y_true, tf.round(y_pred), 1)\n",
        "#     print(mean_iou)\n",
        "#     print(tf.tile(mean_iou, 10))\n",
        "#     print(mean_iou[0])\n",
        "#     return mean_iou[0]\n",
        "#     return tf.reduce_mean(tf.tile(mean_iou, 10))\n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     prec = []\n",
        "#     for t in np.arange(0.5, 1.0, 0.05):\n",
        "#         y_pred_ = tf.to_int32(y_pred > t)\n",
        "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         with tf.control_dependencies([up_opt]):\n",
        "#             score = tf.identity(score)\n",
        "#         prec.append(score)\n",
        "#     return K.mean(K.stack(prec), axis=0)\n",
        "\n",
        "\n",
        "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
        "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if activation == True:\n",
        "        x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "  \n",
        "def residual_block(blockInput, num_filters=16):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = convolution_block(x, num_filters, (3,3) )\n",
        "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
        "    x = Add()([x, blockInput])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "def conv_block(m, dim, acti, bn, res, do=0.5):\n",
        "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
        "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
        "    n = tf.keras.layers.SpatialDropout2D(do)(n) if do else n\n",
        "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
        "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
        "    n = tf.keras.layers.Concatenate()([m, n]) if res else n\n",
        "    return n\n",
        "\n",
        "  \n",
        "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "    if depth > 0:\n",
        "        n = conv_block(m, dim, acti, bn, res)\n",
        "        m = tf.keras.layers.MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "        m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
        "        if up:\n",
        "            m = tf.keras.layers.UpSampling2D()(m)\n",
        "            m = tf.keras.layers.Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "        else:\n",
        "#             padding = ['valid', 'same','same','valid','same','valid']\n",
        "            padding = ['same','same','valid','same','valid']\n",
        "            m = tf.keras.layers.Conv2DTranspose(dim, 3, strides=2, activation=acti, padding=padding[depth-1])(m)\n",
        "        n = tf.keras.layers.Concatenate()([n, m])\n",
        "        m = conv_block(n, dim, acti, bn, res)\n",
        "    else:\n",
        "        m = conv_block(m, dim, acti, bn, res, do=do)\n",
        "    return m\n",
        "\n",
        "  \n",
        "# def UNet(img_shape, out_ch=1, start_ch=32, depth=5, inc_rate=2., activation='relu', \n",
        "#          dropout=False, batchnorm=True, maxpool=True, upconv=False, residual=True):\n",
        "#     i = tf.keras.layers.Input(shape=img_shape)\n",
        "#     o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
        "#     o = tf.keras.layers.Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
        "#     return tf.keras.Model(inputs=i, outputs=o)\n",
        "\n",
        "def UNet(img_shape, out_ch=1, start_ch=32, depth=5, inc_rate=2., activation='relu', \n",
        "         dropout=False, batchnorm=True, maxpool=True, upconv=False, residual=True):\n",
        "    i = tf.keras.layers.Input(shape=img_shape)\n",
        "    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
        "    o = tf.keras.layers.Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
        "    return tf.keras.Model(inputs=i, outputs=o)\n",
        "  \n",
        "  \n",
        "\n",
        "# def ResNet():\n",
        "#   i = Input(shape=(img_size_target, img_size_target))\n",
        "  \n",
        "#   o = Conv2D(filters=1, kernel_size=1, activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NDV_LZ8Ernrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2206
        },
        "outputId": "0a09936c-c198-4b0a-e043-a3a0c3a6b204"
      },
      "cell_type": "code",
      "source": [
        "def build_train_generator(X_train, y_train, batch_size=32):\n",
        "    # Ref: https://keras.io/preprocessing/image/\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(\n",
        "      rotation_range=360.,\n",
        "#       width_shift_range=0.1,\n",
        "#       height_shift_range=0.1,\n",
        "      fill_mode='reflect',\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "    )\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
        "    seed = 1\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "    \n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "\n",
        "    # combine generators into one which yields image and masks\n",
        "    return zip(image_generator, mask_generator)\n",
        "\n",
        "\n",
        "def cv(X_train, y_train, X_test, fold, fast=False):\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    \n",
        "    loss = []\n",
        "    metric = []\n",
        "    pred_test = np.zeros(X_test.shape)\n",
        "    fold_index = 0\n",
        "    \n",
        "    start_ch = 32\n",
        "    depth = 5\n",
        "         \n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train):      \n",
        "#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\n",
        "      model = test_model()\n",
        "      resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "      strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "      model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "      session_master = resolver.master()\n",
        "      \n",
        "      model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"binary_crossentropy\"])\n",
        "\n",
        "#       early_stopping = EarlyStopping(patience=20, verbose=1)\n",
        "#       model_checkpoint = ModelCheckpoint(\"./model/keras_%d.model\"%fold_index, save_best_only=True, verbose=1)\n",
        "#       reduce_lr = ReduceLROnPlateau(factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
        "\n",
        "      epochs = 20000\n",
        "      batch_size = 32\n",
        "\n",
        "      history = model.fit_generator(\n",
        "          build_train_generator(X_train[train_index], y_train[train_index], batch_size),\n",
        "          validation_data=(X_train[test_index], y_train[test_index]),\n",
        "          steps_per_epoch=np.ceil(len(X_train[train_index]) / batch_size), \n",
        "          epochs=epochs,\n",
        "#           callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "      )\n",
        "      \n",
        "      best_idx = np.argmin(history.history['val_loss'])\n",
        "      val_loss = history.history['val_loss'][best_idx]\n",
        "      val_mean_iou = history.history['val_mean_iou'][best_idx]\n",
        "      \n",
        "      print(\"Best val_loss: %.4f, val_mean_iou: %.4f\"%(val_loss, val_mean_iou))\n",
        "      loss.append(val_loss)\n",
        "      metric.append(val_mean_iou)\n",
        "      \n",
        "      model = keras.models.load_model('model/keras_%d.model'%fold_index, custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss, 'dice_loss': dice_loss})\n",
        "      pred_test += model.predict(X_test) / fold\n",
        "      fold_index = fold_index + 1\n",
        "      \n",
        "      if fast:\n",
        "        break\n",
        "      \n",
        "      \n",
        "    print('%d fold val_loss: %.4f, val_mean_iou: %.4f'%(fold, np.mean(loss), np.mean(metric)))\n",
        "    return pred_test, np.mean(loss), np.mean(metric)\n",
        "\n",
        "  \n",
        "pred_test, val_loss, val_mean_iou = cv(X_train, y_train, X_test, 5, fast=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 13486222805198966859)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9984220072123666814)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 18162437541401842557)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9211313139234737039)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4136755912537517626)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6532054137011805886)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3825718477947054890)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5276807511303573055)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5719668420551969614)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10390436815882700867)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15818039368216332404)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6864323238587904500)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: metrics/mean_iou/ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_BOOL, Truncate=false, _tpu_replicate=\"cluster\"](metrics/mean_iou/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0dea82b24951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-0dea82b24951>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(X_train, y_train, X_test, fold, fast)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kaggle-tpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUDistributionStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_to_tpu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0msession_master\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/framework/python/framework/experimental.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m'any time, and without warning.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         decorator_utils.get_qualified_name(func), func.__module__)\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m   new_func.__doc__ = _add_experimental_function_notice_to_docstring(\n\u001b[1;32m     66\u001b[0m       func.__doc__)\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mtpu_model\u001b[0;34m(model, strategy)\u001b[0m\n\u001b[1;32m   1888\u001b[0m   \"\"\"\n\u001b[1;32m   1889\u001b[0m   \u001b[0;31m# Force initialization of the CPU model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m       \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   2706\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 710\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: metrics/mean_iou/ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_BOOL, Truncate=false, _tpu_replicate=\"cluster\"](metrics/mean_iou/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HRemgbrVOpjK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # summarize history for loss\n",
        "# plt.plot(history.history['loss'][1:])\n",
        "# plt.plot(history.history['val_loss'][1:])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train','Validation'], loc='upper left')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNqZwFOebfzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# best_idx = np.argmin(history.history['val_loss'])\n",
        "# val_loss = history.history['val_loss'][best_idx]\n",
        "# val_mean_iou = history.history['val_mean_iou'][best_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgBEjwKHCn1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('model/keras.model', custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EqepTCQa1djX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RLenc(img, order='F', format=True):\n",
        "    \"\"\"\n",
        "    img is binary mask image, shape (r,c)\n",
        "    order is down-then-right, i.e. Fortran\n",
        "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
        "\n",
        "    returns run length as an array or string (if format is True)\n",
        "    \"\"\"\n",
        "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
        "    runs = []  ## list of run lengths\n",
        "    r = 0  ## the current run length\n",
        "    pos = 1  ## count starts from 1 per WK\n",
        "    for c in bytes:\n",
        "        if (c == 0):\n",
        "            if r != 0:\n",
        "                runs.append((pos, r))\n",
        "                pos += r\n",
        "                r = 0\n",
        "            pos += 1\n",
        "        else:\n",
        "            r += 1\n",
        "\n",
        "    # if last run is unsaved (i.e. data ends with 1)\n",
        "    if r != 0:\n",
        "        runs.append((pos, r))\n",
        "        pos += r\n",
        "        r = 0\n",
        "\n",
        "    if format:\n",
        "        z = ''\n",
        "\n",
        "        for rr in runs:\n",
        "            z += '{} {} '.format(rr[0], rr[1])    \n",
        "        return z[:-1]\n",
        "    else:\n",
        "        return runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5wnyMSkd50M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class FasterRle(object):\n",
        "#     \"\"\"Perform RLE in paralell.\"\"\"\n",
        "\n",
        "#     def __init__(self, num_consumers=2):\n",
        "#         \"\"\"Initialize class.\"\"\"\n",
        "#         self._tasks = multiprocessing.JoinableQueue()\n",
        "#         self._results = multiprocessing.Queue()\n",
        "#         self._n_consumers = num_consumers\n",
        "\n",
        "#         # Initialize consumers\n",
        "#         self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n",
        "#         for w in self._consumers:\n",
        "#             w.start()\n",
        "\n",
        "#     def add(self, array, startIndex):\n",
        "#         \"\"\"Add a task to perform.\"\"\"\n",
        "#         self._tasks.put(FasterTask(array, startIndex))\n",
        "\n",
        "#     def get_results(self):\n",
        "#         \"\"\"Close all tasks.\"\"\"\n",
        "#         # Provide poison pill\n",
        "#         [self._tasks.put(None) for _ in range(self._n_consumers)]\n",
        "#         # Wait for finish\n",
        "#         self._tasks.join()\n",
        "#         # Return results\n",
        "#         singles = []\n",
        "#         while not self._results.empty():\n",
        "#             singles.append(self._results.get())\n",
        "            \n",
        "#         resultDic = dict()\n",
        "#         for rles, start in singles:\n",
        "#             #print('start:', start)\n",
        "#             for i,rle in enumerate(rles):\n",
        "#                 #print('i:', i)\n",
        "#                 resultDic[str(start+i)] = rle\n",
        "#         return resultDic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7wRK-ZUEmzA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch(func):\n",
        "  def batch_func(b):\n",
        "    return np.array([func(e) for e in tqdm(b)])\n",
        "  return batch_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sy0eHi4-P5PP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preds_test = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hIcfQ4_pPlHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e52d1ce9-b9b2-4fdf-f039-e606626a790a"
      },
      "cell_type": "code",
      "source": [
        "test['rle_mask'] = batch(RLenc)(np.round(pred_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18000/18000 [00:58<00:00, 309.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ST2JPIMLP666",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7aaf49b7-1c59-4efc-cfb8-2bdd7e3a885f"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_FILE = 'keras_unet_sc32de5wbecjaccloss_%.4f_%.4f.csv'%(val_loss, val_mean_iou)\n",
        "test.to_csv(SUBMISSION_FILE)\n",
        "SUBMISSION_FILE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'keras_unet_sc32de5wbecjaccloss_0.1320_0.7477.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "7iJCa8ZqRppy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cc737439-5883-4295-c670-658d69aaff48"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_MESSAGE='\"Keras U-net iou_bce_loss: val_loss: %.4f, val_mean_iou: %.4f\"'%(val_loss, val_mean_iou)\n",
        "!kaggle competitions submit -f '{SUBMISSION_FILE}' -m '{SUBMISSION_MESSAGE}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using competition: tgs-salt-identification-challenge\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.4.6)\n",
            "Successfully submitted to TGS Salt Identification Challenge"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ql1uneNwTuEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "c9be94ee-85ec-4429-d351-98436cfaf45f"
      },
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv('.env')\n",
        "!gsutil rsync model gs://{os.environ['GCP_BUCKET']}/model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "Copying file://model/keras_0.model [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "Copying file://model/keras_1.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_2.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_3.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m -o ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying file://model/keras_4.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "Operation completed over 5 objects/751.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6UwBwalwuZpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "bad2519d-f469-4e15-ce5c-1a921ed92c84"
      },
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# def batch_iou(pred, truth):\n",
        "#     pred_ = tf.round(pred)\n",
        "#     truth_ = truth\n",
        "    \n",
        "#     i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "#     u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "#     epsilon = tf.keras.backend.epsilon()\n",
        "#     return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "  \n",
        "# def mean_iou(pred, truth):\n",
        "#     ious = batch_iou(pred, truth)\n",
        "#     ious = tf.reshape(ious, (-1,1))\n",
        "#     ious = tf.tile(ious, (1,10))\n",
        "#     thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "#     return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "# BATCH_SIZE = 100\n",
        "# ones = tf.constant(np.ones((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# zeros = tf.constant(np.zeros((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# half = tf.coonstant(np.zeros(BATCH_SIZE, ))\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "  \n",
        "#   print(mean_iou(ones, zeros).eval())\n",
        "#   print(mean_iou(ones, ones).eval())\n",
        "#   print(mean_iou(zeros, ones).eval())\n",
        "#   print(mean_iou(zeros, zeros).eval())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1AayNFWou9ms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2345
        },
        "outputId": "b0ad8070-0ad3-4ddc-d926-aae031e16852"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Standard Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 12\n",
        "\n",
        "# input image dimensions\n",
        "IMG_ROWS, IMG_COLS = 28, 28\n",
        "\n",
        "use_tpu=True\n",
        "\n",
        "\n",
        "def mnist_model(input_shape):\n",
        "  \"\"\"Creates a MNIST model.\"\"\"\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(\n",
        "      tf.keras.layers.Conv2D(\n",
        "          32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(tf.keras.layers.Dropout(0.25))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "\n",
        "def main(unused_dev):\n",
        "  use_tpu = True\n",
        "\n",
        "  print('Mode:', 'TPU' if use_tpu else 'CPU')\n",
        "\n",
        "  if True:\n",
        "    print('Using fake data')\n",
        "    x_train = np.random.random((128, IMG_ROWS, IMG_COLS))\n",
        "    y_train = np.zeros([128, 1], dtype=np.int32)\n",
        "    x_test, y_test = x_train, y_train\n",
        "  else:\n",
        "    # the data, split between train and test sets\n",
        "    print('Using real data')\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "  x_train = x_train.reshape(x_train.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
        "  input_shape = (IMG_ROWS, IMG_COLS, 1)\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  print('x_train shape:', x_train.shape)\n",
        "  print(x_train.shape[0], 'train samples')\n",
        "  print(x_test.shape[0], 'test samples')\n",
        "\n",
        "  # convert class vectors to binary class matrices\n",
        "  y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "  y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "  model = mnist_model(input_shape)\n",
        "\n",
        "  if use_tpu:\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "    )\n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05),\n",
        "      metrics=['accuracy'])\n",
        "\n",
        "  model.fit(\n",
        "      x_train,\n",
        "      y_train,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      epochs=EPOCHS,\n",
        "      verbose=1,\n",
        "      validation_data=(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Loss for final step:', score[0])\n",
        "  print('Accuracy ', score[1])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mode: TPU\n",
            "Using fake data\n",
            "x_train shape: (128, 28, 28, 1)\n",
            "128 train samples\n",
            "128 test samples\n",
            "INFO:tensorflow:Detected old TPUStrategy API.\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 128 samples, validate on 128 samples\n",
            "Epoch 1/12\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(16, 28, 28, 1), dtype=tf.float32, name='conv2d_4_input0'), TensorSpec(shape=(16, 10), dtype=tf.float32, name='dense_5_target0')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_4_input\n",
            "INFO:tensorflow:Started compiling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1260\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'group_assignment' not in Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\", group_assignment=[]](training/TFOptimizer/gradients/tpu_model_139793487692408/conv2d_4/Conv2D_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e8ddd031b353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-e8ddd031b353>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_dev)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1041\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    852\u001b[0m                                                    infeed_manager)\n\u001b[1;32m    853\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_model_compiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;31m# Initialize our TPU weights on the first compile.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_test_model_compiles\u001b[0;34m(self, tpu_model_ops)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m     \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpu_compilation_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompilationResultProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 668\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'group_assignment' not in Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\", group_assignment=[]](training/TFOptimizer/gradients/tpu_model_139793487692408/conv2d_4/Conv2D_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "R3b3873xfUM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}