{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_tpu_unet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RgW68azy-I2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f00137f9-9663-4c4d-9994-64a687796464"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import dotenv\n",
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "\n",
        "dotenv.load_dotenv('.env')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "tQMYWUFxAqcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb718628-db33-4cb4-b8b4-fd29e23eb5cd"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('raw/train.csv').set_index('id')\n",
        "test = pd.read_csv('raw/sample_submission.csv').set_index('id')\n",
        "depths = pd.read_csv('raw/depths.csv').set_index('id')\n",
        "\n",
        "train = pd.merge(train, depths, left_index=True, right_index=True, how='inner')\n",
        "train.shape, test.shape, depths.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 2), (18000, 1), (22000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "RWGJQRBXAqxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Whe2VDUGAquf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "20386569-f800-462a-830b-fa146311f892"
      },
      "cell_type": "code",
      "source": [
        "X_train = [cv2.resize(cv2.imread(\"./raw/images/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(train.index)]\n",
        "X_train = np.expand_dims(np.array(X_train), -1)\n",
        "X_train = X_train / 255\n",
        "X_train = X_train.astype(np.float32)\n",
        "\n",
        "y_train = [cv2.resize(cv2.imread(\"./raw/masks/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(train.index)]\n",
        "y_train = np.expand_dims(np.array(y_train), -1)\n",
        "y_train = y_train / 255\n",
        "y_train = y_train.astype(np.float32)\n",
        "\n",
        "\n",
        "X_test = [cv2.resize(cv2.imread(\"./raw/images/{}.png\".format(idx), cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for idx in tqdm(test.index)]\n",
        "X_test = np.expand_dims(np.array(X_test), -1)\n",
        "X_test = X_test / 255\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4000/4000 [00:01<00:00, 2751.92it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 9316.63it/s]\n",
            "100%|██████████| 18000/18000 [00:06<00:00, 2756.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 128, 128, 1), (4000, 128, 128, 1), (18000, 128, 128, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "9D2pdhBiAqrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4042
        },
        "outputId": "d13e9aca-52e5-486d-b284-dda0e3c3b7b8"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "# from keras import Model\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "# from keras.models import load_model\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, SpatialDropout2D, UpSampling2D, Activation, Add\n",
        "\n",
        "# from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return 1. - score\n",
        "  \n",
        "  \n",
        "\n",
        "# define iou or jaccard loss function\n",
        "def jaccard_loss(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    \n",
        "    score = (intersection + epsilon) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + epsilon)\n",
        "    return 1 - score\n",
        "\n",
        "\n",
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    gts = tf.reduce_sum(gt_sorted)\n",
        "    intersection = gts - tf.cumsum(gt_sorted)\n",
        "    union = gts + tf.cumsum(1. - gt_sorted)\n",
        "    jaccard = 1. - intersection / union\n",
        "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
        "    return jaccard\n",
        "  \n",
        "  \n",
        "  \n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        def treat_image(log_lab):\n",
        "            log, lab = log_lab\n",
        "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
        "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
        "            return lovasz_hinge_flat(log, lab)\n",
        "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
        "        loss = tf.reduce_mean(losses)\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_loss():\n",
        "        labelsf = tf.cast(labels, logits.dtype)\n",
        "        signs = 2. * labelsf - 1.\n",
        "        errors = 1. - logits * tf.stop_gradient(signs)\n",
        "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
        "        gt_sorted = tf.gather(labelsf, perm)\n",
        "        grad = lovasz_grad(gt_sorted)\n",
        "        #loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        loss = tf.tensordot(tf.nn.elu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        return loss\n",
        "\n",
        "    # deal with the void prediction case (only void pixels)\n",
        "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
        "                   lambda: tf.reduce_sum(logits) * 0.,\n",
        "                   compute_loss,\n",
        "                   strict=True,\n",
        "                   name=\"loss\"\n",
        "                   )\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = tf.reshape(scores, (-1,))\n",
        "    labels = tf.reshape(labels, (-1,))\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = tf.not_equal(labels, ignore)\n",
        "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
        "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
        "    return vscores, vlabels\n",
        "\n",
        "def lovasz_loss(y_true, y_pred):\n",
        "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
        "    #logits = K.log(y_pred / (1. - y_pred))\n",
        "    logits = y_pred #Jiaxin\n",
        "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
        "    return loss\n",
        "  \n",
        "\n",
        "# def iou_bce_loss(y_true, y_pred):\n",
        "#     return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.to_float(tf.greater(tf.round(y_true), y_pred)))\n",
        "    \n",
        "\n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     i = tf.round(y_pred) * tf.round(y_true)\n",
        "#     o = tf.round(y_pred) * tf.round(y_true)\n",
        "#     mean_iou = tf.metrics.mean_iou(y_true, tf.round(y_pred), 1)\n",
        "#     print(mean_iou)\n",
        "#     print(tf.tile(mean_iou, 10))\n",
        "#     print(mean_iou[0])\n",
        "#     return mean_iou[0]\n",
        "#     return tf.reduce_mean(tf.tile(mean_iou, 10))\n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     prec = []\n",
        "#     for t in np.arange(0.5, 1.0, 0.05):\n",
        "#         y_pred_ = tf.to_int32(y_pred > t)\n",
        "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         with tf.control_dependencies([up_opt]):\n",
        "#             score = tf.identity(score)\n",
        "#         prec.append(score)\n",
        "#     return K.mean(K.stack(prec), axis=0)\n",
        "\n",
        "\n",
        "# def convolution_block(x, filters, size):\n",
        "#     return x\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# def residual_block(blockInput, filters):\n",
        "#     x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(blockInput)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     x = Conv2D(filters, 3, strides=(1,1), padding='same', activation=None)(x)\n",
        "#     x = Add()([x, blockInput])\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     return x\n",
        "  \n",
        "  \n",
        "# def res_block(i, filters=16):\n",
        "#     n1 = Conv2D(filters, 1, activation='relu', padding='same')(i)\n",
        "#     n3 = Conv2D(filters, 3, activation='relu', padding='same')(i)\n",
        "#     n5 = Conv2D(filters, 5, activation='relu', padding='same')(i)\n",
        "#     n = Concatenate()([n1, n3, n5])\n",
        "#     n = BatchNormalization()(n)\n",
        "    \n",
        "#     n1 = Conv2D(filters, 1, activation='relu', padding='same')(n)\n",
        "#     n3 = Conv2D(filters, 3, activation='relu', padding='same')(n)\n",
        "#     n5 = Conv2D(filters, 5, activation='relu', padding='same')(n)\n",
        "#     n = Concatenate()([n1, n3, n5])\n",
        "#     n = BatchNormalization()(n)\n",
        "    \n",
        "# #     n = Conv2D(1, 1, activation='relu', padding='same')(n)\n",
        "# #     n = Concatenate()([i, n])\n",
        "# #     n = BatchNormalization()(n)\n",
        "#     return n\n",
        "\n",
        "  \n",
        "# def level_block(inp, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "#     if depth > 0:\n",
        "#         n = conv_block(m, dim, acti, bn, res)\n",
        "#         m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "#         m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
        "#         if up:\n",
        "#             m = UpSampling2D()(m)\n",
        "#             m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "#         else:\n",
        "# #             padding = ['valid', 'same','same','valid','same','valid']\n",
        "#             padding = ['same','same','valid','same','valid']\n",
        "#             m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding=padding[depth-1])(m)\n",
        "#         n = Concatenate()([n, m])\n",
        "#         m = conv_block(n, dim, acti, bn, res)\n",
        "#     else:\n",
        "#         m = conv_block(m, dim, acti, bn, res, do=do)\n",
        "#     return m\n",
        "\n",
        "  \n",
        "# def ResNet():\n",
        "#     filter_size = 8\n",
        "#     i = Input(shape=(img_size_target,img_size_target,1))\n",
        "#     o = Conv2D(filter_size, 1, strides=(1,1), padding='same', activation='relu')(i)\n",
        "#     for depth in tqdm(range(128)):\n",
        "#       o = residual_block(o, filter_size)\n",
        "#     o = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(o)\n",
        "#     return Model(inputs=i, outputs=o)\n",
        "  \n",
        "  \n",
        "# def ResNet_2():\n",
        "#     resnet = keras.applications.resnet50.ResNet50(include_top=False, pooling=None, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "#     o = Conv2D(1, 1, strides=(1,1), activation='sigmoid')(resnet.output)\n",
        "#     return Model(inputs=resnet.input, outputs=o)\n",
        "  \n",
        "def res_block(inputs, filters):\n",
        "    inputs = BatchNormalization()(inputs)\n",
        "    conv = Conv2D(filters, 3, strides=1, padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = Conv2D(filters, 3, strides=1, padding='same', activation='relu')(conv)\n",
        "    \n",
        "    conv = Add()([conv, inputs])\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    return Conv2D(filters * 2, 3, strides=2, padding='same', activation='relu')(conv)\n",
        "  \n",
        "\n",
        "def res_block_r(inputs, filters, res):\n",
        "    inputs = Add()([inputs, res])\n",
        "    inputs = BatchNormalization()(inputs)\n",
        "    conv = Conv2DTranspose(filters, 3, strides=1, padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = Conv2DTranspose(filters, 3, strides=1, padding='same', activation='relu')(conv)\n",
        "    \n",
        "    print(conv, inputs)\n",
        "    conv = Add()([conv, inputs])\n",
        "    \n",
        "    conv = BatchNormalization()(conv)\n",
        "    return Conv2DTranspose(filters // 2, 3, strides=2, padding='same', activation='relu')(conv)\n",
        "  \n",
        "  \n",
        "# def build_model(start_channels=8, depth=6):\n",
        "#     inp = Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "#     l = BatchNormalization()(inp)\n",
        "#     l = Conv2D(start_channels, 3, strides=1, padding='same', activation='relu')(l)\n",
        "    \n",
        "#     layers = [l]\n",
        "    \n",
        "#     for i in range(depth):\n",
        "#       layers.append(res_block(layers[-1], start_channels * (2 ** i)))\n",
        "      \n",
        "      \n",
        "#     for i in range(depth):\n",
        "#       layers.append(res_block_r(layers[-1], start_channels * (2 ** (depth - 1 - i)), layers[-2*i-1]))\n",
        "      \n",
        "#     l = BatchNormalization()(layers[-1])\n",
        "#     out = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(l)\n",
        "    \n",
        "#     return Model(inputs=inp, outputs=out)\n",
        "\n",
        "def UNet(inputs, channels, depth):\n",
        "    layers = [inputs]\n",
        "    \n",
        "    for i in range(depth):\n",
        "      layers.append(res_block(layers[-1], channels))\n",
        "      print(channels)\n",
        "      channels = channels * 2\n",
        "      \n",
        "    for i in range(depth):      \n",
        "      print(channels)\n",
        "      layers.append(res_block_r(layers[-1], channels, layers[-2*i-1]))\n",
        "      channels = channels // 2\n",
        "      \n",
        "      \n",
        "    \n",
        "    return layers[-1]\n",
        "    \n",
        "    \n",
        "\n",
        "def build_model(channels=8, depth=6):\n",
        "    inp = keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "    l = BatchNormalization()(inp)\n",
        "    l = Conv2D(channels, 3, strides=1, padding='same', activation='relu')(l)\n",
        "    \n",
        "    l = UNet(l, channels, depth)\n",
        "#     l = UNet(l, channels, depth)\n",
        "    \n",
        "    l = BatchNormalization()(l)\n",
        "    out = Conv2D(1, 1, strides=(1,1), padding='same', activation='sigmoid')(l)\n",
        "    \n",
        "    return keras.Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "  \n",
        "build_model().summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "16\n",
            "32\n",
            "64\n",
            "128\n",
            "256\n",
            "512\n",
            "Tensor(\"conv2d_transpose_8/Relu:0\", shape=(?, 2, 2, 512), dtype=float32) Tensor(\"batch_normalization_120/cond/Merge:0\", shape=(?, 2, 2, 512), dtype=float32)\n",
            "256\n",
            "Tensor(\"conv2d_transpose_11/Relu:0\", shape=(?, 4, 4, 256), dtype=float32) Tensor(\"batch_normalization_123/cond/Merge:0\", shape=(?, 4, 4, 256), dtype=float32)\n",
            "128\n",
            "Tensor(\"conv2d_transpose_14/Relu:0\", shape=(?, 8, 8, 128), dtype=float32) Tensor(\"batch_normalization_126/cond/Merge:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
            "64\n",
            "Tensor(\"conv2d_transpose_17/Relu:0\", shape=(?, 16, 16, 64), dtype=float32) Tensor(\"batch_normalization_129/cond/Merge:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
            "32\n",
            "Tensor(\"conv2d_transpose_20/Relu:0\", shape=(?, 32, 32, 32), dtype=float32) Tensor(\"batch_normalization_132/cond/Merge:0\", shape=(?, 32, 32, 32), dtype=float32)\n",
            "16\n",
            "Tensor(\"conv2d_transpose_23/Relu:0\", shape=(?, 64, 64, 16), dtype=float32) Tensor(\"batch_normalization_135/cond/Merge:0\", shape=(?, 64, 64, 16), dtype=float32)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            (None, 128, 128, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 128, 128, 1)  4           input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 128, 128, 8)  80          batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 128, 128, 8)  32          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 128, 128, 8)  584         batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 128, 128, 8)  32          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 128, 128, 8)  584         batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 128, 128, 8)  0           conv2d_96[0][0]                  \n",
            "                                                                 batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 128, 128, 8)  32          add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 64, 64, 16)   1168        batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 64, 64, 16)   64          conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 64, 64, 16)   2320        batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 64, 64, 16)   64          conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 64, 64, 16)   2320        batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 64, 64, 16)   0           conv2d_99[0][0]                  \n",
            "                                                                 batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 64, 64, 16)   64          add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 32, 32, 32)   4640        batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 32, 32, 32)   128         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 32, 32, 32)   9248        batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 32, 32, 32)   128         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 32, 32, 32)   9248        batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 32, 32, 32)   0           conv2d_102[0][0]                 \n",
            "                                                                 batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 32, 32, 32)   128         add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 16, 16, 64)   18496       batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 16, 16, 64)   256         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 16, 16, 64)   36928       batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 16, 16, 64)   256         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 16, 16, 64)   36928       batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 16, 16, 64)   0           conv2d_105[0][0]                 \n",
            "                                                                 batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 16, 16, 64)   256         add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 8, 8, 128)    73856       batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 8, 8, 128)    512         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 8, 8, 128)    147584      batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 8, 8, 128)    512         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 8, 8, 128)    147584      batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 8, 8, 128)    0           conv2d_108[0][0]                 \n",
            "                                                                 batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 8, 8, 128)    512         add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 4, 4, 256)    295168      batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 4, 4, 256)    1024        conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 4, 4, 256)    590080      batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 4, 4, 256)    1024        conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 4, 4, 256)    590080      batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 4, 4, 256)    0           conv2d_111[0][0]                 \n",
            "                                                                 batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 4, 4, 256)    1024        add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 2, 2, 512)    1180160     batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 2, 2, 512)    0           conv2d_112[0][0]                 \n",
            "                                                                 conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 2, 2, 512)    2048        add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 2, 2, 512)    2359808     batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 2, 2, 512)    2048        conv2d_transpose_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 2, 2, 512)    2359808     batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 2, 2, 512)    0           conv2d_transpose_8[0][0]         \n",
            "                                                                 batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 2, 2, 512)    2048        add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTrans (None, 4, 4, 256)    1179904     batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 4, 4, 256)    0           conv2d_transpose_9[0][0]         \n",
            "                                                                 conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 4, 4, 256)    1024        add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DTran (None, 4, 4, 256)    590080      batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 4, 4, 256)    1024        conv2d_transpose_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_11 (Conv2DTran (None, 4, 4, 256)    590080      batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 4, 4, 256)    0           conv2d_transpose_11[0][0]        \n",
            "                                                                 batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 4, 4, 256)    1024        add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DTran (None, 8, 8, 128)    295040      batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 8, 8, 128)    0           conv2d_transpose_12[0][0]        \n",
            "                                                                 conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 8, 8, 128)    512         add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DTran (None, 8, 8, 128)    147584      batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 8, 8, 128)    512         conv2d_transpose_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_14 (Conv2DTran (None, 8, 8, 128)    147584      batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 8, 8, 128)    0           conv2d_transpose_14[0][0]        \n",
            "                                                                 batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 8, 8, 128)    512         add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_15 (Conv2DTran (None, 16, 16, 64)   73792       batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_15[0][0]        \n",
            "                                                                 conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 16, 16, 64)   256         add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_16 (Conv2DTran (None, 16, 16, 64)   36928       batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 16, 16, 64)   256         conv2d_transpose_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_17 (Conv2DTran (None, 16, 16, 64)   36928       batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 16, 16, 64)   0           conv2d_transpose_17[0][0]        \n",
            "                                                                 batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 16, 16, 64)   256         add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_18 (Conv2DTran (None, 32, 32, 32)   18464       batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 32, 32, 32)   0           conv2d_transpose_18[0][0]        \n",
            "                                                                 conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 32, 32, 32)   128         add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_19 (Conv2DTran (None, 32, 32, 32)   9248        batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 32, 32, 32)   128         conv2d_transpose_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_20 (Conv2DTran (None, 32, 32, 32)   9248        batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 32, 32, 32)   0           conv2d_transpose_20[0][0]        \n",
            "                                                                 batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 32, 32, 32)   128         add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_21 (Conv2DTran (None, 64, 64, 16)   4624        batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 64, 64, 16)   0           conv2d_transpose_21[0][0]        \n",
            "                                                                 conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 64, 64, 16)   64          add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_22 (Conv2DTran (None, 64, 64, 16)   2320        batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 64, 64, 16)   64          conv2d_transpose_22[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_23 (Conv2DTran (None, 64, 64, 16)   2320        batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 64, 64, 16)   0           conv2d_transpose_23[0][0]        \n",
            "                                                                 batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 64, 64, 16)   64          add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_24 (Conv2DTran (None, 128, 128, 8)  1160        batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 128, 128, 8)  32          conv2d_transpose_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 128, 128, 1)  9           batch_normalization_138[0][0]    \n",
            "==================================================================================================\n",
            "Total params: 11,030,165\n",
            "Trainable params: 11,021,075\n",
            "Non-trainable params: 9,090\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q2kwPCzH776M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from albumentations import (\n",
        "    PadIfNeeded,\n",
        "    HorizontalFlip,\n",
        "    VerticalFlip,    \n",
        "    CenterCrop,    \n",
        "    Crop,\n",
        "    Compose,\n",
        "    Transpose,\n",
        "    RandomRotate90,\n",
        "    ElasticTransform,\n",
        "    GridDistortion, \n",
        "    OpticalDistortion,\n",
        "    RandomSizedCrop,\n",
        "    OneOf,\n",
        "    CLAHE,\n",
        "    RandomContrast,\n",
        "    RandomGamma,\n",
        "    RandomBrightness\n",
        ")\n",
        "\n",
        "def random_h_flip(image, mask):\n",
        "  aug = HorizontalFlip(p=0.5)\n",
        "  augmented = aug(image=image, mask=mask)\n",
        "  return augmented['image'], augmented['mask']\n",
        "\n",
        "\n",
        "def random_v_flip(image, mask):\n",
        "  aug = VerticalFlip(p=0.5)\n",
        "  augmented = aug(image=image, mask=mask)\n",
        "  return augmented['image'], augmented['mask']\n",
        "\n",
        "\n",
        "def random_rotate(image, mask):\n",
        "  aug = RandomRotate90(p=1)\n",
        "  augmented = aug(image=image, mask=mask)\n",
        "  return augmented['image'], augmented['mask']\n",
        "\n",
        "\n",
        "def random_aug(image, mask):\n",
        "  image, mask = random_h_flip(image, mask)\n",
        "  image, mask = random_v_flip(image, mask)\n",
        "  image, mask = random_rotate(image, mask)\n",
        "  return image, mask\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def _random_flip_left_right(image, mask):\n",
        "  image = tf.image.random_flip_left_right(image, seed=SEED)\n",
        "  mask = tf.image.random_flip_left_right(mask, seed=SEED)\n",
        "  return image, mask\n",
        "\n",
        "def _random_flip_up_down(image, mask):\n",
        "  image = tf.image.random_flip_up_down(image, seed=SEED)\n",
        "  mask = tf.image.random_flip_up_down(mask, seed=SEED)\n",
        "  return image, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PjE-J85UBkVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6939
        },
        "outputId": "4c314a5f-e1a9-48b3-f68e-85593e9b4f73"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def build_train_generator(X_train, y_train, batch_size=32):\n",
        "    # Ref: https://keras.io/preprocessing/image/\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(\n",
        "#       rotation_range=360.,\n",
        "#       width_shift_range=0.1,\n",
        "#       height_shift_range=0.1,\n",
        "      fill_mode='reflect',\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "    )\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
        "    seed = 1\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "    \n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "\n",
        "    # combine generators into one which yields image and masks\n",
        "    return zip(image_generator, mask_generator)\n",
        "\n",
        "  \n",
        "\n",
        "def cv(X_train, y_train, X_test, fold, fast=False):\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    \n",
        "    loss = []\n",
        "    metric = []\n",
        "    pred_test = np.zeros(X_test.shape)\n",
        "    fold_index = 0\n",
        "    \n",
        "    start_ch = 32\n",
        "    depth = 5\n",
        "         \n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train):      \n",
        "#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\n",
        "      model = build_model()\n",
        "  \n",
        "      resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "      strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "      model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "      session_master = resolver.master()\n",
        "  \n",
        "      model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[mean_iou, \"binary_crossentropy\"])\n",
        "    \n",
        "      EPOCHS = 20000\n",
        "      BATCH_SIZE = 64\n",
        "\n",
        "      def create_data_fn(index, training=False):\n",
        "        def data_fn():\n",
        "          dataset = tf.data.Dataset.from_tensor_slices((X_train[index], y_train[index]))\n",
        "          if training:\n",
        "            dataset = dataset.map(_random_flip_left_right).map(_random_flip_up_down)\n",
        "          dataset = dataset.shuffle(len(X_train[index])).repeat(EPOCHS).batch(BATCH_SIZE, drop_remainder=True)\n",
        "          return dataset\n",
        "        return data_fn\n",
        "          \n",
        "\n",
        "      early_stopping = EarlyStopping(patience=10, verbose=1)\n",
        "      model_checkpoint = ModelCheckpoint(\"./model/keras_%d.model\"%fold_index, save_best_only=True, verbose=1)\n",
        "      reduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "      history = model.fit(\n",
        "          create_data_fn(train_index, training=True),\n",
        "          steps_per_epoch=int(np.ceil(len(X_train[train_index]) / BATCH_SIZE)),\n",
        "          validation_data=create_data_fn(test_index, training=False),\n",
        "          validation_steps=int(np.ceil(len(X_train[test_index]) / BATCH_SIZE)),\n",
        "          epochs=EPOCHS\n",
        "#           callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
        "#           callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "      \n",
        "      best_idx = np.argmin(history.history['val_loss'])\n",
        "      val_loss = history.history['val_loss'][best_idx]\n",
        "      val_mean_iou = history.history['val_mean_iou'][best_idx]\n",
        "      \n",
        "      print(\"Best val_loss: %.4f, val_mean_iou: %.4f\"%(val_loss, val_mean_iou))\n",
        "      loss.append(val_loss)\n",
        "      metric.append(val_mean_iou)\n",
        "      \n",
        "      model = keras.models.load_model('model/keras_%d.model'%fold_index, custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss, 'dice_loss': dice_loss})\n",
        "      pred_test += model.predict(X_test) / fold\n",
        "      fold_index = fold_index + 1\n",
        "      \n",
        "      del model\n",
        "      gc.collect()\n",
        "      \n",
        "      if fast:\n",
        "        break\n",
        "      \n",
        "      \n",
        "    print('%d fold val_loss: %.4f, val_mean_iou: %.4f'%(fold, np.mean(loss), np.mean(metric)))\n",
        "    return pred_test, np.mean(loss), np.mean(metric)\n",
        "\n",
        "  \n",
        "pred_test, val_loss, val_mean_iou = cv(X_train, y_train, X_test, 5, fast=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "16\n",
            "32\n",
            "64\n",
            "128\n",
            "256\n",
            "512\n",
            "Tensor(\"conv2d_transpose_26/Relu:0\", shape=(?, 2, 2, 512), dtype=float32) Tensor(\"batch_normalization_158/cond/Merge:0\", shape=(?, 2, 2, 512), dtype=float32)\n",
            "256\n",
            "Tensor(\"conv2d_transpose_29/Relu:0\", shape=(?, 4, 4, 256), dtype=float32) Tensor(\"batch_normalization_161/cond/Merge:0\", shape=(?, 4, 4, 256), dtype=float32)\n",
            "128\n",
            "Tensor(\"conv2d_transpose_32/Relu:0\", shape=(?, 8, 8, 128), dtype=float32) Tensor(\"batch_normalization_164/cond/Merge:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
            "64\n",
            "Tensor(\"conv2d_transpose_35/Relu:0\", shape=(?, 16, 16, 64), dtype=float32) Tensor(\"batch_normalization_167/cond/Merge:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
            "32\n",
            "Tensor(\"conv2d_transpose_38/Relu:0\", shape=(?, 32, 32, 32), dtype=float32) Tensor(\"batch_normalization_170/cond/Merge:0\", shape=(?, 32, 32, 32), dtype=float32)\n",
            "16\n",
            "Tensor(\"conv2d_transpose_41/Relu:0\", shape=(?, 64, 64, 16), dtype=float32) Tensor(\"batch_normalization_173/cond/Merge:0\", shape=(?, 64, 64, 16), dtype=float32)\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8986511690744612254)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17578464049263961541)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 1822323957473087955)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11699108356318831405)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15578942636750116165)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 4723819265577645578)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15658835048562211150)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14084991468488383314)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 13322110499786951539)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6306473378593074235)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5054784506900200386)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 18203919771645832346)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: grpc://10.240.1.2:8470\n",
            "Epoch 1/20000\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(64, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(64, 128, 128, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_10\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'amsgrad': False, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'beta_1': 0.8999999761581421, 'epsilon': 1e-07}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 26.00396728515625 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "49/50 [============================>.] - ETA: 1s - loss: 0.5336 - mean_iou: 0.1716 - binary_crossentropy: 0.5336INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(64, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(64, 128, 128, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_10\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'amsgrad': False, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'beta_1': 0.8999999761581421, 'epsilon': 1e-07}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.776343822479248 secs\n",
            "50/50 [==============================] - 92s 2s/step - loss: 0.5304 - mean_iou: 0.1740 - binary_crossentropy: 0.5304 - val_loss: 0.7930 - val_mean_iou: 0.1228 - val_binary_crossentropy: 0.7930\n",
            "Epoch 2/20000\n",
            "50/50 [==============================] - 21s 428ms/step - loss: 0.3464 - mean_iou: 0.2748 - binary_crossentropy: 0.3464 - val_loss: 1.8527 - val_mean_iou: 0.1258 - val_binary_crossentropy: 1.8527\n",
            "Epoch 3/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.2155 - mean_iou: 0.3689 - binary_crossentropy: 0.2155 - val_loss: 4.1861 - val_mean_iou: 0.1436 - val_binary_crossentropy: 4.1861\n",
            "Epoch 4/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.1637 - mean_iou: 0.5278 - binary_crossentropy: 0.1637 - val_loss: 2.5910 - val_mean_iou: 0.1846 - val_binary_crossentropy: 2.5910\n",
            "Epoch 5/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.1206 - mean_iou: 0.6372 - binary_crossentropy: 0.1206 - val_loss: 1.6836 - val_mean_iou: 0.2399 - val_binary_crossentropy: 1.6836\n",
            "Epoch 6/20000\n",
            "50/50 [==============================] - 22s 437ms/step - loss: 0.1009 - mean_iou: 0.7120 - binary_crossentropy: 0.1009 - val_loss: 1.0404 - val_mean_iou: 0.4095 - val_binary_crossentropy: 1.0404\n",
            "Epoch 7/20000\n",
            "50/50 [==============================] - 21s 428ms/step - loss: 0.0819 - mean_iou: 0.7266 - binary_crossentropy: 0.0819 - val_loss: 0.6952 - val_mean_iou: 0.5373 - val_binary_crossentropy: 0.6952\n",
            "Epoch 8/20000\n",
            "50/50 [==============================] - 21s 426ms/step - loss: 0.0602 - mean_iou: 0.7745 - binary_crossentropy: 0.0602 - val_loss: 0.6880 - val_mean_iou: 0.5464 - val_binary_crossentropy: 0.6880\n",
            "Epoch 9/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0665 - mean_iou: 0.7670 - binary_crossentropy: 0.0665 - val_loss: 0.4069 - val_mean_iou: 0.5875 - val_binary_crossentropy: 0.4069\n",
            "Epoch 10/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0577 - mean_iou: 0.7873 - binary_crossentropy: 0.0577 - val_loss: 0.3010 - val_mean_iou: 0.6285 - val_binary_crossentropy: 0.3010\n",
            "Epoch 11/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0468 - mean_iou: 0.7917 - binary_crossentropy: 0.0468 - val_loss: 0.3335 - val_mean_iou: 0.6452 - val_binary_crossentropy: 0.3335\n",
            "Epoch 12/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0432 - mean_iou: 0.8108 - binary_crossentropy: 0.0432 - val_loss: 0.3215 - val_mean_iou: 0.6514 - val_binary_crossentropy: 0.3215\n",
            "Epoch 13/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.0375 - mean_iou: 0.8174 - binary_crossentropy: 0.0375 - val_loss: 0.3597 - val_mean_iou: 0.6465 - val_binary_crossentropy: 0.3597\n",
            "Epoch 14/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0275 - mean_iou: 0.8526 - binary_crossentropy: 0.0275 - val_loss: 0.3514 - val_mean_iou: 0.6499 - val_binary_crossentropy: 0.3514\n",
            "Epoch 15/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0221 - mean_iou: 0.8902 - binary_crossentropy: 0.0221 - val_loss: 0.4222 - val_mean_iou: 0.6534 - val_binary_crossentropy: 0.4222\n",
            "Epoch 16/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0199 - mean_iou: 0.8948 - binary_crossentropy: 0.0199 - val_loss: 0.3227 - val_mean_iou: 0.6469 - val_binary_crossentropy: 0.3227\n",
            "Epoch 17/20000\n",
            "50/50 [==============================] - 21s 427ms/step - loss: 0.0189 - mean_iou: 0.8979 - binary_crossentropy: 0.0189 - val_loss: 0.3741 - val_mean_iou: 0.6327 - val_binary_crossentropy: 0.3741\n",
            "Epoch 18/20000\n",
            "50/50 [==============================] - 21s 427ms/step - loss: 0.0307 - mean_iou: 0.8718 - binary_crossentropy: 0.0307 - val_loss: 0.4185 - val_mean_iou: 0.5113 - val_binary_crossentropy: 0.4185\n",
            "Epoch 19/20000\n",
            "50/50 [==============================] - 21s 427ms/step - loss: 0.0469 - mean_iou: 0.8018 - binary_crossentropy: 0.0469 - val_loss: 0.3537 - val_mean_iou: 0.5800 - val_binary_crossentropy: 0.3537\n",
            "Epoch 20/20000\n",
            "50/50 [==============================] - 21s 426ms/step - loss: 0.0507 - mean_iou: 0.7879 - binary_crossentropy: 0.0507 - val_loss: 0.3458 - val_mean_iou: 0.6285 - val_binary_crossentropy: 0.3458\n",
            "Epoch 21/20000\n",
            "50/50 [==============================] - 22s 440ms/step - loss: 0.0275 - mean_iou: 0.8542 - binary_crossentropy: 0.0275 - val_loss: 0.3236 - val_mean_iou: 0.6144 - val_binary_crossentropy: 0.3236\n",
            "Epoch 22/20000\n",
            "50/50 [==============================] - 21s 426ms/step - loss: 0.0393 - mean_iou: 0.8256 - binary_crossentropy: 0.0393 - val_loss: 0.3965 - val_mean_iou: 0.5923 - val_binary_crossentropy: 0.3965\n",
            "Epoch 23/20000\n",
            "50/50 [==============================] - 21s 428ms/step - loss: 0.0315 - mean_iou: 0.8328 - binary_crossentropy: 0.0315 - val_loss: 0.3388 - val_mean_iou: 0.6423 - val_binary_crossentropy: 0.3388\n",
            "Epoch 24/20000\n",
            "50/50 [==============================] - 21s 426ms/step - loss: 0.0244 - mean_iou: 0.8705 - binary_crossentropy: 0.0244 - val_loss: 0.3494 - val_mean_iou: 0.6240 - val_binary_crossentropy: 0.3494\n",
            "Epoch 25/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.0177 - mean_iou: 0.8949 - binary_crossentropy: 0.0177 - val_loss: 0.3518 - val_mean_iou: 0.6585 - val_binary_crossentropy: 0.3518\n",
            "Epoch 26/20000\n",
            "50/50 [==============================] - 21s 429ms/step - loss: 0.0197 - mean_iou: 0.9089 - binary_crossentropy: 0.0197 - val_loss: 0.4416 - val_mean_iou: 0.6123 - val_binary_crossentropy: 0.4416\n",
            "Epoch 27/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0256 - mean_iou: 0.8640 - binary_crossentropy: 0.0256 - val_loss: 0.3440 - val_mean_iou: 0.6429 - val_binary_crossentropy: 0.3440\n",
            "Epoch 28/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0179 - mean_iou: 0.8923 - binary_crossentropy: 0.0179 - val_loss: 0.3952 - val_mean_iou: 0.6762 - val_binary_crossentropy: 0.3952\n",
            "Epoch 29/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0188 - mean_iou: 0.8937 - binary_crossentropy: 0.0188 - val_loss: 0.3015 - val_mean_iou: 0.6856 - val_binary_crossentropy: 0.3015\n",
            "Epoch 30/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0164 - mean_iou: 0.9041 - binary_crossentropy: 0.0164 - val_loss: 0.2985 - val_mean_iou: 0.6816 - val_binary_crossentropy: 0.2985\n",
            "Epoch 31/20000\n",
            "50/50 [==============================] - 21s 428ms/step - loss: 0.0129 - mean_iou: 0.9141 - binary_crossentropy: 0.0129 - val_loss: 0.3235 - val_mean_iou: 0.6839 - val_binary_crossentropy: 0.3235\n",
            "Epoch 32/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0114 - mean_iou: 0.9358 - binary_crossentropy: 0.0114 - val_loss: 0.4080 - val_mean_iou: 0.6702 - val_binary_crossentropy: 0.4080\n",
            "Epoch 33/20000\n",
            "50/50 [==============================] - 21s 429ms/step - loss: 0.0110 - mean_iou: 0.9325 - binary_crossentropy: 0.0110 - val_loss: 0.3772 - val_mean_iou: 0.6696 - val_binary_crossentropy: 0.3772\n",
            "Epoch 34/20000\n",
            "50/50 [==============================] - 21s 426ms/step - loss: 0.0104 - mean_iou: 0.9347 - binary_crossentropy: 0.0104 - val_loss: 0.4243 - val_mean_iou: 0.6828 - val_binary_crossentropy: 0.4243\n",
            "Epoch 35/20000\n",
            "50/50 [==============================] - 22s 441ms/step - loss: 0.0104 - mean_iou: 0.9375 - binary_crossentropy: 0.0104 - val_loss: 0.4155 - val_mean_iou: 0.6588 - val_binary_crossentropy: 0.4155\n",
            "Epoch 36/20000\n",
            "50/50 [==============================] - 21s 429ms/step - loss: 0.0094 - mean_iou: 0.9371 - binary_crossentropy: 0.0094 - val_loss: 0.3296 - val_mean_iou: 0.6690 - val_binary_crossentropy: 0.3296\n",
            "Epoch 37/20000\n",
            "50/50 [==============================] - 21s 427ms/step - loss: 0.0091 - mean_iou: 0.9411 - binary_crossentropy: 0.0091 - val_loss: 0.3971 - val_mean_iou: 0.6657 - val_binary_crossentropy: 0.3971\n",
            "Epoch 38/20000\n",
            "50/50 [==============================] - 21s 428ms/step - loss: 0.0092 - mean_iou: 0.9445 - binary_crossentropy: 0.0092 - val_loss: 0.4289 - val_mean_iou: 0.6653 - val_binary_crossentropy: 0.4289\n",
            "Epoch 39/20000\n",
            "50/50 [==============================] - 21s 427ms/step - loss: 0.0089 - mean_iou: 0.9444 - binary_crossentropy: 0.0089 - val_loss: 0.4225 - val_mean_iou: 0.6782 - val_binary_crossentropy: 0.4225\n",
            "Epoch 40/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0088 - mean_iou: 0.9403 - binary_crossentropy: 0.0088 - val_loss: 0.4205 - val_mean_iou: 0.6663 - val_binary_crossentropy: 0.4205\n",
            "Epoch 41/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0086 - mean_iou: 0.9449 - binary_crossentropy: 0.0086 - val_loss: 0.4320 - val_mean_iou: 0.6778 - val_binary_crossentropy: 0.4320\n",
            "Epoch 42/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0081 - mean_iou: 0.9473 - binary_crossentropy: 0.0081 - val_loss: 0.4375 - val_mean_iou: 0.6665 - val_binary_crossentropy: 0.4375\n",
            "Epoch 43/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0083 - mean_iou: 0.9500 - binary_crossentropy: 0.0083 - val_loss: 0.4034 - val_mean_iou: 0.6661 - val_binary_crossentropy: 0.4034\n",
            "Epoch 44/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0080 - mean_iou: 0.9484 - binary_crossentropy: 0.0080 - val_loss: 0.4815 - val_mean_iou: 0.6442 - val_binary_crossentropy: 0.4815\n",
            "Epoch 45/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0078 - mean_iou: 0.9523 - binary_crossentropy: 0.0078 - val_loss: 0.4529 - val_mean_iou: 0.6573 - val_binary_crossentropy: 0.4529\n",
            "Epoch 46/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0083 - mean_iou: 0.9512 - binary_crossentropy: 0.0083 - val_loss: 0.4379 - val_mean_iou: 0.6691 - val_binary_crossentropy: 0.4379\n",
            "Epoch 47/20000\n",
            "50/50 [==============================] - 21s 420ms/step - loss: 0.0082 - mean_iou: 0.9444 - binary_crossentropy: 0.0082 - val_loss: 0.3964 - val_mean_iou: 0.6505 - val_binary_crossentropy: 0.3964\n",
            "Epoch 48/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.0077 - mean_iou: 0.9557 - binary_crossentropy: 0.0077 - val_loss: 0.4390 - val_mean_iou: 0.6555 - val_binary_crossentropy: 0.4390\n",
            "Epoch 49/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0101 - mean_iou: 0.9318 - binary_crossentropy: 0.0101 - val_loss: 0.5140 - val_mean_iou: 0.6252 - val_binary_crossentropy: 0.5140\n",
            "Epoch 50/20000\n",
            "50/50 [==============================] - 22s 436ms/step - loss: 0.0095 - mean_iou: 0.9442 - binary_crossentropy: 0.0095 - val_loss: 0.4598 - val_mean_iou: 0.6554 - val_binary_crossentropy: 0.4598\n",
            "Epoch 51/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0080 - mean_iou: 0.9504 - binary_crossentropy: 0.0080 - val_loss: 0.4564 - val_mean_iou: 0.6546 - val_binary_crossentropy: 0.4564\n",
            "Epoch 52/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0079 - mean_iou: 0.9493 - binary_crossentropy: 0.0079 - val_loss: 0.4320 - val_mean_iou: 0.6478 - val_binary_crossentropy: 0.4320\n",
            "Epoch 53/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0078 - mean_iou: 0.9552 - binary_crossentropy: 0.0078 - val_loss: 0.3678 - val_mean_iou: 0.6781 - val_binary_crossentropy: 0.3678\n",
            "Epoch 54/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0072 - mean_iou: 0.9565 - binary_crossentropy: 0.0072 - val_loss: 0.3275 - val_mean_iou: 0.6819 - val_binary_crossentropy: 0.3275\n",
            "Epoch 55/20000\n",
            "50/50 [==============================] - 21s 420ms/step - loss: 0.0071 - mean_iou: 0.9583 - binary_crossentropy: 0.0071 - val_loss: 0.4364 - val_mean_iou: 0.6653 - val_binary_crossentropy: 0.4364\n",
            "Epoch 56/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0071 - mean_iou: 0.9571 - binary_crossentropy: 0.0071 - val_loss: 0.3800 - val_mean_iou: 0.6792 - val_binary_crossentropy: 0.3800\n",
            "Epoch 57/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0069 - mean_iou: 0.9579 - binary_crossentropy: 0.0069 - val_loss: 0.4831 - val_mean_iou: 0.6522 - val_binary_crossentropy: 0.4831\n",
            "Epoch 58/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0068 - mean_iou: 0.9584 - binary_crossentropy: 0.0068 - val_loss: 0.4109 - val_mean_iou: 0.6817 - val_binary_crossentropy: 0.4109\n",
            "Epoch 59/20000\n",
            "50/50 [==============================] - 21s 420ms/step - loss: 0.0071 - mean_iou: 0.9597 - binary_crossentropy: 0.0071 - val_loss: 0.4716 - val_mean_iou: 0.6764 - val_binary_crossentropy: 0.4716\n",
            "Epoch 60/20000\n",
            "50/50 [==============================] - 21s 420ms/step - loss: 0.0067 - mean_iou: 0.9568 - binary_crossentropy: 0.0067 - val_loss: 0.4156 - val_mean_iou: 0.6847 - val_binary_crossentropy: 0.4156\n",
            "Epoch 61/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0066 - mean_iou: 0.9585 - binary_crossentropy: 0.0066 - val_loss: 0.4069 - val_mean_iou: 0.6498 - val_binary_crossentropy: 0.4069\n",
            "Epoch 62/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0066 - mean_iou: 0.9623 - binary_crossentropy: 0.0066 - val_loss: 0.4767 - val_mean_iou: 0.6688 - val_binary_crossentropy: 0.4767\n",
            "Epoch 63/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0065 - mean_iou: 0.9605 - binary_crossentropy: 0.0065 - val_loss: 0.4307 - val_mean_iou: 0.6831 - val_binary_crossentropy: 0.4307\n",
            "Epoch 64/20000\n",
            "50/50 [==============================] - 21s 428ms/step - loss: 0.0063 - mean_iou: 0.9579 - binary_crossentropy: 0.0063 - val_loss: 0.5216 - val_mean_iou: 0.6444 - val_binary_crossentropy: 0.5216\n",
            "Epoch 65/20000\n",
            "50/50 [==============================] - 22s 433ms/step - loss: 0.0067 - mean_iou: 0.9615 - binary_crossentropy: 0.0067 - val_loss: 0.4380 - val_mean_iou: 0.6732 - val_binary_crossentropy: 0.4380\n",
            "Epoch 66/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0064 - mean_iou: 0.9604 - binary_crossentropy: 0.0064 - val_loss: 0.4957 - val_mean_iou: 0.6637 - val_binary_crossentropy: 0.4957\n",
            "Epoch 67/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0064 - mean_iou: 0.9616 - binary_crossentropy: 0.0064 - val_loss: 0.4709 - val_mean_iou: 0.6793 - val_binary_crossentropy: 0.4709\n",
            "Epoch 68/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.0067 - mean_iou: 0.9662 - binary_crossentropy: 0.0067 - val_loss: 0.4885 - val_mean_iou: 0.6529 - val_binary_crossentropy: 0.4885\n",
            "Epoch 69/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0062 - mean_iou: 0.9640 - binary_crossentropy: 0.0062 - val_loss: 0.5912 - val_mean_iou: 0.6143 - val_binary_crossentropy: 0.5912\n",
            "Epoch 70/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0063 - mean_iou: 0.9630 - binary_crossentropy: 0.0063 - val_loss: 0.4589 - val_mean_iou: 0.6536 - val_binary_crossentropy: 0.4589\n",
            "Epoch 71/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0061 - mean_iou: 0.9638 - binary_crossentropy: 0.0061 - val_loss: 0.5328 - val_mean_iou: 0.6585 - val_binary_crossentropy: 0.5328\n",
            "Epoch 72/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0060 - mean_iou: 0.9666 - binary_crossentropy: 0.0060 - val_loss: 0.3896 - val_mean_iou: 0.6837 - val_binary_crossentropy: 0.3896\n",
            "Epoch 73/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0061 - mean_iou: 0.9644 - binary_crossentropy: 0.0061 - val_loss: 0.4790 - val_mean_iou: 0.6496 - val_binary_crossentropy: 0.4790\n",
            "Epoch 74/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0062 - mean_iou: 0.9624 - binary_crossentropy: 0.0062 - val_loss: 0.5135 - val_mean_iou: 0.6374 - val_binary_crossentropy: 0.5135\n",
            "Epoch 75/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0061 - mean_iou: 0.9654 - binary_crossentropy: 0.0061 - val_loss: 0.5394 - val_mean_iou: 0.6439 - val_binary_crossentropy: 0.5394\n",
            "Epoch 76/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0064 - mean_iou: 0.9606 - binary_crossentropy: 0.0064 - val_loss: 0.4816 - val_mean_iou: 0.6362 - val_binary_crossentropy: 0.4816\n",
            "Epoch 77/20000\n",
            "50/50 [==============================] - 21s 420ms/step - loss: 0.0062 - mean_iou: 0.9616 - binary_crossentropy: 0.0062 - val_loss: 0.4414 - val_mean_iou: 0.6875 - val_binary_crossentropy: 0.4414\n",
            "Epoch 78/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.1157 - mean_iou: 0.8023 - binary_crossentropy: 0.1157 - val_loss: 3.3910 - val_mean_iou: 0.2483 - val_binary_crossentropy: 3.3910\n",
            "Epoch 79/20000\n",
            "50/50 [==============================] - 22s 434ms/step - loss: 0.1310 - mean_iou: 0.6621 - binary_crossentropy: 0.1310 - val_loss: 4.2204 - val_mean_iou: 0.2314 - val_binary_crossentropy: 4.2204\n",
            "Epoch 80/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.0666 - mean_iou: 0.7721 - binary_crossentropy: 0.0666 - val_loss: 0.5088 - val_mean_iou: 0.5947 - val_binary_crossentropy: 0.5088\n",
            "Epoch 81/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0440 - mean_iou: 0.7897 - binary_crossentropy: 0.0440 - val_loss: 0.2429 - val_mean_iou: 0.6635 - val_binary_crossentropy: 0.2429\n",
            "Epoch 82/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0538 - mean_iou: 0.7968 - binary_crossentropy: 0.0538 - val_loss: 0.3179 - val_mean_iou: 0.5555 - val_binary_crossentropy: 0.3179\n",
            "Epoch 83/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0459 - mean_iou: 0.7958 - binary_crossentropy: 0.0459 - val_loss: 0.3167 - val_mean_iou: 0.6281 - val_binary_crossentropy: 0.3167\n",
            "Epoch 84/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0303 - mean_iou: 0.8470 - binary_crossentropy: 0.0303 - val_loss: 0.3187 - val_mean_iou: 0.6288 - val_binary_crossentropy: 0.3187\n",
            "Epoch 85/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0206 - mean_iou: 0.8867 - binary_crossentropy: 0.0206 - val_loss: 0.3065 - val_mean_iou: 0.6998 - val_binary_crossentropy: 0.3065\n",
            "Epoch 86/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0165 - mean_iou: 0.8971 - binary_crossentropy: 0.0165 - val_loss: 0.3376 - val_mean_iou: 0.6766 - val_binary_crossentropy: 0.3376\n",
            "Epoch 87/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0158 - mean_iou: 0.8989 - binary_crossentropy: 0.0158 - val_loss: 0.3864 - val_mean_iou: 0.6620 - val_binary_crossentropy: 0.3864\n",
            "Epoch 88/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0198 - mean_iou: 0.8899 - binary_crossentropy: 0.0198 - val_loss: 0.3066 - val_mean_iou: 0.6770 - val_binary_crossentropy: 0.3066\n",
            "Epoch 89/20000\n",
            "50/50 [==============================] - 21s 426ms/step - loss: 0.0168 - mean_iou: 0.9105 - binary_crossentropy: 0.0168 - val_loss: 0.3217 - val_mean_iou: 0.6986 - val_binary_crossentropy: 0.3217\n",
            "Epoch 90/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0182 - mean_iou: 0.9128 - binary_crossentropy: 0.0182 - val_loss: 0.3883 - val_mean_iou: 0.6663 - val_binary_crossentropy: 0.3883\n",
            "Epoch 91/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0187 - mean_iou: 0.9022 - binary_crossentropy: 0.0187 - val_loss: 0.3672 - val_mean_iou: 0.6894 - val_binary_crossentropy: 0.3672\n",
            "Epoch 92/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0141 - mean_iou: 0.9194 - binary_crossentropy: 0.0141 - val_loss: 0.3723 - val_mean_iou: 0.6671 - val_binary_crossentropy: 0.3723\n",
            "Epoch 93/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0111 - mean_iou: 0.9268 - binary_crossentropy: 0.0111 - val_loss: 0.3664 - val_mean_iou: 0.6734 - val_binary_crossentropy: 0.3664\n",
            "Epoch 94/20000\n",
            "50/50 [==============================] - 22s 441ms/step - loss: 0.0105 - mean_iou: 0.9343 - binary_crossentropy: 0.0105 - val_loss: 0.4015 - val_mean_iou: 0.6611 - val_binary_crossentropy: 0.4015\n",
            "Epoch 95/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.0096 - mean_iou: 0.9425 - binary_crossentropy: 0.0096 - val_loss: 0.4260 - val_mean_iou: 0.6696 - val_binary_crossentropy: 0.4260\n",
            "Epoch 96/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0094 - mean_iou: 0.9398 - binary_crossentropy: 0.0094 - val_loss: 0.3564 - val_mean_iou: 0.6669 - val_binary_crossentropy: 0.3564\n",
            "Epoch 97/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.0093 - mean_iou: 0.9418 - binary_crossentropy: 0.0093 - val_loss: 0.4201 - val_mean_iou: 0.6744 - val_binary_crossentropy: 0.4201\n",
            "Epoch 98/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0090 - mean_iou: 0.9447 - binary_crossentropy: 0.0090 - val_loss: 0.4519 - val_mean_iou: 0.6535 - val_binary_crossentropy: 0.4519\n",
            "Epoch 99/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0084 - mean_iou: 0.9440 - binary_crossentropy: 0.0084 - val_loss: 0.4624 - val_mean_iou: 0.6794 - val_binary_crossentropy: 0.4624\n",
            "Epoch 100/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0085 - mean_iou: 0.9440 - binary_crossentropy: 0.0085 - val_loss: 0.4174 - val_mean_iou: 0.6683 - val_binary_crossentropy: 0.4174\n",
            "Epoch 101/20000\n",
            "50/50 [==============================] - 21s 426ms/step - loss: 0.0084 - mean_iou: 0.9480 - binary_crossentropy: 0.0084 - val_loss: 0.4205 - val_mean_iou: 0.6680 - val_binary_crossentropy: 0.4205\n",
            "Epoch 102/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0088 - mean_iou: 0.9470 - binary_crossentropy: 0.0088 - val_loss: 0.4682 - val_mean_iou: 0.6291 - val_binary_crossentropy: 0.4682\n",
            "Epoch 103/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.0089 - mean_iou: 0.9457 - binary_crossentropy: 0.0089 - val_loss: 0.4532 - val_mean_iou: 0.6613 - val_binary_crossentropy: 0.4532\n",
            "Epoch 104/20000\n",
            "50/50 [==============================] - 21s 421ms/step - loss: 0.0080 - mean_iou: 0.9502 - binary_crossentropy: 0.0080 - val_loss: 0.4181 - val_mean_iou: 0.6544 - val_binary_crossentropy: 0.4181\n",
            "Epoch 105/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0082 - mean_iou: 0.9517 - binary_crossentropy: 0.0082 - val_loss: 0.4195 - val_mean_iou: 0.6508 - val_binary_crossentropy: 0.4195\n",
            "Epoch 106/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0076 - mean_iou: 0.9519 - binary_crossentropy: 0.0076 - val_loss: 0.4496 - val_mean_iou: 0.6536 - val_binary_crossentropy: 0.4496\n",
            "Epoch 107/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0074 - mean_iou: 0.9545 - binary_crossentropy: 0.0074 - val_loss: 0.4493 - val_mean_iou: 0.6548 - val_binary_crossentropy: 0.4493\n",
            "Epoch 108/20000\n",
            "50/50 [==============================] - 22s 431ms/step - loss: 0.0072 - mean_iou: 0.9568 - binary_crossentropy: 0.0072 - val_loss: 0.4410 - val_mean_iou: 0.6344 - val_binary_crossentropy: 0.4410\n",
            "Epoch 109/20000\n",
            "50/50 [==============================] - 21s 430ms/step - loss: 0.0069 - mean_iou: 0.9610 - binary_crossentropy: 0.0069 - val_loss: 0.4672 - val_mean_iou: 0.6555 - val_binary_crossentropy: 0.4672\n",
            "Epoch 110/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.0071 - mean_iou: 0.9591 - binary_crossentropy: 0.0071 - val_loss: 0.4875 - val_mean_iou: 0.6570 - val_binary_crossentropy: 0.4875\n",
            "Epoch 111/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0069 - mean_iou: 0.9550 - binary_crossentropy: 0.0069 - val_loss: 0.4817 - val_mean_iou: 0.6611 - val_binary_crossentropy: 0.4817\n",
            "Epoch 112/20000\n",
            "50/50 [==============================] - 21s 420ms/step - loss: 0.0074 - mean_iou: 0.9625 - binary_crossentropy: 0.0074 - val_loss: 0.4348 - val_mean_iou: 0.6398 - val_binary_crossentropy: 0.4348\n",
            "Epoch 113/20000\n",
            "50/50 [==============================] - 21s 427ms/step - loss: 0.0068 - mean_iou: 0.9538 - binary_crossentropy: 0.0068 - val_loss: 0.5042 - val_mean_iou: 0.6249 - val_binary_crossentropy: 0.5042\n",
            "Epoch 114/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0069 - mean_iou: 0.9585 - binary_crossentropy: 0.0069 - val_loss: 0.4478 - val_mean_iou: 0.6421 - val_binary_crossentropy: 0.4478\n",
            "Epoch 115/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0067 - mean_iou: 0.9562 - binary_crossentropy: 0.0067 - val_loss: 0.4141 - val_mean_iou: 0.6219 - val_binary_crossentropy: 0.4141\n",
            "Epoch 116/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0066 - mean_iou: 0.9603 - binary_crossentropy: 0.0066 - val_loss: 0.5187 - val_mean_iou: 0.5995 - val_binary_crossentropy: 0.5187\n",
            "Epoch 117/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0065 - mean_iou: 0.9617 - binary_crossentropy: 0.0065 - val_loss: 0.4451 - val_mean_iou: 0.6405 - val_binary_crossentropy: 0.4451\n",
            "Epoch 118/20000\n",
            "50/50 [==============================] - 21s 420ms/step - loss: 0.0063 - mean_iou: 0.9559 - binary_crossentropy: 0.0063 - val_loss: 0.5202 - val_mean_iou: 0.6591 - val_binary_crossentropy: 0.5202\n",
            "Epoch 119/20000\n",
            "50/50 [==============================] - 21s 425ms/step - loss: 0.0069 - mean_iou: 0.9602 - binary_crossentropy: 0.0069 - val_loss: 0.4298 - val_mean_iou: 0.6751 - val_binary_crossentropy: 0.4298\n",
            "Epoch 120/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0065 - mean_iou: 0.9608 - binary_crossentropy: 0.0065 - val_loss: 0.4577 - val_mean_iou: 0.6743 - val_binary_crossentropy: 0.4577\n",
            "Epoch 121/20000\n",
            "50/50 [==============================] - 21s 424ms/step - loss: 0.0065 - mean_iou: 0.9613 - binary_crossentropy: 0.0065 - val_loss: 0.3605 - val_mean_iou: 0.6674 - val_binary_crossentropy: 0.3605\n",
            "Epoch 122/20000\n",
            "50/50 [==============================] - 21s 426ms/step - loss: 0.0063 - mean_iou: 0.9640 - binary_crossentropy: 0.0063 - val_loss: 0.5255 - val_mean_iou: 0.6350 - val_binary_crossentropy: 0.5255\n",
            "Epoch 123/20000\n",
            "50/50 [==============================] - 22s 438ms/step - loss: 0.0063 - mean_iou: 0.9610 - binary_crossentropy: 0.0063 - val_loss: 0.4500 - val_mean_iou: 0.6566 - val_binary_crossentropy: 0.4500\n",
            "Epoch 124/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0062 - mean_iou: 0.9592 - binary_crossentropy: 0.0062 - val_loss: 0.4622 - val_mean_iou: 0.6525 - val_binary_crossentropy: 0.4622\n",
            "Epoch 125/20000\n",
            "50/50 [==============================] - 21s 423ms/step - loss: 0.0062 - mean_iou: 0.9620 - binary_crossentropy: 0.0062 - val_loss: 0.4559 - val_mean_iou: 0.6564 - val_binary_crossentropy: 0.4559\n",
            "Epoch 126/20000\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 0.0062 - mean_iou: 0.9663 - binary_crossentropy: 0.0062 - val_loss: 0.3971 - val_mean_iou: 0.6260 - val_binary_crossentropy: 0.3971\n",
            "Epoch 127/20000\n",
            "40/50 [=======================>......] - ETA: 4s - loss: 0.0059 - mean_iou: 0.9633 - binary_crossentropy: 0.0059"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-12d1ee92d004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-12d1ee92d004>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(X_train, y_train, X_test, fold, fast)\u001b[0m\n\u001b[1;32m     75\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_data_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;31m#           callbacks=[early_stopping, model_checkpoint, reduce_lr],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#           callbacks=[early_stopping, reduce_lr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1487\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0;31m# Sample-wise fit loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_step_wise\u001b[0;34m(self, ins, callbacks, steps_per_epoch, epochs, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m           \u001b[0mnext_step_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         outs = f.pipeline_run(cur_step_inputs=ins,\n\u001b[0;32m-> 1631\u001b[0;31m                               next_step_inputs=next_step_inputs)\n\u001b[0m\u001b[1;32m   1632\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         logging.warning('Your dataset iterator ran out of data; '\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mpipeline_run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0mnext_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         ], infeed_dict)\n\u001b[0m\u001b[1;32m   1089\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcur_tpu_model_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uf9lolkBBkSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6fwJZzEBkQl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uk1SXFM3BkNT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yQXxqr0MAqoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tFBYhgB5NEjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "71db68f4-a00c-46db-c6d5-250cb187778d"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def test_model():\n",
        "  x = tf.keras.layers.Input(shape=(101,101,1), dtype=np.float)\n",
        "  y = tf.keras.layers.Conv2D(1,1, padding='same', activation='sigmoid', dtype=np.float)(x)\n",
        "  m = tf.keras.Model(x, y)\n",
        "  return m\n",
        "\n",
        "model = test_model()\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 101, 101, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 101, 101, 1)       2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4tHvN3g6PA92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "4d8bf063-37b5-4805-a9d5-452f8815d2af"
      },
      "cell_type": "code",
      "source": [
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "session_master = resolver.master()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6931466752683336584)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2319523528817134981)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2621229120846880910)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13017604127097533172)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13684817192547276644)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15939364587101623240)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6777068171437765544)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 677236183219516705)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 17055041862000479642)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13317328517731664289)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7926127133639364078)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9787759974722640898)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: grpc://10.240.1.2:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cXtj4ZEb85dv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIyUjH87qLvJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2068
        },
        "outputId": "ea6b8821-4837-4968-ea22-0ada27acd56a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def _set_shape(image, mask):\n",
        "  image.set_shape([101, 101, 1])\n",
        "  mask.set_shape([101, 101, 1])\n",
        "  return image, mask\n",
        "\n",
        "def input_fn():\n",
        "  images = tf.random_uniform([1024, 101, 101, 1])\n",
        "  masks = tf.random_uniform([1024, 101, 101, 1])\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
        "#   print(dataset)\n",
        "#   dataset = dataset.map(lambda image, mask: tuple(tf.py_func(random_h_flip, [image, mask], [tf.float32, tf.float32])))\n",
        "#   print(dataset)\n",
        "  dataset = dataset.map(_random_flip_left_right)\n",
        "#   print(dataset)\n",
        "  dataset = dataset.shuffle(1000).repeat(100000).batch(32, drop_remainder=True)\n",
        "#   print(dataset)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01),\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['binary_crossentropy'])\n",
        "\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    input_fn,\n",
        "    validation_data=input_fn,\n",
        "    validation_steps=100,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<MapDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<BatchDataset shapes: ((32, 101, 101, 1), (32, 101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<TensorSliceDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<MapDataset shapes: ((101, 101, 1), (101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "<BatchDataset shapes: ((32, 101, 101, 1), (32, 101, 101, 1)), types: (tf.float32, tf.float32)>\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 11.059766054153442 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.6932 - binary_crossentropy: 0.6932INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 101, 101, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 10.902057647705078 secs\n",
            "100/100 [==============================] - 32s 319ms/step - loss: 0.6932 - binary_crossentropy: 0.6932 - val_loss: 0.6932 - val_binary_crossentropy: 0.6932\n",
            "Epoch 2/20\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.6932 - binary_crossentropy: 0.6932"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-292190155012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     epochs=20)\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1487\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0;31m# Sample-wise fit loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_step_wise\u001b[0;34m(self, ins, callbacks, steps_per_epoch, epochs, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                                            \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                                            \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                                            verbose=0)\n\u001b[0m\u001b[1;32m   1657\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    402\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1006\u001b[0m           \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m           \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       ], infeed_dict)\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6XdIwEcAqor5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2241
        },
        "outputId": "6f555f74-f366-4eb1-bd79-7687c211c77e"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef expected inputs 'float' do not match 2 inputs specified; Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\"](training/TFOptimizer/gradients/tpu_model_140369801514736/conv2d_1/Conv2D_grad/tuple/control_dependency_1, training/TFOptimizer/CrossReplicaSum/group_assignment)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-959ee3a79e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   def _pipeline_fit_loop(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_sample_wise\u001b[0;34m(self, ins, callbacks, index_array, shuffle, batch_size, num_training_samples, indices_for_conversion_to_dense, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m       outs = f.pipeline_run(cur_step_inputs=ins_last_batch,\n\u001b[0;32m-> 1558\u001b[0;31m                             next_step_inputs=ins_batch)\n\u001b[0m\u001b[1;32m   1559\u001b[0m       \u001b[0mins_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mpipeline_run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1075\u001b[0m           next_input_tensors)\n\u001b[1;32m   1076\u001b[0m       next_tpu_model_ops = self._tpu_model_ops_for_input_specs(\n\u001b[0;32m-> 1077\u001b[0;31m           next_input_specs, next_step_infeed_manager)\n\u001b[0m\u001b[1;32m   1078\u001b[0m       \u001b[0minfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_infeed_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_tpu_model_ops_for_input_specs\u001b[0;34m(self, input_specs, infeed_manager)\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mshape_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_specs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m         logging.info('New input shapes; (re-)compiling: mode=%s, %s',\n\u001b[1;32m    915\u001b[0m                      self.execution_mode, input_specs)\n",
            "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mtpu_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[0;34m\"\"\"Yields a TPU session and sets it as the default Keras session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m       \u001b[0mdefault_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1812\u001b[0m       \u001b[0;31m# N.B. We have to call `K.set_session()` AND set our session as the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m       \u001b[0;31m# TF default. `K.get_session()` surprisingly does not return the value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 710\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef expected inputs 'float' do not match 2 inputs specified; Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\"](training/TFOptimizer/gradients/tpu_model_140369801514736/conv2d_1/Conv2D_grad/tuple/control_dependency_1, training/TFOptimizer/CrossReplicaSum/group_assignment)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ibenmWsXzMNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7fe904b0-88ba-4e00-f9a6-1dc9f27aecbc"
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"ResNet-50 implemented with Keras running on Cloud TPUs.\n",
        "This file shows how you can run ResNet-50 on a Cloud TPU using the TensorFlow\n",
        "Keras support. This is configured for ImageNet (e.g. 1000 classes), but you can\n",
        "easily adapt to your own datasets by changing the code appropriately.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import imagenet_input\n",
        "\n",
        "try:\n",
        "  import h5py as _  # pylint: disable=g-import-not-at-top\n",
        "  HAS_H5PY = True\n",
        "except ImportError:\n",
        "  logging.warning('`h5py` is not installed. Please consider installing it '\n",
        "                  'to save weights for long-running training.')\n",
        "  HAS_H5PY = False\n",
        "\n",
        "\n",
        "flags.DEFINE_bool('use_tpu', True, 'Use TPU model instead of CPU.')\n",
        "flags.DEFINE_string('tpu', None, 'Name of the TPU to use.')\n",
        "flags.DEFINE_string('data', None, 'Path to training and testing data.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "PER_CORE_BATCH_SIZE = 128\n",
        "NUM_CLASSES = 1000\n",
        "IMAGE_SIZE = 224\n",
        "EPOCHS = 90  # Standard imagenet training regime.\n",
        "APPROX_IMAGENET_TRAINING_IMAGES = 1280000  # Approximate number of images.\n",
        "APPROX_IMAGENET_TEST_IMAGES = 48000  # Approximate number of images.\n",
        "\n",
        "WEIGHTS_TXT = '/tmp/resnet50_weights.h5'\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "  logging.info('Building Keras ResNet-50 model.')\n",
        "  model = tf.keras.applications.resnet50.ResNet50(\n",
        "      include_top=True,\n",
        "      weights=None,\n",
        "      input_tensor=None,\n",
        "      input_shape=None,\n",
        "      pooling=None,\n",
        "      classes=NUM_CLASSES)\n",
        "\n",
        "  num_cores = 8\n",
        "  batch_size = PER_CORE_BATCH_SIZE * num_cores\n",
        "\n",
        "  if FLAGS.use_tpu:\n",
        "    logging.info('Converting from CPU to TPU model.')\n",
        "    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "    session_master = resolver.master()\n",
        "  else:\n",
        "    session_master = ''\n",
        "\n",
        "  logging.info('Compiling model.')\n",
        "  model.compile(\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "  if FLAGS.data is None:\n",
        "    training_images = np.random.randn(\n",
        "        batch_size, IMAGE_SIZE, IMAGE_SIZE, 3).astype(np.float32)\n",
        "    training_labels = np.random.randint(NUM_CLASSES, size=batch_size,\n",
        "                                        dtype=np.int32)\n",
        "    logging.info('Training model using synthetica data.')\n",
        "    model.fit(training_images, training_labels, epochs=EPOCHS,\n",
        "              batch_size=batch_size)\n",
        "    logging.info('Evaluating the model on synthetic data.')\n",
        "    model.evaluate(training_images, training_labels, verbose=0)\n",
        "  else:\n",
        "    imagenet_train, imagenet_eval = [imagenet_input.ImageNetInput(\n",
        "        is_training=is_training,\n",
        "        data_dir=FLAGS.data,\n",
        "        per_core_batch_size=PER_CORE_BATCH_SIZE)\n",
        "                                     for is_training in [True, False]]\n",
        "    logging.info('Training model using real data in directory \"%s\".',\n",
        "                 FLAGS.data)\n",
        "    model.fit(imagenet_train.input_fn,\n",
        "              epochs=EPOCHS,\n",
        "              steps_per_epoch=int(APPROX_IMAGENET_TRAINING_IMAGES / batch_size))\n",
        "\n",
        "    if HAS_H5PY:\n",
        "      logging.info('Save weights into %s', WEIGHTS_TXT)\n",
        "      model.save_weights(WEIGHTS_TXT, overwrite=True)\n",
        "\n",
        "    logging.info('Evaluating the model on the validation dataset.')\n",
        "    score = model.evaluate(\n",
        "        imagenet_eval.input_fn,\n",
        "        steps=int(APPROX_IMAGENET_TEST_IMAGES // batch_size),\n",
        "        verbose=1)\n",
        "    print('Evaluation score', score)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c62c05d6ad63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: No module named 'imagenet_input'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "r9DlwwbXq6o_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import dotenv\n",
        "# import keras\n",
        "import os\n",
        "dotenv.load_dotenv('.env')\n",
        "\n",
        "!mkdir model\n",
        "# !gsutil rsync gs://{os.environ['GCP_BUCKET']}/model model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoL4Y1yXqtaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5cb32500-ba58-4b15-c239-2fd8bf232040"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('raw/train.csv').set_index('id')\n",
        "test = pd.read_csv('raw/sample_submission.csv').set_index('id')\n",
        "depths = pd.read_csv('raw/depths.csv').set_index('id')\n",
        "\n",
        "train = pd.merge(train, depths, left_index=True, right_index=True, how='inner')\n",
        "train.shape, test.shape, depths.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 2), (18000, 1), (22000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "6TALjxylQlox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_size_target = 101"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6nC41yXq8j0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "73d65f68-7590-4b43-f4b5-f712fd457b61"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "\n",
        "train[\"image\"] = [np.array(load_img(\"./raw/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train.index)]\n",
        "train[\"mask\"] = [np.array(load_img(\"./raw/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train.index)]\n",
        "\n",
        "X_train = np.array(train['image'].tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
        "y_train = np.array(train['mask'].tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
        "\n",
        "X_test = np.array([np.array(load_img(\"./raw/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(test.index)]).reshape(-1, img_size_target, img_size_target, 1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "  0%|          | 0/4000 [00:00<?, ?it/s]/home/Hoiy/.local/lib/python3.5/site-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n",
            "100%|██████████| 4000/4000 [00:02<00:00, 1517.39it/s]\n",
            "100%|██████████| 4000/4000 [00:01<00:00, 3034.27it/s]\n",
            "100%|██████████| 18000/18000 [00:11<00:00, 1533.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ns-p0XzLKHQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16f71cf5-ebcf-44e8-9fa0-96bb485f9f53"
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32', copy=False)\n",
        "y_train = y_train.astype('float32', copy=False)\n",
        "\n",
        "X_test = X_test.astype('float32', copy=False)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 101, 101, 1), (4000, 101, 101, 1), (18000, 101, 101, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "EKDfbbaX_1WY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_config\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_estimator\n",
        "# from tensorflow.contrib.tpu.python.tpu import tpu_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWtHOnEjhZs5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def sweep_thresholds_mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "def metric_fn(y_true, y_pred):\n",
        "  return {\n",
        "    'sweep_thresholds_mean_iou': sweep_thresholds_mean_iou(y_true, y_pred)\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIPW4QCxJEYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def host_call_fn(gs, loss, lr, ce):\n",
        "#   \"\"\"Training host call. Creates scalar summaries for training metrics.\n",
        "#   This function is executed on the CPU and should not directly reference\n",
        "#   any Tensors in the rest of the `model_fn`. To pass Tensors from the\n",
        "#   model to the `metric_fn`, provide as part of the `host_call`. See\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "#   for more information.\n",
        "#   Arguments should match the list of `Tensor` objects passed as the second\n",
        "#   element in the tuple passed to `host_call`.\n",
        "#   Args:\n",
        "#     gs: `Tensor with shape `[batch]` for the global_step\n",
        "#     loss: `Tensor` with shape `[batch]` for the training loss.\n",
        "#     lr: `Tensor` with shape `[batch]` for the learning_rate.\n",
        "#     ce: `Tensor` with shape `[batch]` for the current_epoch.\n",
        "#   Returns:\n",
        "#     List of summary ops to run on the CPU host.\n",
        "#   \"\"\"\n",
        "#   gs = gs[0]\n",
        "#   # Host call fns are executed FLAGS.iterations_per_loop times after one\n",
        "#   # TPU loop is finished, setting max_queue value to the same as number of\n",
        "#   # iterations will make the summary writer only flush the data to storage\n",
        "#   # once per loop.\n",
        "#   with summary.create_file_writer(\n",
        "#       FLAGS.model_dir, max_queue=FLAGS.iterations_per_loop).as_default():\n",
        "#     with summary.always_record_summaries():\n",
        "#       summary.scalar('loss', loss[0], step=gs)\n",
        "#       summary.scalar('learning_rate', lr[0], step=gs)\n",
        "#       summary.scalar('current_epoch', ce[0], step=gs)\n",
        "\n",
        "#       return summary.all_summary_ops()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sIa5GsA-_0rb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def test_model_fn(features, labels, mode, params):\n",
        "#     ###################################################################\n",
        "#     # The Keras LSTM layer that causes an error unless it is unrolled #\n",
        "#     ###################################################################\n",
        "#     predictions = tf.keras.layers.Conv2D(filters=1, kernel_size=1)(features)\n",
        "\n",
        "#     # Create ops for the TPUEstimatorSpec\n",
        "#     loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n",
        "#     optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
        "# #     if FLAGS.use_tpu:\n",
        "#     optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)\n",
        "  \n",
        "#     global_step = tf.train.get_global_step()\n",
        "#     train_op = optimizer.minimize(loss, global_step)\n",
        "\n",
        "#     # Return the TPUEstimatorSpec\n",
        "#     return tpu_estimator.TPUEstimatorSpec(\n",
        "#         mode=mode,\n",
        "#         loss=loss,\n",
        "#         train_op=train_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpUjJCzRBNUy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # https://github.com/tensorflow/tpu/blob/master/models/experimental/cifar_keras/cifar_keras.py\n",
        "\n",
        "# def model_fn(features, labels, mode, config, params):\n",
        "# #   print(features)\n",
        "#   v = tf.keras.layers.Input(tensor=features)\n",
        "#   pred = tf.keras.layers.Conv2D(\n",
        "#     filters=1,\n",
        "#     kernel_size=3,\n",
        "#     padding='same',\n",
        "#     activation='relu'\n",
        "#   )(v)\n",
        "  \n",
        "# #   print(pred)\n",
        "\n",
        "# #   pred=features\n",
        "  \n",
        "# #   if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "#     # Compute the current epoch and associated learning rate from global_step.\n",
        "#   global_step = tf.train.get_global_step()\n",
        "\n",
        "#   loss = tf.keras.losses.binary_crossentropy(labels, pred)  \n",
        "\n",
        "#   optimizer = tf.train.AdamOptimizer()\n",
        "#   optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "#   # Batch normalization requires UPDATE_OPS to be added as a dependency to\n",
        "#   # the train operation.\n",
        "#   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "#   with tf.control_dependencies(update_ops):\n",
        "#     train_op = optimizer.minimize(loss, global_step)\n",
        "      \n",
        "# #     model = tf.keras.Model(inputs=features, outputs=pred)\n",
        "    \n",
        "#   return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "#     mode=mode,\n",
        "#     loss=loss,\n",
        "#     train_op=train_op,\n",
        "#     host_call=None,\n",
        "#     eval_metrics=(metric_fn, [labels, pred]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oz4OuUomhxzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def main(unused_argv=None):\n",
        "#     tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver('kaggle-tpu')\n",
        "\n",
        "    \n",
        "#     config = tf.contrib.tpu.RunConfig(\n",
        "#       cluster=tpu_cluster_resolver,\n",
        "#       model_dir='gs://kaggle-195720-tgs-salt-identification-challenge/temp',\n",
        "# #       save_checkpoints_secs=3600,\n",
        "# #       session_config=tf.ConfigProto(\n",
        "# #           allow_soft_placement=True, log_device_placement=True),\n",
        "# #       tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "# #           iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "# #           num_shards=FLAGS.num_shards),\n",
        "#   )\n",
        "    \n",
        "    \n",
        "# #     config = tpu_config.RunConfig(\n",
        "# #         master=master_url,\n",
        "# #         model_dir='gs://kaggle-195720-tgs-salt-identification-challenge/temp')\n",
        "\n",
        "#     # Create the TPUEstimator\n",
        "#     estimator = tpu_estimator.TPUEstimator(\n",
        "#       use_tpu=True,\n",
        "#       model_fn=model_fn,\n",
        "#       config=config,\n",
        "#       train_batch_size=1024)\n",
        "\n",
        "#     # Train the estimator for 10 steps\n",
        "#     estimator.train(input_fn, max_steps=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXICpUNGitvo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def input_fn(params):\n",
        "#     dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "# #     dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(params['batch_size']))\n",
        "    \n",
        "#     print(dataset)\n",
        "\n",
        "#     # Make input_fn for the TPUEstimator train step\n",
        "# #     dataset_fn = dataset.make_one_shot_iterator().get_next()\n",
        "#     dataset = dataset.shuffle(1000).repeat().batch(params['batch_size'])\n",
        "    \n",
        "# #     dataset = dataset.prefetch(1)\n",
        "#     return dataset\n",
        "  \n",
        "  \n",
        "# # dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "# # dataset\n",
        "# # dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(128))\n",
        "\n",
        "# # print(dataset)\n",
        "\n",
        "# # # Make input_fn for the TPUEstimator train step\n",
        "# # dataset_fn = dataset.make_one_shot_iterator().get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFjqvXl4ijH9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myc4wE2NdCN-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.contrib import tpu\n",
        "# from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
        "\n",
        "# def axy_computation(a, x, y):\n",
        "#   return a * x + y\n",
        "\n",
        "# inputs = [\n",
        "#     3.0,\n",
        "#     tf.ones([3, 3], tf.float32),\n",
        "#     tf.ones([3, 3], tf.float32),\n",
        "# ]\n",
        "\n",
        "# tpu_computation = tpu.rewrite(axy_computation, inputs)\n",
        "\n",
        "# tpu_grpc_url = TPUClusterResolver(\n",
        "#     tpu=['kaggle-tpu']).get_master()\n",
        "\n",
        "# with tf.Session(tpu_grpc_url) as sess:\n",
        "#   sess.run(tpu.initialize_system())\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "#   output = sess.run(tpu_computation)\n",
        "#   print(output)\n",
        "#   sess.run(tpu.shutdown_system())\n",
        "\n",
        "# print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16XOGte5q_0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import keras\n",
        "# from keras import Model\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "# from keras.models import load_model\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, SpatialDropout2D, UpSampling2D\n",
        "\n",
        "# from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# input: (x, 101, 101, 1), (x, 101, 101, 1)\n",
        "# output: (x,)\n",
        "def batch_iou(y_true, y_pred):\n",
        "    pred_ = tf.round(y_pred)\n",
        "    truth_ = y_true\n",
        "    \n",
        "    i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "    u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "# input: (x,)\n",
        "# output: ()\n",
        "def mean_iou(y_true, y_pred):\n",
        "    ious = batch_iou(y_true, y_pred)\n",
        "    ious = tf.reshape(ious, (-1,1))\n",
        "    ious = tf.tile(ious, (1,10))\n",
        "    thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "    return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "# def dice_loss(y_true, y_pred):\n",
        "#     smooth = 1.\n",
        "#     y_true_f = K.flatten(y_true)\n",
        "#     y_pred_f = K.flatten(y_pred)\n",
        "#     intersection = y_true_f * y_pred_f\n",
        "#     score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "#     return 1. - score\n",
        "  \n",
        "  \n",
        "\n",
        "# # define iou or jaccard loss function\n",
        "# def jaccard_loss(y_true, y_pred):\n",
        "#     y_true = tf.reshape(y_true, [-1])\n",
        "#     y_pred = tf.reshape(y_pred, [-1])\n",
        "#     intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    \n",
        "#     epsilon = tf.keras.backend.epsilon()\n",
        "    \n",
        "#     score = (intersection + epsilon) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + epsilon)\n",
        "#     return 1 - score\n",
        "\n",
        "\n",
        "  \n",
        "# def iou_bce_loss(y_true, y_pred):\n",
        "#     return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * jaccard_loss(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.to_float(tf.greater(tf.round(y_true), y_pred)))\n",
        "    \n",
        "\n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     i = tf.round(y_pred) * tf.round(y_true)\n",
        "#     o = tf.round(y_pred) * tf.round(y_true)\n",
        "#     mean_iou = tf.metrics.mean_iou(y_true, tf.round(y_pred), 1)\n",
        "#     print(mean_iou)\n",
        "#     print(tf.tile(mean_iou, 10))\n",
        "#     print(mean_iou[0])\n",
        "#     return mean_iou[0]\n",
        "#     return tf.reduce_mean(tf.tile(mean_iou, 10))\n",
        "  \n",
        "# def mean_iou(y_true, y_pred):\n",
        "#     prec = []\n",
        "#     for t in np.arange(0.5, 1.0, 0.05):\n",
        "#         y_pred_ = tf.to_int32(y_pred > t)\n",
        "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         with tf.control_dependencies([up_opt]):\n",
        "#             score = tf.identity(score)\n",
        "#         prec.append(score)\n",
        "#     return K.mean(K.stack(prec), axis=0)\n",
        "\n",
        "\n",
        "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
        "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if activation == True:\n",
        "        x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "  \n",
        "def residual_block(blockInput, num_filters=16):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = convolution_block(x, num_filters, (3,3) )\n",
        "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
        "    x = Add()([x, blockInput])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "def conv_block(m, dim, acti, bn, res, do=0.5):\n",
        "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
        "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
        "    n = tf.keras.layers.SpatialDropout2D(do)(n) if do else n\n",
        "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
        "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
        "    n = tf.keras.layers.Concatenate()([m, n]) if res else n\n",
        "    return n\n",
        "\n",
        "  \n",
        "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "    if depth > 0:\n",
        "        n = conv_block(m, dim, acti, bn, res)\n",
        "        m = tf.keras.layers.MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "        m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
        "        if up:\n",
        "            m = tf.keras.layers.UpSampling2D()(m)\n",
        "            m = tf.keras.layers.Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "        else:\n",
        "#             padding = ['valid', 'same','same','valid','same','valid']\n",
        "            padding = ['same','same','valid','same','valid']\n",
        "            m = tf.keras.layers.Conv2DTranspose(dim, 3, strides=2, activation=acti, padding=padding[depth-1])(m)\n",
        "        n = tf.keras.layers.Concatenate()([n, m])\n",
        "        m = conv_block(n, dim, acti, bn, res)\n",
        "    else:\n",
        "        m = conv_block(m, dim, acti, bn, res, do=do)\n",
        "    return m\n",
        "\n",
        "  \n",
        "# def UNet(img_shape, out_ch=1, start_ch=32, depth=5, inc_rate=2., activation='relu', \n",
        "#          dropout=False, batchnorm=True, maxpool=True, upconv=False, residual=True):\n",
        "#     i = tf.keras.layers.Input(shape=img_shape)\n",
        "#     o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
        "#     o = tf.keras.layers.Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
        "#     return tf.keras.Model(inputs=i, outputs=o)\n",
        "\n",
        "def UNet(img_shape, out_ch=1, start_ch=32, depth=5, inc_rate=2., activation='relu', \n",
        "         dropout=False, batchnorm=True, maxpool=True, upconv=False, residual=True):\n",
        "    i = tf.keras.layers.Input(shape=img_shape)\n",
        "    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
        "    o = tf.keras.layers.Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
        "    return tf.keras.Model(inputs=i, outputs=o)\n",
        "  \n",
        "  \n",
        "\n",
        "# def ResNet():\n",
        "#   i = Input(shape=(img_size_target, img_size_target))\n",
        "  \n",
        "#   o = Conv2D(filters=1, kernel_size=1, activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NDV_LZ8Ernrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2206
        },
        "outputId": "0a09936c-c198-4b0a-e043-a3a0c3a6b204"
      },
      "cell_type": "code",
      "source": [
        "def build_train_generator(X_train, y_train, batch_size=32):\n",
        "    # Ref: https://keras.io/preprocessing/image/\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(\n",
        "      rotation_range=360.,\n",
        "#       width_shift_range=0.1,\n",
        "#       height_shift_range=0.1,\n",
        "      fill_mode='reflect',\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "    )\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
        "    seed = 1\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "    \n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "\n",
        "    # combine generators into one which yields image and masks\n",
        "    return zip(image_generator, mask_generator)\n",
        "\n",
        "\n",
        "def cv(X_train, y_train, X_test, fold, fast=False):\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    \n",
        "    loss = []\n",
        "    metric = []\n",
        "    pred_test = np.zeros(X_test.shape)\n",
        "    fold_index = 0\n",
        "    \n",
        "    start_ch = 32\n",
        "    depth = 5\n",
        "         \n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train):      \n",
        "#       model = UNet((img_size_target,img_size_target,1),start_ch=start_ch,depth=depth,batchnorm=True, dropout=False)\n",
        "      model = test_model()\n",
        "      resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "      strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "      model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "      session_master = resolver.master()\n",
        "      \n",
        "      model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"binary_crossentropy\"])\n",
        "\n",
        "#       early_stopping = EarlyStopping(patience=20, verbose=1)\n",
        "#       model_checkpoint = ModelCheckpoint(\"./model/keras_%d.model\"%fold_index, save_best_only=True, verbose=1)\n",
        "#       reduce_lr = ReduceLROnPlateau(factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
        "\n",
        "      epochs = 20000\n",
        "      batch_size = 32\n",
        "\n",
        "      history = model.fit_generator(\n",
        "          build_train_generator(X_train[train_index], y_train[train_index], batch_size),\n",
        "          validation_data=(X_train[test_index], y_train[test_index]),\n",
        "          steps_per_epoch=np.ceil(len(X_train[train_index]) / batch_size), \n",
        "          epochs=epochs,\n",
        "#           callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "      )\n",
        "      \n",
        "      best_idx = np.argmin(history.history['val_loss'])\n",
        "      val_loss = history.history['val_loss'][best_idx]\n",
        "      val_mean_iou = history.history['val_mean_iou'][best_idx]\n",
        "      \n",
        "      print(\"Best val_loss: %.4f, val_mean_iou: %.4f\"%(val_loss, val_mean_iou))\n",
        "      loss.append(val_loss)\n",
        "      metric.append(val_mean_iou)\n",
        "      \n",
        "      model = keras.models.load_model('model/keras_%d.model'%fold_index, custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss, 'dice_loss': dice_loss})\n",
        "      pred_test += model.predict(X_test) / fold\n",
        "      fold_index = fold_index + 1\n",
        "      \n",
        "      if fast:\n",
        "        break\n",
        "      \n",
        "      \n",
        "    print('%d fold val_loss: %.4f, val_mean_iou: %.4f'%(fold, np.mean(loss), np.mean(metric)))\n",
        "    return pred_test, np.mean(loss), np.mean(metric)\n",
        "\n",
        "  \n",
        "pred_test, val_loss, val_mean_iou = cv(X_train, y_train, X_test, 5, fast=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 13486222805198966859)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9984220072123666814)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 18162437541401842557)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9211313139234737039)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4136755912537517626)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6532054137011805886)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3825718477947054890)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5276807511303573055)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5719668420551969614)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10390436815882700867)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15818039368216332404)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6864323238587904500)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: metrics/mean_iou/ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_BOOL, Truncate=false, _tpu_replicate=\"cluster\"](metrics/mean_iou/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0dea82b24951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-0dea82b24951>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(X_train, y_train, X_test, fold, fast)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kaggle-tpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUDistributionStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_to_tpu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0msession_master\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/framework/python/framework/experimental.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m'any time, and without warning.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         decorator_utils.get_qualified_name(func), func.__module__)\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m   new_func.__doc__ = _add_experimental_function_notice_to_docstring(\n\u001b[1;32m     66\u001b[0m       func.__doc__)\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mtpu_model\u001b[0;34m(model, strategy)\u001b[0m\n\u001b[1;32m   1888\u001b[0m   \"\"\"\n\u001b[1;32m   1889\u001b[0m   \u001b[0;31m# Force initialization of the CPU model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m       \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   2706\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 710\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: metrics/mean_iou/ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_BOOL, Truncate=false, _tpu_replicate=\"cluster\"](metrics/mean_iou/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HRemgbrVOpjK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # summarize history for loss\n",
        "# plt.plot(history.history['loss'][1:])\n",
        "# plt.plot(history.history['val_loss'][1:])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train','Validation'], loc='upper left')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNqZwFOebfzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# best_idx = np.argmin(history.history['val_loss'])\n",
        "# val_loss = history.history['val_loss'][best_idx]\n",
        "# val_mean_iou = history.history['val_mean_iou'][best_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgBEjwKHCn1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('model/keras.model', custom_objects={'mean_iou': mean_iou, 'iou_bce_loss': iou_bce_loss})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EqepTCQa1djX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RLenc(img, order='F', format=True):\n",
        "    \"\"\"\n",
        "    img is binary mask image, shape (r,c)\n",
        "    order is down-then-right, i.e. Fortran\n",
        "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
        "\n",
        "    returns run length as an array or string (if format is True)\n",
        "    \"\"\"\n",
        "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
        "    runs = []  ## list of run lengths\n",
        "    r = 0  ## the current run length\n",
        "    pos = 1  ## count starts from 1 per WK\n",
        "    for c in bytes:\n",
        "        if (c == 0):\n",
        "            if r != 0:\n",
        "                runs.append((pos, r))\n",
        "                pos += r\n",
        "                r = 0\n",
        "            pos += 1\n",
        "        else:\n",
        "            r += 1\n",
        "\n",
        "    # if last run is unsaved (i.e. data ends with 1)\n",
        "    if r != 0:\n",
        "        runs.append((pos, r))\n",
        "        pos += r\n",
        "        r = 0\n",
        "\n",
        "    if format:\n",
        "        z = ''\n",
        "\n",
        "        for rr in runs:\n",
        "            z += '{} {} '.format(rr[0], rr[1])    \n",
        "        return z[:-1]\n",
        "    else:\n",
        "        return runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5wnyMSkd50M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class FasterRle(object):\n",
        "#     \"\"\"Perform RLE in paralell.\"\"\"\n",
        "\n",
        "#     def __init__(self, num_consumers=2):\n",
        "#         \"\"\"Initialize class.\"\"\"\n",
        "#         self._tasks = multiprocessing.JoinableQueue()\n",
        "#         self._results = multiprocessing.Queue()\n",
        "#         self._n_consumers = num_consumers\n",
        "\n",
        "#         # Initialize consumers\n",
        "#         self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n",
        "#         for w in self._consumers:\n",
        "#             w.start()\n",
        "\n",
        "#     def add(self, array, startIndex):\n",
        "#         \"\"\"Add a task to perform.\"\"\"\n",
        "#         self._tasks.put(FasterTask(array, startIndex))\n",
        "\n",
        "#     def get_results(self):\n",
        "#         \"\"\"Close all tasks.\"\"\"\n",
        "#         # Provide poison pill\n",
        "#         [self._tasks.put(None) for _ in range(self._n_consumers)]\n",
        "#         # Wait for finish\n",
        "#         self._tasks.join()\n",
        "#         # Return results\n",
        "#         singles = []\n",
        "#         while not self._results.empty():\n",
        "#             singles.append(self._results.get())\n",
        "            \n",
        "#         resultDic = dict()\n",
        "#         for rles, start in singles:\n",
        "#             #print('start:', start)\n",
        "#             for i,rle in enumerate(rles):\n",
        "#                 #print('i:', i)\n",
        "#                 resultDic[str(start+i)] = rle\n",
        "#         return resultDic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7wRK-ZUEmzA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch(func):\n",
        "  def batch_func(b):\n",
        "    return np.array([func(e) for e in tqdm(b)])\n",
        "  return batch_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sy0eHi4-P5PP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preds_test = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hIcfQ4_pPlHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e52d1ce9-b9b2-4fdf-f039-e606626a790a"
      },
      "cell_type": "code",
      "source": [
        "test['rle_mask'] = batch(RLenc)(np.round(pred_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18000/18000 [00:58<00:00, 309.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ST2JPIMLP666",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7aaf49b7-1c59-4efc-cfb8-2bdd7e3a885f"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_FILE = 'keras_unet_sc32de5wbecjaccloss_%.4f_%.4f.csv'%(val_loss, val_mean_iou)\n",
        "test.to_csv(SUBMISSION_FILE)\n",
        "SUBMISSION_FILE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'keras_unet_sc32de5wbecjaccloss_0.1320_0.7477.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "7iJCa8ZqRppy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cc737439-5883-4295-c670-658d69aaff48"
      },
      "cell_type": "code",
      "source": [
        "SUBMISSION_MESSAGE='\"Keras U-net iou_bce_loss: val_loss: %.4f, val_mean_iou: %.4f\"'%(val_loss, val_mean_iou)\n",
        "!kaggle competitions submit -f '{SUBMISSION_FILE}' -m '{SUBMISSION_MESSAGE}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using competition: tgs-salt-identification-challenge\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.4.6)\n",
            "Successfully submitted to TGS Salt Identification Challenge"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ql1uneNwTuEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "c9be94ee-85ec-4429-d351-98436cfaf45f"
      },
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv('.env')\n",
        "!gsutil rsync model gs://{os.environ['GCP_BUCKET']}/model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "Copying file://model/keras_0.model [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "Copying file://model/keras_1.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_2.model [Content-Type=application/octet-stream]...\n",
            "Copying file://model/keras_3.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m -o ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying file://model/keras_4.model [Content-Type=application/octet-stream]...\n",
            "|\n",
            "Operation completed over 5 objects/751.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6UwBwalwuZpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "bad2519d-f469-4e15-ce5c-1a921ed92c84"
      },
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# def batch_iou(pred, truth):\n",
        "#     pred_ = tf.round(pred)\n",
        "#     truth_ = truth\n",
        "    \n",
        "#     i = tf.reduce_sum(pred_ * truth_, axis=[1,2,3])\n",
        "#     u = tf.reduce_sum((pred_ + truth_) - (pred_ * truth_), axis=[1,2,3])\n",
        "    \n",
        "#     epsilon = tf.keras.backend.epsilon()\n",
        "#     return (i + epsilon) / (u + epsilon)\n",
        "\n",
        "  \n",
        "# def mean_iou(pred, truth):\n",
        "#     ious = batch_iou(pred, truth)\n",
        "#     ious = tf.reshape(ious, (-1,1))\n",
        "#     ious = tf.tile(ious, (1,10))\n",
        "#     thresholds = tf.range(0.5, 1.0, delta=0.05)\n",
        "    \n",
        "#     return tf.reduce_mean(tf.to_float(ious >= thresholds))\n",
        "\n",
        "\n",
        "# BATCH_SIZE = 100\n",
        "# ones = tf.constant(np.ones((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# zeros = tf.constant(np.zeros((BATCH_SIZE, 101, 101, 1), dtype='float32'))\n",
        "# half = tf.coonstant(np.zeros(BATCH_SIZE, ))\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "  \n",
        "#   print(mean_iou(ones, zeros).eval())\n",
        "#   print(mean_iou(ones, ones).eval())\n",
        "#   print(mean_iou(zeros, ones).eval())\n",
        "#   print(mean_iou(zeros, zeros).eval())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1AayNFWou9ms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2345
        },
        "outputId": "b0ad8070-0ad3-4ddc-d926-aae031e16852"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Standard Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 12\n",
        "\n",
        "# input image dimensions\n",
        "IMG_ROWS, IMG_COLS = 28, 28\n",
        "\n",
        "use_tpu=True\n",
        "\n",
        "\n",
        "def mnist_model(input_shape):\n",
        "  \"\"\"Creates a MNIST model.\"\"\"\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(\n",
        "      tf.keras.layers.Conv2D(\n",
        "          32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(tf.keras.layers.Dropout(0.25))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "\n",
        "def main(unused_dev):\n",
        "  use_tpu = True\n",
        "\n",
        "  print('Mode:', 'TPU' if use_tpu else 'CPU')\n",
        "\n",
        "  if True:\n",
        "    print('Using fake data')\n",
        "    x_train = np.random.random((128, IMG_ROWS, IMG_COLS))\n",
        "    y_train = np.zeros([128, 1], dtype=np.int32)\n",
        "    x_test, y_test = x_train, y_train\n",
        "  else:\n",
        "    # the data, split between train and test sets\n",
        "    print('Using real data')\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "  x_train = x_train.reshape(x_train.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
        "  input_shape = (IMG_ROWS, IMG_COLS, 1)\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  print('x_train shape:', x_train.shape)\n",
        "  print(x_train.shape[0], 'train samples')\n",
        "  print(x_test.shape[0], 'test samples')\n",
        "\n",
        "  # convert class vectors to binary class matrices\n",
        "  y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "  y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "  model = mnist_model(input_shape)\n",
        "\n",
        "  if use_tpu:\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='kaggle-tpu')\n",
        "    )\n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05),\n",
        "      metrics=['accuracy'])\n",
        "\n",
        "  model.fit(\n",
        "      x_train,\n",
        "      y_train,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      epochs=EPOCHS,\n",
        "      verbose=1,\n",
        "      validation_data=(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Loss for final step:', score[0])\n",
        "  print('Accuracy ', score[1])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mode: TPU\n",
            "Using fake data\n",
            "x_train shape: (128, 28, 28, 1)\n",
            "128 train samples\n",
            "128 test samples\n",
            "INFO:tensorflow:Detected old TPUStrategy API.\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 128 samples, validate on 128 samples\n",
            "Epoch 1/12\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(16, 28, 28, 1), dtype=tf.float32, name='conv2d_4_input0'), TensorSpec(shape=(16, 10), dtype=tf.float32, name='dense_5_target0')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_4_input\n",
            "INFO:tensorflow:Started compiling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1260\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'group_assignment' not in Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\", group_assignment=[]](training/TFOptimizer/gradients/tpu_model_139793487692408/conv2d_4/Conv2D_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e8ddd031b353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-e8ddd031b353>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_dev)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m           **kwargs)\n\u001b[0m\u001b[1;32m   1041\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    852\u001b[0m                                                    infeed_manager)\n\u001b[1;32m    853\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_model_compiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;31m# Initialize our TPU weights on the first compile.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_test_model_compiles\u001b[0;34m(self, tpu_model_ops)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m     \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpu_compilation_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompilationResultProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 668\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef mentions attr 'group_assignment' not in Op<name=CrossReplicaSum; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: training/TFOptimizer/CrossReplicaSum = CrossReplicaSum[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], _tpu_replicate=\"cluster\", group_assignment=[]](training/TFOptimizer/gradients/tpu_model_139793487692408/conv2d_4/Conv2D_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "R3b3873xfUM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}